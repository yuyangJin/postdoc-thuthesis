% !TeX root = ../thuthesis-example.tex

\chapter{基于弹性张量的大模型微调内存优化}

\section{概述}

大语言模型 [1, 30, 34, 39]（LLMs）正日益普及。
与此同时，个性化以及支持个性化的微调技术 [7, 38, 40] 已成为大模型后训练领域的热门话题。
微调通常在有限的数据和典型的硬件配置下完成 [8, 10, 45]，并且需要快速生成多个微调模型 [11, 16, 47]。
然而，由于大语言模型参数数量巨大，进行全参数微调难以满足上述要求 [15, 35, 46]。

为解决这一问题，研究人员提出了参数高效微调（Parameter-efficient finetuning, PEFT）。
与全参数微调不同，PEFT 从大模型的预训练权重出发，仅训练一小部分权重，同时冻结大部分权重 [14, 21, 26, 32, 42]。
这大幅减少了训练模型的计算量和存储需求（因为只需存储训练的那部分权重），同时仍能实现理想的模型性能。因此，PEFT 现已成为微调的主流方法 [6, 15, 23]。

在微调过程中，每个设备存储模型权重以及训练过程中生成的中间结果（激活值）。权重和激活值的大小与模型规模相关，大型模型的参数规模可达数十亿甚至数百亿。然而，单个 GPU 的内存限制通常仅为几十 GB。因此，权重和激活值需要在多个设备间进行划分，由此产生的通信开销成为性能瓶颈 [10, 45]。

内存可用于优化通信，这对训练性能至关重要。对于权重而言，更多的本地可用权重可以减少训练过程中的通信量；对于激活值，使用更大的批量大小虽然会增加激活值的内存需求，但可以降低每个样本的通信量。如图 1（a）所示，随着内存使用量的增加，不同模型的吞吐量能够得到显著提升。

现有的训练优化方法从内存角度展开。Deepspeed [33] 和 Torch-FSDP [43] 采用数据并行和零冗余优化（ZeRO）方法，为每个设备分配一部分样本。Megatron [20] 集成了数据并行、张量并行和流水线并行，对模型权重进行划分以减少通信开销。然而，由于以下两个原因，它们仍然无法充分利用内存：

激活值的峰谷模式：由于微调过程中反向传播的特性，在前向传播阶段首先生成的激活值直到反向传播的最后阶段才会被使用和释放。如图 1（c）所示，这种先进后出（FILO）的工作流程导致激活值内存使用呈现峰谷模式，造成大量内存未被充分利用（白色部分）。
静态划分的权重：激活值的内存使用在运行时动态变化，而权重在训练前被静态划分，并且在训练过程中不会进行调整。这种限制使得权重无法根据当前内存使用情况及其变化趋势进行动态调整。如图 1（c）所示，静态划分的权重无法利用激活值在低谷区域留下的内存空间。

因此，如图 1（b）所示，现有框架的时间平均内存利用率较低。其根本原因可归结为激活值和权重内存的静态调度：激活值以固定的速度和顺序产生和消耗，而静态分配的权重无法利用激活值留下的内存空间。

经观察，在 PEFT 中，权重和激活值的内存都可以进行动态高效的调整：对于 PEFT 中的权重，由于大部分权重被冻结，在不同设备上复制这些权重不会引入额外的同步开销。同时，通过细粒度地改变计算执行顺序，可以灵活修改驻留在内存中的激活值的大小和生命周期。

基于此观察，本章提出弹性张量的概念，以统一权重和激活值的动态内存管理。张量（权重和激活值）的大小在运行时动态调整，使权重内存和激活值内存之间的权衡适应当前内存使用情况和训练过程。在收集完整权重进行计算时调整权重内存；通过细粒度地生成、保存检查点和消耗激活值，灵活确定激活值的生命周期，从而调整其内存使用量。如图 1（d）所示，权重可以填充激活值内存的低谷区域，减少通信量；激活值可以降低峰值内存使用量，保持较高的内存利用率。

基于弹性张量，本章构建了 mTuner，这是一个具有高内存利用率的微调系统，可提高 PEFT 效率。mTuner 在运行时利用弹性权重和激活值，实现高内存利用率以节省通信量。在配备八个 GPU 的服务器上，对基于 Transformer、参数规模从 70 亿到 700 亿的大语言模型进行评估，结果表明，与 Deepspeed [33] 和 Megatron [20] 等最先进的训练系统相比，mTuner 可将微调性能提升最高达 1.51 倍（平均提升 1.28 倍）。mTuner 的吞吐量超过每秒 30,000 个标记，并且能够在 3 小时内使用 Llama-2 13B 模型对数学数据集 [12] 进行微调。

本文做出了以下贡献：

对 PEFT 的内存使用情况进行了详细分析，其主要包括权重和激活值；
提出弹性张量，能够灵活动态地调整权重和激活值的内存使用量；
基于弹性张量构建 mTuner，使执行计划能够适应内存状态，实现高效微调；
在参数规模从 70 亿到 700 亿的各种大语言模型上对 mTuner 进行评估，在 PCIe GPU 服务器上实现了最高 1.51 倍（平均 1.28 倍）的吞吐量提升，并且能够在 3 小时内微调模型以适应特定领域。

\section{mTuner系统概述}
图 3 展示了 mTuner 的系统概述。mTuner 的设计基于微调过程中张量的特殊属性--张量的弹性：权重和激活值张量的内存使用量都可以灵活动态地变化。第 4 节将分别深入探讨权重和激活值的张量弹性，讨论改变它们大小的方法以及相关的成本和收益。

接着介绍执行计划如何适应权重和激活值的弹性。因此XXXXXXXXXXXXXXXXXXXXXXXXXXX，mTuner 能够通过利用弹性张量实现自适应内存的训练计划。第 5 节将探讨 mTuner 如何感知内存状态。并根据内存状态，开发自适应内存的执行计划，利用张量弹性针对不同的内存状态进行优化，之后将介绍 mTuner 如何根据硬件设置和模型结构搜索具体的执行计划。


\section{弹性张量}
\label{mtuner:sec:elastic-tensor}

在微调过程中，需要长时间驻留在内存中的张量会占用大量内存，这包括分配给设备的权重内存以及设备计算过程中生成的中间结果（激活值）。本章研究发现，在 PEFT 的背景下，这些张量的内存大小可以进行调整。通过各种方式，可以增加或减少权重和激活值的内存使用量，同时需要考虑执行成本和收益之间的权衡。

\subsection{权重弹性}

\subsection{冻结权重}

由于 PEFT 区分基础模型和适配器的设计，其大部分权重（来自基础模型的权重）在训练过程中不会被更新，这些权重被称为冻结权重。尽管冻结权重不会更新，但它们仍然参与前向和后向计算（分别用于获取激活值和计算激活值的梯度）。因此，在传统的并行训练系统中，它们被视为普通权重。然而，我们发现由于这些冻结权重保持不变，它们可以在不同设备上冗余存储而不引入额外的同步开销。也就是说，对于在 D 个设备上进行的分布式微调，每个设备上权重的存储比例可以从\(\frac{1}{D}\)（全分片）连续变化到 100\%（全复制）。我们将这些存储比例可连续变化的权重称为弹性权重。

\subsubsection{权重大小的动态调整}

如图 4（a）和（b）所示，在执行一个算子时，每个设备通过通信（解分片，unshard）获取其完整权重，然后用于计算。计算完成后，收集到的权重被释放（重新分片，reshard）。
由于设备在计算过程中拥有完整的权重，mTuner 可以在不产生额外通信开销的情况下重新调整权重大小。在图 4（c）的示例中，在计算前，每个设备仅拥有算子权重的\(\frac{1}{D}\)，计算后调整为\(\frac{2}{D}\)。这种动态调整可以在每次使用权重进行计算时进行，我们将这些时刻称为调整点。在一次迭代中，每个算子的权重至少有两个调整点：前向计算时和反向计算时。如果应用激活值检查点的重新计算，调整点可能会多于两个。mTuner 可以在这些调整点动态改变权重大小，以适应不同的内存使用情况。

\subsubsection{弹性权重的搜索空间}

弹性权重的大小可以在两个维度上进行调整：算子内和算子间。在算子内维度上，对于给定的算子，mTuner 可以将单个设备上存储的权重大小从\(\frac{1}{D}\)调整到 100\%。
然而，如果将弹性权重大小设置为\(\frac{k}{D}\)，只有当 k 是 D 的因数时，才能缩小通信范围（从 D 个设备间的通信缩小到\(\frac{D}{k}\)个设备间的通信），从而显著提高通信效率。因此，在算子内维度上，mTuner 仅在 k 是 D 的因数时将弹性权重大小设置为\(\frac{k}{D}\)。例如，在使用八个 GPU 进行微调的情况下，每个算子的权重大小仅会选择 12.5\%、25\%、50\% 或 100\%。尽管算子内维度的这种选择策略可能会限制权重大小的灵活性，但在算子间维度上，不同的算子可以独立选择不同的弹性权重大小。由于大语言模型由大量算子组成，通过组合不同权重大小的算子，可以灵活配置整体弹性权重大小，使其连续覆盖从\(\frac{1}{D}\)到 100\% 的值。

\subsection{激活弹性}
在参数高效微调（PEFT）中，mTuner 还可以使激活具备弹性，以缓解因峰谷内存模式导致的内存利用率低下问题。我们发现，激活在内存中累积的主要原因是其先进后出（FILO）模式，以及由此产生的激活驻留时间长。因此，mTuner 采用两种方法来缩短生成的激活的生命周期，从而减少 FILO 模式带来的问题。第一种方法是提高反向计算的执行优先级，优先消耗前向传播过程中生成的激活；第二种方法是丢弃部分激活，并在反向传播过程中重新生成这些激活。
\subsubsection{提高反向计算优先级}
激活在前向传播阶段生成，在反向传播阶段使用并释放。因此，更早地执行反向计算可以缩短生成的激活的生命周期，使其更早地释放，减少在内存中的驻留时间。然而，在前向 - 反向计算范式中，必须先计算所有前向算子以获取损失，然后才能进行反向计算，这个顺序是不可改变的。为了便于更早地执行反向计算，我们提出沿样本维度拆分和重新排列计算顺序。
如图 5（a）所示，对于在四个输入样本上训练一个 4 层模型，典型的过程是将它们一起处理，先执行四个前向计算，然后执行四个反向计算。如图 5（b）所示，这个过程导致四个样本在前向传播结束时生成的激活都驻留在内存中，直到反向传播开始。由于所有激活都累积起来，内存使用量达到峰值。
为了更早地释放激活，mTuner 将样本划分为更小的批次，每个批次包含一个或多个样本。然后，对每个批次依次执行前向和反向计算，而不是将所有样本一起处理。如图 5（c）所示，对于四个样本的批次，mTuner 将它们分成两个批次，每个批次包含两个样本。它先对第一个批次执行前向计算，然后执行反向计算，释放第一个批次生成的激活。然后，对第二个批次重复相同的过程。通过这种方式，每个批次生成的激活在反向计算完成后立即释放，而不是累积到所有前向计算完成后，从而降低了峰值内存使用量。
\subsubsection{激活重计算}
除了重新排列计算顺序外，mTuner 还采用激活重计算技术进一步减少激活内存使用。激活重计算是一种通过重新计算激活而不是将其存储在内存中来减少激活内存占用的方法。在前向传播过程中，mTuner 不存储所有生成的激活，而是仅存储少量关键激活，这些关键激活足以在反向传播过程中重新计算其余激活。在反向传播过程中，mTuner 使用存储的关键激活重新计算所需的激活，而不是从内存中读取它们。
如图 6 所示，对于一个 4 层模型，mTuner 可以选择仅存储第 1 层和第 3 层的激活作为关键激活。在反向传播过程中，当需要第 2 层的激活时，mTuner 使用存储的第 1 层激活重新计算第 2 层的激活。通过这种方式，mTuner 可以显著减少激活内存使用，因为它只需要存储少量关键激活，而不是所有生成的激活。然而，激活重计算并非没有代价。由于在反向传播过程中需要重新计算激活，这会增加计算时间。因此，mTuner 需要在激活内存使用和计算时间之间进行权衡，以确定要存储的关键激活的最佳数量。
\subsubsection{激活弹性的搜索空间}
激活弹性的大小可以在两个维度上进行调整：样本维度和层维度。在样本维度上，mTuner 可以调整批次大小，即将样本划分为不同大小的批次。较小的批次大小可以更早地释放激活，降低峰值内存使用量，但会增加计算开销，因为每个批次都需要单独的前向和反向计算。较大的批次大小可以减少计算开销，但会增加峰值内存使用量。因此，mTuner 需要根据可用内存和计算资源在样本维度上选择合适的批次大小。
在层维度上，mTuner 可以选择存储哪些层的激活作为关键激活。不同的层选择会导致不同的激活内存使用和计算时间。存储更多层的激活作为关键激活可以减少反向传播过程中的计算时间，但会增加激活内存使用。存储较少层的激活作为关键激活可以减少激活内存使用，但会增加反向传播过程中的计算时间。因此，mTuner 需要在层维度上找到激活内存使用和计算时间之间的最佳平衡。
\section{自适应内存执行计划}
在\Cref{mtuner:sec:elastic-tensor}中，我们讨论了权重和激活的弹性，以及它们在内存使用方面的动态调整能力。在本节中，我们将介绍 mTuner 如何利用这些弹性张量来制定自适应内存执行计划，以适应不同的内存状态并优化微调性能。
\subsection{内存状态感知}
为了制定自适应内存执行计划，mTuner 首先需要感知内存状态。mTuner 通过监测设备内存使用情况和通信开销来获取内存状态信息。具体来说，mTuner 跟踪以下指标：
权重内存使用量：每个设备上存储的权重的大小。这包括分配给设备的固定权重和动态调整的弹性权重。
激活内存使用量：每个设备上存储的激活的大小。这包括当前驻留在内存中的激活和由于激活重计算而存储的关键激活。
空闲内存量：每个设备上当前未被使用的内存大小。空闲内存量是通过从设备的总内存容量中减去权重内存使用量和激活内存使用量得到的。
通信开销：在一次迭代中，设备之间为了获取完整权重和交换激活而进行通信所消耗的时间和带宽。通信开销是评估微调性能的重要指标，因为它直接影响训练速度。
mTuner 定期收集这些指标，并根据它们来评估当前的内存状态。例如，如果空闲内存量较低，mTuner 会认为内存处于紧张状态，需要采取措施减少内存使用。如果通信开销较高，mTuner 会尝试调整权重和激活的分布，以减少通信量。
\subsection{执行计划制定}
根据感知到的内存状态，mTuner 制定自适应内存执行计划。执行计划包括如何在设备之间分配权重、如何生成和管理激活，以及何时进行计算等决策。
当内存处于紧张状态时，mTuner 会优先减少内存使用。对于权重，它会减少每个设备上存储的弹性权重大小，以释放更多内存用于激活。例如，如果当前空闲内存量不足以存储所有激活，mTuner 可能会将某些算子的弹性权重大小从 100\% 调整为 50\%，从而为激活腾出空间。对于激活，mTuner 会采用较小的批次大小和更多的激活重计算，以降低峰值内存使用量。较小的批次大小可以更早地释放激活，而更多的激活重计算可以减少存储的激活数量。
当内存相对充裕时，mTuner 会尝试提高计算效率。对于权重，它会增加每个设备上存储的弹性权重大小，以减少通信量。例如，如果空闲内存量较多，mTuner 可能会将更多算子的弹性权重大小设置为 100\%，这样每个设备在计算时就不需要与其他设备进行通信来获取完整权重。对于激活，mTuner 会采用较大的批次大小和较少的激活重计算，以减少计算开销。较大的批次大小可以减少前向和反向计算的次数，而较少的激活重计算可以减少重新计算激活的时间。
\subsection{执行计划搜索}
mTuner 使用搜索算法来找到最佳的执行计划。由于权重和激活弹性的搜索空间较大，穷举搜索所有可能的执行计划是不现实的。因此，mTuner 采用启发式搜索算法来有效地探索搜索空间。
mTuner 从一个初始执行计划开始，该计划可以是默认的静态执行计划或基于一些简单规则生成的计划。然后，mTuner 通过对当前执行计划进行小的调整来生成候选执行计划。这些调整可以包括改变某个算子的弹性权重大小、调整批次大小或选择不同的激活重计算层。对于每个候选执行计划，mTuner 评估其性能，包括内存使用量、通信开销和训练吞吐量。根据评估结果，mTuner 选择性能最佳的候选执行计划作为新的当前执行计划，并继续生成新的候选执行计划进行评估。这个过程会持续进行，直到达到预定的搜索预算（例如，一定数量的迭代次数或时间限制）或找到满意的执行计划。
mTuner 还利用硬件设置和模型结构的信息来指导搜索过程。例如，如果设备具有高带宽通信网络，mTuner 可能会更倾向于选择那些虽然通信量较大但计算效率更高的执行计划。如果模型具有较多的层或较大的参数规模，mTuner 会更加关注激活内存使用和权重分配，以避免内存不足和通信瓶颈。


\section{实验评估}
\subsection{实验设置}
本章在配备八个 NVIDIA A100 GPU 的服务器上进行实验，每个 GPU 具有 80GB 内存。本章评估了 mTuner 在不同规模的基于 Transformer 的大语言模型上的性能，包括 7B、13B、30B 和 70B 参数模型。本章使用 PyTorch 作为深度学习框架，并将 mTuner 与两个最先进的训练系统进行比较：Deepspeed [33] 和 Megatron [20]。
本章使用 Llama-2 [30] 作为基础模型，并在各种数据集上进行微调，包括用于通用语言任务的 C4 [37] 和用于特定领域任务的数学数据集 [12]。本章使用 AdamW 优化器 [27] 进行训练，学习率设置为1e-4，批量大小根据模型规模和可用内存进行调整。
\subsection{性能比较}
图 7 展示了 mTuner 与 Deepspeed 和 Megatron 在不同模型规模上的吞吐量比较。结果表明，mTuner 在所有模型规模上都显著优于 Deepspeed 和 Megatron。在 7B 模型上，mTuner 的吞吐量比 Deepspeed 高 1.23 倍，比 Megatron 高 1.18 倍。在 13B 模型上，mTuner 的吞吐量比 Deepspeed 高 1.35 倍，比 Megatron 高 1.27 倍。在 30B 模型上，mTuner 的吞吐量比 Deepspeed 高 1.42 倍，比 Megatron 高 1.34 倍。在 70B 模型上，mTuner 的吞吐量比 Deepspeed 高 1.51 倍，比 Megatron 高 1.45 倍。平均而言，mTuner 的吞吐量比 Deepspeed 高 1.28 倍，比 Megatron 高 1.22 倍。
这些结果表明，mTuner 能够有效地利用弹性张量来优化内存使用和减少通信开销，从而显著提高微调性能。通过动态调整权重和激活的内存使用量，mTuner 能够更好地适应不同模型规模和内存状态，实现更高的训练效率。
\subsection{内存使用分析}
图 8 展示了 mTuner 与 Deepspeed 和 Megatron 在 13B 模型上的内存使用情况比较。结果表明，mTuner 能够更有效地利用内存。在峰值内存使用方面，mTuner 比 Deepspeed 低 23\%，比 Megatron 低 18\%。这是因为 mTuner 通过调整批次大小和采用激活重计算技术，降低了激活的峰值内存使用量。在平均内存使用方面，mTuner 比 Deepspeed 高 12\%，比 Megatron 高 8\%。这是因为 mTuner 在内存相对充裕时，通过增加弹性权重大小来减少通信量，从而提高了内存利用率。
这些结果表明，mTuner 在内存管理方面具有优势。通过灵活调整权重和激活的内存使用量，mTuner 能够在保证不超过设备内存限制的前提下，充分利用空闲内存，提高内存利用率，同时降低峰值内存使用量，避免内存不足的问题。
\subsection{特定领域微调}
本章还评估了 mTuner 在特定领域微调任务中的性能。本章使用 Llama-2 13B 模型在数学数据集 [12] 上进行微调，以增强其数学能力。图 9 展示了 mTuner 与 Deepspeed 和 Megatron 在数学数据集上的微调时间比较。结果表明，mTuner 能够显著缩短微调时间。mTuner 在 3 小时内完成了微调，而 Deepspeed 需要 4.5 小时，Megatron 需要 5 小时。
这些结果表明，mTuner 不仅在通用语言任务的微调中表现出色，而且在特定领域任务的微调中也具有显著优势。通过提高微调性能，mTuner 能够更快地使模型适应特定领域，提高模型在特定领域任务中的表现。
\section{结论}
本章提出了 mTuner，这是一个利用弹性张量在多 GPU 服务器上加速参数高效微调的系统。通过对 PEFT 中内存使用情况的详细分析，发现权重和激活的内存都可以进行动态高效的调整。基于此，提出了弹性张量的概念，使张量的大小能够在运行时动态灵活地改变，从而提高内存利用率。
基于弹性张量构建了 mTuner，它能够感知内存状态并制定自适应内存执行计划。实验结果表明，与最先进的训练系统相比，mTuner 在各种规模的大语言模型上实现了显著的性能提升，吞吐量最高提升可达 1.51 倍（平均提升 1.28 倍）。mTuner 还能够在 3 小时内对 Llama - 2 13B 模型进行微调，以增强其数学能力。
未来，计划进一步探索如何将 mTuner 应用于更广泛的模型和任务，并研究如何在不同的硬件平台上优化 mTuner 的性能，还将研究如何将 mTuner 与其他先进的训练技术相结合，以进一步提高微调效率。

% \section{论文的语言及表述}

% 除国际研究生外，学位论文一律须用汉语书写。
% 学位论文应当用规范汉字进行撰写，除古汉语研究中涉及的古文字和参考文献中引用的外文文献之外，均采用简体汉字撰写。

% 国际研究生一般应以中文或英文书写学位论文，格式要求同上。
% 论文须用中文封面。

% 研究生学位论文是学术作品，因此其表述要严谨简明，重点突出，专业常识应简写或不写，做到立论正确、数据可靠、说明透彻、推理严谨、文字凝练、层次分明，避免使用文学性质的或带感情色彩的非学术性语言。

% 论文中如出现一个非通用性的新名词、新术语或新概念，需随即解释清楚。



% \section{论文题目的写法}

% 论文题目应简明扼要地反映论文工作的主要内容，力求精炼、准确，切忌笼统。
% 论文题目是对研究对象的准确、具体描述，一般要在一定程度上体现研究结论，因此，论文题目不仅应告诉读者这本论文研究了什么问题，更要告诉读者这个研究得出的结论。
% 例如：“在事实与虚构之间：梅乐、卡彭特、沃尔夫的新闻观”就比“三个美国作家的新闻观研究”更专业、更准确。



% \section{摘要的写法}

% 论文摘要是对论文研究内容的高度概括，应具有独立性和自含性，即应是 一篇简短但意义完整的文章。
% 通过阅读论文摘要，读者应该能够对论文的研究 方法及结论有一个整体性的了解，因此摘要的写法应力求精确简明。
% 论文摘要 应包括对问题及研究目的的描述、对使用的方法和研究过程进行的简要介绍、 对研究结论的高度凝练等，重点是结果和结论。

% 论文摘要切忌写成全文的提纲，尤其要避免“第 1 章……；第 2 章……；……”这样的陈述方式。



% \section{引言的写法}

% 一篇学位论文的引言大致包含如下几个部分：
% 1、问题的提出；
% 2、选题背 景及意义；
% 3、文献综述；
% 4、研究方法；
% 5、论文结构安排。
% \begin{itemize}
%   \item 问题的提出：要清晰地阐述所要研究的问题“是什么”。
%     \footnote{选题时切记要有“问题意识”，不要选不是问题的问题来研究。}
%   \item 选题背景及意义：论述清楚为什么选择这个题目来研究，即阐述该研究对学科发展的贡献、对国计民生的理论与现实意义等。
%   \item 文献综述：对本研究主题范围内的文献进行详尽的综合述评，“述”的同时一定要有“评”，指出现有研究状态，仍存在哪些尚待解决的问题，讲出自己的研究有哪些探索性内容。
%   \item 研究方法：讲清论文所使用的学术研究方法。
%   \item 论文结构安排：介绍本论文的写作结构安排。
% \end{itemize}



% \section{正文的写法}

% 本部分是论文作者的研究内容，不能将他人研究成果不加区分地掺和进来。
% 已经在引言的文献综述部分讲过的内容，这里不需要再重复。
% 各章之间要存在有机联系，符合逻辑顺序。



% \section{结论的写法}

% 结论是对论文主要研究结果、论点的提炼与概括，应精炼、准确、完整，使读者看后能全面了解论文的意义、目的和工作内容。
% 结论是最终的、总体的结论，不是正文各章小结的简单重复。
% 结论应包括论文的核心观点，主要阐述作者的创造性工作及所取得的研究成果在本领域中的地位、作用和意义，交代研究工作的局限，提出未来工作的意见或建议。
% 同时，要严格区分自己取得的成果与指导教师及他人的学术成果。

% 在评价自己的研究工作成果时，要实事求是，除非有足够的证据表明自己的研究是“首次”、“领先”、“填补空白”的，否则应避免使用这些或类似词语。
