\chapter{相关工作}


在人工智能系统的发展进程中，为应对日益增长的计算需求与复杂的应用场景，研究者们在算子编译优化、内存管理优化和分布式通信优化等关键领域展开了广泛且深入的探索。

\section{人工智能算子编译优化相关工作}

随着神经网络模型的复杂度不断攀升，从早期的AlexNet、VGG到如今的GPT系列、BERT及其变体，模型参数量和计算量呈指数级增长，传统的编译器难以满足人工智能领域对高效计算的需求。人工智能算子编译优化旨在针对深度学习特定的计算模式，如张量运算、卷积操作、注意力机制等，对算子进行深度优化，以充分发挥硬件的计算潜力。

% 当前深度学习框架针对张量程序的优化主要聚焦于两个维度：
% \begin{enumerate}
%     \item **算子生成优化**：致力于设计并实现更高性能的算子。此过程充分考量硬件架构特性，通过优化算法和内存访问模式，实现算子层面的性能提升。
%     \item **计算图优化**：通过对计算图中的算子进行等价变换，以提升整体计算效率。例如，将复杂的数学表达式简化为更高效的形式，或是将多个算子进行融合操作。
% \end{enumerate}

\subsection{算子库}

早期的工作主要依赖于硬件厂商提供的计算库，如英伟达的CuDNN \cite{chetlur2014cudnn}。
CuDNN对常见的深度学习算法进行了高度优化，利用GPU硬件的并行性和向量化特性，通过半精度浮点数运算、算法重排等技术加速计算。
以卷积运算为例，CuDNN通过优化卷积核的滑动窗口计算方式，减少不必要的内存访问，大幅提升了卷积操作的效率。
但这类方法存在局限性，面对人工智能领域快速迭代的算子，如Transformer中的多头注意力机制、复杂的自定义算子等，更新跟进困难，且在多平台移植时易出现一致性问题。
例如，将基于CuDNN优化的模型从NVIDIA GPU迁移到AMD GPU或其他人工智能加速芯片时，往往需要重新适配和调整。
FlashAttention~\cite{dao2022flashattention, dao2023flashattention, shah2024flashattention} 和 Flashinfer~\cite{ye2025flashinfer} 为注意力机制提供了快速内核，将所有算子融合为单个内核以最小化内存访问开销。
对于更复杂的注意力变体算子，FlashAttention 这类方法同样会融合所有算子，但这会因为数据依赖，导致融合后并行性不佳和硬件利用率低下，从而产生次优的性能。
% 对于像 TVM~\cite{chen2018tvm}、Triton~\cite{tillet2019triton}、FreeTensor~\cite{tang2022freetensor} 和 FlexAttention~\cite{flexattention} 这样的领域特定语言，它们支持自定义代码生成，但需要专业的编程知识。 

\subsection{人工智能编译器}

近年来，学术界和工业界纷纷致力于开发新型的人工智能编译器。
现有的深度学习框架通常将深度学习程序建模为计算图结构，该结构由张量(Tensor)和算子(Operator)构成。
主流的人工智能编译器在高效算子生成之外，都非常关注计算图优化。
主要的计算图层编译优化技术如下：
\begin{itemize}
\item 基于规则的替换优化：TensorFlow~\cite{abadi2016tensorflow}、TensorRT~\cite{tensorrt} 和 TVM~\cite{chen2018tvm} 等框架采用基于规则的策略实现计算图优化。
具体而言，这类优化涵盖数学表达式的等价化简，例如将冗余的乘法和加法运算合并；激活函数与基础计算算子(如卷积、池化)的融合操作，减少中间数据的存储和传输开销；以及逐元素计算算子的融合，通过将多个连续的逐元素操作合并为单个算子执行，降低计算过程中的调度开销。这些优化策略由框架开发者预先定义，在保证计算结果正确性的前提下，能够显著提升模型的执行效率。
\item 扩展优化空间的探索：MetaFLow~\cite{MetaFlow} 突破了传统优化思路的局限，允许将可能导致性能暂时下降的计算图变换作为中间步骤纳入优化搜索过程。例如，在某些情况下，将计算图 $G_1$ 变换为 $G_2$ 时，虽然 $G_2$ 的性能相较于 $G_1$ 有所降低，但通过进一步将 $G_2$ 变换为 $G_3$，最终可以实现 $G_3$ 相对于 $G_1$ 的性能提升。这种方法通过引入 “性能下降” 的中间状态，极大地扩展了优化搜索空间，为发现更优的计算图结构提供了可能。
\item 自动化图替换生成：TASO~\cite{jia2019taso} 提出了一种自动化的计算图优化方法，其核心在于根据基础运算符的属性自动生成计算图替换方案。TASO 首先通过穷举搜索算法，将原始计算图遍历变换为算子数量不超过特定阈值的新计算图(该过程不强制保证变换后的计算图与原始图等价)。随后，利用基于规则的验证器对生成的所有计算图进行验证，仅保留在数学上与原始图等价的计算图。通过这种方式，TASO 能够系统地探索所有数学等价的计算图变换空间，减少人工设计优化规则的局限性，显著提升了计算图优化的自动化程度和优化效果。类似这种方法，PET\cite{wang2021pet}、EinNet\cite{zheng2023einnet}等工作继续基于不等价变换和基于表达式变换的方式，进一步开展图层优化。
\end{itemize}


% 近年来，学术界和工业界纷纷致力于开发新型的人工智能编译器。
% 微软亚洲研究院提出的Rammer \cite{rammer}，为DNN生成有效的静态时空调度，减少调度开销。
% Rammer通过引入新的与硬件无关的抽象，将深度学习计算图中的算子进行重新组织和调度，实现了算子间和算子内的协同调度，全面利用并行性。这一成果显著提升了深度学习模型在不同硬件平台上的计算效率，为多硬件环境下的高效部署提供了新的思路。

% Roller \cite{rammer}则聚焦于内存特性，通过估算张量切割的最佳方法和大小，实现快速编译。在深度学习计算中，张量的存储和访问对计算性能影响巨大，Roller通过分析张量的形状、数据类型以及计算依赖关系，在几秒内即可生成高度优化的内核，编译时间较现有编译器有三个数量级的改进。这使得模型在训练和推理阶段能够更快地完成编译过程，尤其适用于快速迭代的模型开发场景。

% Welder \cite{rammer}针对现代DNN模型对高速内存的高要求，通过链接不同算子，以流水线方式处理数据块，大幅降低访存量，有效提升计算效率。在实际的深度学习任务中，频繁的内存访问会严重制约计算速度，Welder通过优化算子执行顺序和数据流动方式，减少了数据在内存和计算单元之间的传输次数，从而提高了整体计算性能。


% 例如，TVM [1] 提出了一种统一的中间表示(IR)，能够对不同的深度学习框架和硬件平台进行优化。它通过对计算图进行调度和优化，生成高效的代码。TensorRT [2] 是 NVIDIA 推出的一款深度学习推理优化器，它通过对模型进行层融合、精度校准等操作，减少计算量和内存访问开销，提高推理速度。然而，这些方法主要关注操作符层面的优化，对张量属性的利用不够充分，在处理复杂的长上下文任务时，优化效果有限。

% 此外，有一些研究涉及张量的属性，大多集中在粗粒度的张量属性，如张量的形状和大小。例如，一些工作通过调整张量的布局来减少内存访问开销 [3]，但这种方法没有考虑到张量的细粒度属性，如规约依赖和广播能力。还有一些研究在特定的操作符或模型中利用张量的某些属性进行优化 [4]，但缺乏系统性和通用性，无法应用于广泛的 DNN 模型。

% 在将计算图映射到硬件内核执行方面，也有不少研究。传统的方法通常将计算图中的操作符映射到凸内核中执行 [5]，这种方式在处理简单的计算图时具有一定的有效性，但在面对复杂的计算图结构时，由于其对操作符依赖关系的严格限制，往往无法充分发挥硬件的性能。
% 一些研究尝试放宽内核的定义，提出了非凸内核的概念 [6]，以提高操作符分配的灵活性。然而，这些研究大多没有结合张量属性进行内核映射，无法根据张量的特点选择最优的内核执行方案。

% \subsection{特定架构下的优化}

% 此外，针对存算一体芯片架构的多样性，CIM-MLC \cite{cim_mlc}提出了多层次开源编译框架。存算一体芯片将存储和计算单元紧密结合，打破了传统冯・诺依曼架构的存储墙限制。CIM-MLC通过图层级、矩阵计算、向量计算层级的调度优化，充分利用存算一体架构资源。在处理大规模图像数据时，CIM-MLC能够根据存算一体芯片的特点，将图像数据的存储和计算过程进行深度融合，提高数据处理效率。

% Souffle \cite{cim_mlc}结合传统编译分析技术和DNN应用特征，在多个层级分析依赖关系并进行张量表达式变换。Souffle通过对深度学习计算图的深入分析，将复杂的张量运算转化为更高效的计算形式，相比厂商深度优化的TensorRT，获得平均3.7的加速比。这一成果展示了在现有硬件基础上，通过编译优化提升模型性能的巨大潜力。

% 针对动态形状场景，MikPoly \cite{cim_mlc}提出两阶段人工智能算子编译优化方法。在自然语言处理、视频处理等场景中，输入数据的形状往往是动态变化的，MikPoly通过第一阶段的动态形状分析和第二阶段的针对性优化，在GPU、NPU等平台实现了平均1.49倍的性能提升，有效解决了动态形状数据处理效率低下的问题。

% 一些工作通过优化计算图来提高 DNN 的执行效率。例如，TVM [1] 提出了一种统一的中间表示(IR)，能够对不同的深度学习框架和硬件平台进行优化。它通过对计算图进行调度和优化，生成高效的代码。TensorRT [2] 是 NVIDIA 推出的一款深度学习推理优化器，它通过对模型进行层融合、精度校准等操作，减少计算量和内存访问开销，提高推理速度。然而，这些方法主要关注操作符层面的优化，对张量属性的利用不够充分，在处理复杂的长上下文任务时，优化效果有限。
% \subsection{张量属性相关研究}
% 虽然也有一些研究涉及张量的属性，但大多集中在粗粒度的属性，如张量的形状和大小。例如，一些工作通过调整张量的布局来减少内存访问开销 [3]，但这种方法没有考虑到张量的细粒度属性，如规约依赖和广播能力。还有一些研究在特定的操作符或模型中利用张量的某些属性进行优化 [4]，但缺乏系统性和通用性，无法应用于广泛的 DNN 模型。
% 与现有研究不同，FlashTensor 系统地总结了多种关键的张量属性，并提出了一种通用的方法来识别和利用这些属性进行优化。通过考虑细粒度的张量属性，FlashTensor 能够更深入地分析 DNN 模型的计算特点，从而实现更高效的优化。
% \subsection{内核映射相关研究}
% 在将计算图映射到硬件内核执行方面，也有不少研究。传统的方法通常将计算图中的操作符映射到凸内核中执行 [5]，这种方式在处理简单的计算图时具有一定的有效性，但在面对复杂的计算图结构时，由于其对操作符依赖关系的严格限制，往往无法充分发挥硬件的性能。
% 一些研究尝试放宽内核的定义，提出了非凸内核的概念 [6]，以提高操作符分配的灵活性。然而，这些研究大多没有结合张量属性进行内核映射，无法根据张量的特点选择最优的内核执行方案。

\section{人工智能系统内存管理优化相关工作}

内存管理在人工智能系统中至关重要，尤其是随着模型规模的增大和数据处理需求的增长，高效的内存管理成为提升系统性能的关键因素。从早期的小型神经网络到如今参数量达数百亿甚至数千亿的大模型，内存管理面临的挑战越来越大。

% \subsection{传统内存管理方式}

% 传统的内存管理方式在面对深度学习任务时存在不足，例如在处理大规模神经网络时，难以有效分配和回收内存资源，容易导致内存碎片和内存泄漏。
% 早期尝试通过优化内存分配算法来提高内存利用率，如采用伙伴系统(Buddy System)等经典算法，对内存进行分层管理，减少内存碎片的产生 \cite{buddy_system}。
% 伙伴系统将内存划分为不同大小的块，当需要分配内存时，寻找最适合的块进行分配，释放内存时，尝试合并相邻的空闲块。
% 然而，在深度学习场景下，模型结构复杂且动态变化，张量的创建、销毁和形状变化频繁，传统算法无法充分适应这种特性。例如，在循环神经网络(RNN)的训练过程中，随着时间步的推进，张量的形状和数量不断变化，伙伴系统难以快速、高效地进行内存分配和回收。

% \subsection{深度学习特定的内存管理优化}

% 近年来，随着深度学习模型的发展，出现了一些专门针对深度学习的内存管理优化方法。
% 一些研究提出动态内存分配策略，根据模型计算过程中张量的生命周期，动态分配和释放内存 \cite{dynamic_memory}。
% 例如，在模型的前向传播过程中，根据每层计算所需的张量大小和存活时间，动态分配内存空间，在前向传播结束后，及时释放不再使用的张量内存，避免内存浪费。
% 还有研究通过对模型计算图的分析，预测张量的内存需求，提前进行内存分配，避免运行时的内存分配开销 \cite{memory_prediction}。
% 通过对计算图的静态分析，获取张量的创建、使用和销毁信息，构建内存需求预测模型，在训练或推理开始前，预先分配足够的内存，减少运行时的内存分配延迟。

% 在内存复用方面，一些工作通过优化计算图的执行顺序，实现张量内存的复用。
% 例如，通过对神经网络计算图进行拓扑排序，找到可以复用内存的张量节点，在不影响计算结果的前提下，减少内存占用 \cite{memory_reuse}。
% 在一些复杂的神经网络结构中，某些中间层的张量在计算完成后，后续计算不再使用，此时可以将这些张量的内存空间复用给其他需要的张量，从而提高内存利用率。


\subsection{静态内存管理}
现有的方法通过利用并行训练技术来静态管理内存，如权重参数和优化器状态分配。DeepSpeed~\cite{singh2023deepspeed-ted,rasley2020DeepSpeed,raj2021zeroinfinity} 引入了 ZeRO 方法，该方法通过在设备间划分静态张量或将其卸载到 CPU 内存来增强数据并行性。
为了减轻通信开销，后续的研究~\cite{Centauri,better-together} 改进了重叠机制，从而有效地隐藏通信延迟。
Gpipe~\cite{huang2019gpipe}、Pipedream~\cite{narayanan2019pipedream} 和 MPress~\cite{zhou2023mpress} 采用流水线并行，在设备间按层划分权重，并实现具有最小通信开销的流水线并行执行。然而，这种方法容易产生流水线气泡，导致设备闲置。
ZeroBubble~\cite{zero-bubble} 通过调整流水线调度消除了气泡，但累积的激活值仍然会造成巨大的内存压力。
张量并行~\cite{krothikanti2022megatronv3,flux,transformer_engine} 采用了不同的方法，即沿着额外的维度划分权重，使多个设备能够协同处理同一样本。像Megatron~\cite{shoeybi2019megatron,krothikanti2022megatronv3} 和 Alpa~\cite{zheng2022alpa,zhuang2022alpacomm} 这样的系统，通过手动或自动搜索的方式结合数据并行、流水线并行和张量并行，以确定最优的并行策略。
尽管做出了这些努力，但现有方法主要侧重于通过并行性进行静态内存优化，在很大程度上忽略了运行时张量带来的动态内存限制和利用率问题。因此，它们缺乏自适应、细粒度的内存优化机制。

\subsection{运行时内存管理}
运行时内存管理主要使用检查点和卸载技术。
激活值检查点~\cite{cybertronai_gradient_checkpointing} 是优化激活内存的重要技术。
该技术最早在\cite{chen2016recomp} 中提出，并在 Megatron~\cite{krothikanti2022megatronv3} 和 MPress~\cite{zhou2023mpress} 等训练框架中得到广泛应用。
有研究~\cite{jain2020checkmate} 尝试结合模型结构寻找最优的检查点策略。
然而，这些激活值优化方法与静态张量优化相互独立。 







\section{人工智能系统分布式通信优化相关工作}

随着人工智能模型规模的不断扩大，单节点计算资源已难以满足训练需求，分布式训练成为主流解决方案。以GPT-3、PaLM等超大规模模型为例，其训练需要成百上千个计算节点协同工作。
然而，分布式训练中的通信开销严重制约了系统的可扩展性，因此分布式通信优化成为研究热点。

\subsection{参数服务器}

早期的分布式训练框架在通信方面采用较为简单的策略，如参数服务器(Parameter Server)架构，通过中心节点(参数服务器)来协调各计算节点之间的参数更新 \cite{li2014communication}。
在这种架构下，计算节点负责进行模型训练并计算梯度，然后将梯度发送到参数服务器，参数服务器汇总梯度并更新模型参数，再将更新后的参数下发给计算节点。
但这种架构在大规模分布式训练中存在通信瓶颈，随着节点数量的增加，中心节点的负载急剧上升，导致通信延迟增大。
当计算节点数量达到数百甚至上千个时，参数服务器可能成为整个系统的性能瓶颈，无法及时处理大量的梯度更新请求，严重影响训练效率。

\subsection{其它通信优化技术}

为解决上述问题，研究者们提出了多种优化方案。
计算和通信重叠技术成为重要的优化手段之一，传统的计算和通信串行方式导致通信开销较大，而将计算和通信重叠起来，可有效减少通信等待时间 \cite{flux,zhai2023smartmoe, wang2024hiding}。
例如，在梯度计算过程中，同时进行部分梯度的通信传输，使计算资源得到更充分的利用。
当计算节点在计算某一层的梯度时，可以将已经计算完成的前几层梯度先发送出去，而无需等待整个梯度计算完成后再进行通信，这样可以在不增加额外计算资源的情况下，提高系统的整体效率。

梯度压缩技术通过对传输的梯度进行压缩，减少数据传输量\cite{jiasdp4bit, feng2024accelerating}。
例如，将梯度从FP32压缩到FP16，并建立相应的数据缩放机制，在保证计算精度的前提下，降低通信带宽需求。
除了精度压缩，还有一些研究采用稀疏化技术，只传输梯度中绝对值较大的部分，忽略较小的梯度值，进一步减少数据传输量。

分级通信优化则针对传统环形通信方式在以太网通信上的性能限制，通过在节点内部和节点之间进行分级规约通信，减少以太网上传输的数据量，并通过流水线方式将节点内部和节点之间的规约通信重叠起来，进一步减少整体通信时间 \cite{he2021fastmoe}。
在分级通信架构中，首先在节点内部进行局部梯度规约，然后将规约后的结果在节点之间进行通信和汇总，这种方式可以有效减少网络传输压力，提高通信效率。

% \subsection{工业界的创新实践}

% 阿里云的E人工智能S软件池化技术通过虚拟网卡技术将异构计算加速实例挂载到前端ECS上，在推理前端服务和推理后端服务之间建立高速安全通信通道，优化了分布式通信流程，提升了人工智能推理计算效率 \cite{e人工智能s}。E人工智能S技术通过对网络通信的深度优化，实现了异构计算资源的高效协同工作，在实际应用中，为大规模人工智能推理任务提供了稳定、高效的通信支持。同时，一些研究通过优化网络拓扑结构、采用高速网络设备等方式，进一步降低通信延迟，提高分布式训练的整体性能 \cite{network_topology}。例如，采用胖树(Fat-Tree)拓扑结构可以提供更高的网络带宽和更好的容错性，使用InfiniBand等高速网络设备可以显著降低数据传输延迟，为分布式训练创造更有利的通信环境。

综上所述，在算子编译优化、内存管理优化和分布式通信优化等方面，已有大量的研究工作取得了显著进展。但随着人工智能技术的不断发展，新的模型结构和应用场景不断涌现，如多模态大模型、边缘计算场景下的人工智能应用等，这些领域仍面临诸多挑战，需要进一步深入研究和探索。

% \begin{thebibliography}{99}
% \bibitem{cudnn}
%   NVIDIA. \textit{CUDA Deep Neural Network library (CuDNN)}.
%   \url{https://developer.nvidia.com/cudnn}

% \bibitem{eigen}
%   Eigen. \textit{Eigen: A C++ template library for linear algebra}.
%   \url{http://eigen.tuxfamily.org}

% \bibitem{rammer}
%   Microsoft Research. \textit{Rammer, Roller, and Welder: Compiler Techniques for High-Performance DNNs}.
%   \url{https://www.microsoft.com/en-us/research/blog/rammer-roller-and-welder-compiler-techniques-for-high-performance-dnns/}

% \bibitem{cim_mlc}
%   Authors. \textit{Title of the paper about CIM-MLC}.
%   \textit{Conference Name}, year.

% \bibitem{buddy_system}
%   Knuth, D. E. \textit{The Art of Computer Programming, Volume 1: Fundamental Algorithms}.
%   Addison-Wesley, 1968.

% \bibitem{dynamic_memory}
%   Authors. \textit{Title of the paper about dynamic memory allocation in DL}.
%   \textit{Journal Name}, year.

% \bibitem{memory_prediction}
%   Authors. \textit{Title of the paper about memory prediction in DL}.
%   \textit{Conference Name}, year.

% \bibitem{memory_reuse}
%   Authors. \textit{Title of the paper about memory reuse in DL}.
%   \textit{Journal Name}, year.

% \bibitem{distributed_memory}
%   Authors. \textit{Title of the paper about distributed memory management in DL}.
%   \textit{Conference Name}, year.

% \bibitem{parameter_server}
%   Li, M., et al. \textit{Scaling Distributed Machine Learning with the Parameter Server}.
%   \textit{OSDI}, 2014.

% \bibitem{compute_communication_overlap}
%   Authors. \textit{Title of the paper about compute-communication overlap}.
%   \textit{Conference Name}, year.

% \bibitem{decentralized_gradient}
%   Authors. \textit{Title of the paper about decentralized gradient negotiation}.
%   \textit{Journal Name}, year.

% \bibitem{gradient_compression}
%   Authors. \textit{Title of the paper about gradient compression}.
%   \textit{Conference Name}, year.

% \bibitem{hierarchical_communication}
%   Authors. \textit{Title of the paper about hierarchical communication optimization}.
%   \textit{Journal Name}, year.

% \bibitem{gradient_fusion}
%   Authors. \textit{Title of the paper about gradient fusion optimization}.
%   \textit{Conference Name}, year.

% \bibitem{e人工智能s}
%   Alibaba Cloud. \textit{E人工智能S: Elastic 人工智能 Service}.
%   \url{https://www.aliyun.com/product/e人工智能s}

% \bibitem{network_topology}
%   Authors. \textit{Title of the paper about network topology optimization for distributed tr人工智能ning}.
%   \textit{Conference Name}, year.
% \end{thebibliography}

% \end{document}


% \section{大语言模型}















% 7.1 大语言模型训练优化
% 许多研究致力于优化大语言模型的训练过程。数据并行 [13, 24, 31] 是一种常见的方法，它将训练数据划分为多个部分，每个部分在不同的设备上进行训练。张量并行 [2, 5, 22] 通过将模型张量划分为多个部分并在不同设备上并行计算，减少了通信开销。流水线并行 [4, 25, 28] 将模型划分为多个阶段，每个阶段在不同设备上执行，从而提高了计算资源的利用率。
% Deepspeed [33] 是一个流行的训练框架，它结合了数据并行和零冗余优化(ZeRO)方法，以减少内存使用和提高训练效率。Megatron [20] 集成了数据并行、张量并行和流水线并行，为训练大型模型提供了高效的解决方案。然而，这些方法主要关注全参数训练，对于参数高效微调的优化效果有限。
% 7.2 参数高效微调
% 参数高效微调(PEFT)近年来受到了广泛关注。LoRA [14] 通过在模型中添加低秩适配器来减少可训练参数的数量。Prefix - Tuning [42] 通过优化前缀参数来调整模型的输出。Prompt - Tuning [29] 通过设计特定的提示来引导模型生成期望的输出。这些方法主要关注如何减少可训练参数的数量和提高微调性能，但它们没有充分考虑微调过程中的内存使用和通信开销问题。
% 7.3 内存优化技术
% 内存优化技术在深度学习训练中也得到了广泛研究。激活重计算 [17, 18, 19] 是一种常用的技术，它通过重新计算激活而不是存储它们来减少内存使用。内存池化 [3, 21] 通过复用内存来提高内存利用率。然而，这些技术通常是独立应用的，没有考虑权重和激活之间的相互作用，也没有根据内存状态进行动态调整。


% \section{大模型后训练}

% \subsection{大模型微调}

% 大语言模型微调是指在预训练模型的基础上，基于少量特定领域数据，赋予大语言模型特定领域知识的过程。微调在大语言模型应用于各个行业和领域中具有重要意义。常见的微调方法可分为两类。

% 第一类是全参数微调，其过程类似于预训练。如图 2(b)所示，所有权重参数均可训练，并且在反向传播过程中需要进行更新。因此，对于每个样本，全参数微调的计算成本与预训练相同，需要相同大小的梯度和优化器状态。所以，全参数微调不适合低成本的微调，因为它会产生与预训练相同的内存和每个样本的计算成本。由于微调通常在成本较低的硬件资源上进行，这会导致微调效率低下。此外，全参数微调会更新所有参数，存在破坏或削弱预训练模型原有能力的潜在风险。最后，全参数微调还存在一定的可移植性问题，因为微调后的新模型参数与巨大的预训练模型相当，例如 700 亿参数。

% 第二类是参数高效微调(PEFT)，同样基于预训练模型展开。与全参数微调不同，PEFT 不会改变预训练模型的参数，而是在模型结构中引入少量称为适配器的可训练参数。如图 2(c)所示，在训练过程中，预训练模型(基础模型)的参数参与计算，但不会被更新；只有适配器的参数根据损失和优化器状态进行更新。与全参数微调相比，PEFT 具有以下优势：1)显著降低了优化器状态的内存需求，仅需适配器的优化器状态，无需基础模型的优化器状态；2)由于在反向传播过程中无需为大部分参数计算梯度，节省了计算资源；3)由于微调后仅适配器的参数发生变化，因此仅保存适配器的参数(以及原始基础模型的参数)即可构成微调后的模型，便于传播和服务。因此，PEFT 已成为当前微调实践中的常用方法，也是本文的主要研究对象。




% 大语言模型对齐：大语言模型对齐在 ChatGPT [19]、LLaMA2 [26] 和 GPT-4 [20] 等对话式人工智能模型的发展中起着关键作用，其核心在于利用这些模型强大的知识和能力，生成特定的、以用户为导向的响应和行为。这一概念对于确保这些模型具备安全性、有效性和可管理性至关重要。需要明确的是，单纯扩大这些语言模型的规模并不能自动使其更好地契合用户意图。已有实例表明，无论大语言模型规模大小，都可能生成具有误导性、有害或对用户无实际帮助的输出。现有的人类偏好对齐方法大致可分为三大类：强化学习 [21, 24]、对比学习 [22,34] 和事后指令重标记 [14,33]。在这些方法中，强化学习方法是实现对齐的主要手段。在本研究中，我们聚焦于 PPO 算法，它是 RLHF [4] 中最为有效的方法，能够满足高效对齐的迫切需求。在 RLHF 中，该过程主要包括三个步骤：收集人类反馈、构建奖励模型以及采用 RL 优化策略 [21]。本文主要关注最后一个步骤。如图 1(c)所示为 InstructGPT 实施的 RL 优化策略步骤示例，该过程涉及 Actor、Ref、Critic 和 RM 等多个模型，同时涵盖了解码、推理和训练等多种工作负载。Actor 和 Ref 的结构通常相似，其中 Actor 的参数可训练，而 Ref 的参数保持冻结状态；同样，Critic 和 RM 具有相同的结构，且 Critic 的参数可训练。 

% 混合并行化：拥有数百亿参数的大语言模型无法在单个设备上高效地进行训练或推理，因此需要并行执行方案，而权重的布局决定了这些方案，包括并行计算和通信策略。数据并行、张量并行和流水线并行是分布式训练中常用的三种方案。数据并行是指在所有设备上复制模型参数(DP)或在设备间进行参数分片(FSDP)，然后为每个设备分配不同的训练数据批次，设备独立进行前向和反向传播计算，通信过程涉及梯度聚合或共享参数收集。张量并行同样需要在设备间共享参数和数据，每个工作节点负责部分算子计算，必须通过通信聚合输出以获得最终结果。流水线并行则是将模型顺序划分为多个部分，每个设备维护模型的特定部分，对于模型的某一部分，相应设备处理来自上一阶段(或训练数据)的数据，其输出需要传输到维护模型后续部分的设备。虽然流水线并行通常通信量较少，但面临 “流水线气泡” 问题，此前已有大量研究对此展开探讨 [5,17]。混合并行化旨在提升分布式训练性能，它整合了多种并行策略，以适应特定模型和独特的训练硬件配置。值得一提的是，Megatron-v2 [18] 通过精心设计的混合并行执行方案，为基于 Transformer 的模型实现了高性能分布式训练。鉴于混合并行化的广泛应用 [35]，本研究着重探究其在大语言模型对齐中的效率。 

% 多模型执行计划：大语言模型对齐涉及多种具有异构上下文的模型，每个模型对计算资源的需求各不相同。因此，这些模型在系统中的布局和执行方式对于实现最优性能至关重要。如图 2 所示，现有策略可分为三类。在该图中展示了三个不同模型的训练过程，包括前向和反向传播。需要注意的是，尽管上述示例仅展示了流水线并行，但 PUZZLE 并不局限于此，同样适用于数据并行和张量并行。 
% 逐个模型(MBM)：MBM 是大语言模型对齐中一种直观的模型布局和执行方法(如图 2(a)所示)，其中模型具有相同的设备设置，并按顺序执行。现有系统 [12, 31] 采用此方案。然而，尽管该方案简单实用，但存在局限性：模型在运行时占用所有资源，且仅支持顺序执行；此外，如果工作负载无法有效并行化，该方案可能导致效率低下。 
% 单模型专用设备(SMDD)：SMDD 是另一种常见的模型布局和执行方法(如图 2(b)所示)，该方法涉及预先为不同模型分配设备，确保每个模型拥有专用设备以实现并发执行。尽管在 [16] 等研究中采用了此方法，但 SMDD 面临诸多挑战，其资源分配策略复杂且容易导致效率低下。例如，如果模型 1 必须等待模型 2 完成执行，这将导致某些设备出现空闲时间，进而降低资源利用率。 
% 多模型专用设备(MMDD)：MMDD 整合了上述 MBM 和 SMDD 方法(如图 2(c)所示)，该方案将部分模型分配到相同资源，同时为其他模型分配不同资源，从而使某些模型能够并行执行，而分配到相同资源的模型则顺序执行。现有研究(如 [7])采用了此方案。然而，尽管 MMDD 解决了 MBM 和 SMDD 的部分局限性，但我们发现它缺乏对细粒度调度的考量，而这对于有效管理异构上下文切换至关重要。 



% \subsection{DNN 优化方法}
% 在深度学习领域，已经有许多研究致力于 DNN 的优化。早期的研究主要集中在算法层面，如改进神经网络的结构、优化训练算法等，以提高模型的准确性和泛化能力。近年来，随着硬件性能的提升和 DNN 模型规模的不断扩大，更多的研究开始关注计算效率和资源利用的优化。
% 一些工作通过优化计算图来提高 DNN 的执行效率。例如，TVM [1] 提出了一种统一的中间表示(IR)，能够对不同的深度学习框架和硬件平台进行优化。它通过对计算图进行调度和优化，生成高效的代码。TensorRT [2] 是 NVIDIA 推出的一款深度学习推理优化器，它通过对模型进行层融合、精度校准等操作，减少计算量和内存访问开销，提高推理速度。然而，这些方法主要关注操作符层面的优化，对张量属性的利用不够充分，在处理复杂的长上下文任务时，优化效果有限。
% \subsection{张量属性相关研究}
% 虽然也有一些研究涉及张量的属性，但大多集中在粗粒度的属性，如张量的形状和大小。例如，一些工作通过调整张量的布局来减少内存访问开销 [3]，但这种方法没有考虑到张量的细粒度属性，如规约依赖和广播能力。还有一些研究在特定的操作符或模型中利用张量的某些属性进行优化 [4]，但缺乏系统性和通用性，无法应用于广泛的 DNN 模型。
% 与现有研究不同，FlashTensor 系统地总结了多种关键的张量属性，并提出了一种通用的方法来识别和利用这些属性进行优化。通过考虑细粒度的张量属性，FlashTensor 能够更深入地分析 DNN 模型的计算特点，从而实现更高效的优化。
% \subsection{内核映射相关研究}
% 在将计算图映射到硬件内核执行方面，也有不少研究。传统的方法通常将计算图中的操作符映射到凸内核中执行 [5]，这种方式在处理简单的计算图时具有一定的有效性，但在面对复杂的计算图结构时，由于其对操作符依赖关系的严格限制，往往无法充分发挥硬件的性能。
% 一些研究尝试放宽内核的定义，提出了非凸内核的概念 [6]，以提高操作符分配的灵活性。然而，这些研究大多没有结合张量属性进行内核映射，无法根据张量的特点选择最优的内核执行方案。FlashTensor 在非凸内核映射的基础上，充分考虑张量属性，通过综合评估计算强度、内存访问模式和并行性等因素，找到最优的内核映射方案，从而提高计算效率。
