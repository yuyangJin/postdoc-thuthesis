\chapter{相关工作}

\section{大语言模型}
7.1 大语言模型训练优化
许多研究致力于优化大语言模型的训练过程。数据并行 [13, 24, 31] 是一种常见的方法，它将训练数据划分为多个部分，每个部分在不同的设备上进行训练。张量并行 [2, 5, 22] 通过将模型张量划分为多个部分并在不同设备上并行计算，减少了通信开销。流水线并行 [4, 25, 28] 将模型划分为多个阶段，每个阶段在不同设备上执行，从而提高了计算资源的利用率。
Deepspeed [33] 是一个流行的训练框架，它结合了数据并行和零冗余优化（ZeRO）方法，以减少内存使用和提高训练效率。Megatron [20] 集成了数据并行、张量并行和流水线并行，为训练大型模型提供了高效的解决方案。然而，这些方法主要关注全参数训练，对于参数高效微调的优化效果有限。
7.2 参数高效微调
参数高效微调（PEFT）近年来受到了广泛关注。LoRA [14] 通过在模型中添加低秩适配器来减少可训练参数的数量。Prefix - Tuning [42] 通过优化前缀参数来调整模型的输出。Prompt - Tuning [29] 通过设计特定的提示来引导模型生成期望的输出。这些方法主要关注如何减少可训练参数的数量和提高微调性能，但它们没有充分考虑微调过程中的内存使用和通信开销问题。
7.3 内存优化技术
内存优化技术在深度学习训练中也得到了广泛研究。激活重计算 [17, 18, 19] 是一种常用的技术，它通过重新计算激活而不是存储它们来减少内存使用。内存池化 [3, 21] 通过复用内存来提高内存利用率。然而，这些技术通常是独立应用的，没有考虑权重和激活之间的相互作用，也没有根据内存状态进行动态调整。


\section{大模型后训练}

\subsection{大模型微调}

大语言模型微调是指在预训练模型的基础上，基于少量特定领域数据，赋予大语言模型特定领域知识的过程。微调在大语言模型应用于各个行业和领域中具有重要意义。常见的微调方法可分为两类。

第一类是全参数微调，其过程类似于预训练。如图 2（b）所示，所有权重参数均可训练，并且在反向传播过程中需要进行更新。因此，对于每个样本，全参数微调的计算成本与预训练相同，需要相同大小的梯度和优化器状态。所以，全参数微调不适合低成本的微调，因为它会产生与预训练相同的内存和每个样本的计算成本。由于微调通常在成本较低的硬件资源上进行，这会导致微调效率低下。此外，全参数微调会更新所有参数，存在破坏或削弱预训练模型原有能力的潜在风险。最后，全参数微调还存在一定的可移植性问题，因为微调后的新模型参数与巨大的预训练模型相当，例如 700 亿参数。

第二类是参数高效微调（PEFT），同样基于预训练模型展开。与全参数微调不同，PEFT 不会改变预训练模型的参数，而是在模型结构中引入少量称为适配器的可训练参数。如图 2（c）所示，在训练过程中，预训练模型（基础模型）的参数参与计算，但不会被更新；只有适配器的参数根据损失和优化器状态进行更新。与全参数微调相比，PEFT 具有以下优势：1）显著降低了优化器状态的内存需求，仅需适配器的优化器状态，无需基础模型的优化器状态；2）由于在反向传播过程中无需为大部分参数计算梯度，节省了计算资源；3）由于微调后仅适配器的参数发生变化，因此仅保存适配器的参数（以及原始基础模型的参数）即可构成微调后的模型，便于传播和服务。因此，PEFT 已成为当前微调实践中的常用方法，也是本文的主要研究对象。

2.2 权重与并行训练
在大语言模型中，每个权重都与一个算子相关联，为了对该算子进行前向或后向计算，需要完整的权重。一个模型由大量算子组成，导致总权重规模巨大，参数数量从数十亿到数百亿不等。

因此，模型权重的大小超出了单个 GPU 的存储限制。为了训练大型模型，需要将模型权重分片存储在多个 GPU 上。以配备八个 GPU 的服务器为例，每个 GPU 存储每个算子 12.5\%（1/8）的权重。在前向传播过程中，当需要执行某个算子的计算时，GPU 之间通过全收集（allgather）操作相互通信，获取该算子的所有权重，然后使用完整权重对本地样本进行算子计算，并丢弃收集到的权重。类似地，在反向传播过程中，也需要进行权重收集和丢弃操作。因此，在一次迭代中，每个 GPU 的通信量是模型权重大小的两倍。这就是全分片数据并行（FSDP）[43] 的工作流程，该方法常用于并行训练 [9, 33]。
2.3 激活值与运行时内存
除了并行化方案分配的模型权重外，微调的内存开销还包括运行时生成的中间结果，即激活值。这些激活值在前向传播过程中生成，用于梯度计算，并在反向传播过程中释放。由于反向传播的特性，前向传播开始时产生的激活值在反向传播结束时才会被消耗。这导致大量激活值驻留在内存中，形成明显的峰谷模式。峰值出现在前向传播结束（反向传播开始）时，谷值出现在前向传播开始（反向传播结束）时。激活值的总大小与批量大小、输入序列长度和模型大小有关。其消耗巨大，例如，当输入序列长度为 1024 时，70 亿参数模型上每个样本生成的激活值约为 4GB，这与模型参数占用的总内存（14GB）相当。
2.4 局限性：内存使用效率低且缺乏灵活性
效率低下：激活值内存闲置率达 50\%。具有峰谷特性的激活值内存利用率较低：尽管在峰值时设备内存得到充分利用，但在其他时间存在未使用的内存，导致激活值内存平均闲置率达 50\%。峰值的存在进一步限制了批量大小。在用于微调的小规模硬件配置中，可实现的批量大小较小，导致计算并行度低，每个样本的通信开销较大。此外，峰值高度与模型大小和输入序列长度呈正相关。随着训练数据越来越倾向于长文本，激活值已成为更显著的内存开销。
缺乏灵活性：静态划分的权重。现有方法在训练前确定权重的分布并将其划分到各个设备，该决策基于设备数量和总权重大小。在训练过程中，权重占用的内存大小无法改变。这种静态的权重内存分配方法无法感知运行时内存使用情况及其变化趋势，可能导致内存未充分利用或内存不足的情况。
忽视权重与激活值的相互作用：在训练过程中，权重和激活值共存于内存中，竞争内存资源。此外，它们呈现出不同的内存模式：已分配的权重持续占用内存，而激活值呈现周期性的峰谷变化。因此，它们之间的相互作用对于高效利用内存至关重要。然而，现有方法分别独立优化权重的并行训练和激活值的运行时计划，无法实现同时考虑这两个方面的最优解决方案。


大语言模型对齐：大语言模型对齐在 ChatGPT [19]、LLaMA2 [26] 和 GPT-4 [20] 等对话式人工智能模型的发展中起着关键作用，其核心在于利用这些模型强大的知识和能力，生成特定的、以用户为导向的响应和行为。这一概念对于确保这些模型具备安全性、有效性和可管理性至关重要。需要明确的是，单纯扩大这些语言模型的规模并不能自动使其更好地契合用户意图。已有实例表明，无论大语言模型规模大小，都可能生成具有误导性、有害或对用户无实际帮助的输出。现有的人类偏好对齐方法大致可分为三大类：强化学习 [21, 24]、对比学习 [22,34] 和事后指令重标记 [14,33]。在这些方法中，强化学习方法是实现对齐的主要手段。在本研究中，我们聚焦于 PPO 算法，它是 RLHF [4] 中最为有效的方法，能够满足高效对齐的迫切需求。在 RLHF 中，该过程主要包括三个步骤：收集人类反馈、构建奖励模型以及采用 RL 优化策略 [21]。本文主要关注最后一个步骤。如图 1（c）所示为 InstructGPT 实施的 RL 优化策略步骤示例，该过程涉及 Actor、Ref、Critic 和 RM 等多个模型，同时涵盖了解码、推理和训练等多种工作负载。Actor 和 Ref 的结构通常相似，其中 Actor 的参数可训练，而 Ref 的参数保持冻结状态；同样，Critic 和 RM 具有相同的结构，且 Critic 的参数可训练。 

混合并行化：拥有数百亿参数的大语言模型无法在单个设备上高效地进行训练或推理，因此需要并行执行方案，而权重的布局决定了这些方案，包括并行计算和通信策略。数据并行、张量并行和流水线并行是分布式训练中常用的三种方案。数据并行是指在所有设备上复制模型参数（DP）或在设备间进行参数分片（FSDP），然后为每个设备分配不同的训练数据批次，设备独立进行前向和反向传播计算，通信过程涉及梯度聚合或共享参数收集。张量并行同样需要在设备间共享参数和数据，每个工作节点负责部分算子计算，必须通过通信聚合输出以获得最终结果。流水线并行则是将模型顺序划分为多个部分，每个设备维护模型的特定部分，对于模型的某一部分，相应设备处理来自上一阶段（或训练数据）的数据，其输出需要传输到维护模型后续部分的设备。虽然流水线并行通常通信量较少，但面临 “流水线气泡” 问题，此前已有大量研究对此展开探讨 [5,17]。混合并行化旨在提升分布式训练性能，它整合了多种并行策略，以适应特定模型和独特的训练硬件配置。值得一提的是，Megatron-v2 [18] 通过精心设计的混合并行执行方案，为基于 Transformer 的模型实现了高性能分布式训练。鉴于混合并行化的广泛应用 [35]，本研究着重探究其在大语言模型对齐中的效率。 

多模型执行计划：大语言模型对齐涉及多种具有异构上下文的模型，每个模型对计算资源的需求各不相同。因此，这些模型在系统中的布局和执行方式对于实现最优性能至关重要。如图 2 所示，现有策略可分为三类。在该图中展示了三个不同模型的训练过程，包括前向和反向传播。需要注意的是，尽管上述示例仅展示了流水线并行，但 PUZZLE 并不局限于此，同样适用于数据并行和张量并行。 
逐个模型（MBM）：MBM 是大语言模型对齐中一种直观的模型布局和执行方法（如图 2（a）所示），其中模型具有相同的设备设置，并按顺序执行。现有系统 [12, 31] 采用此方案。然而，尽管该方案简单实用，但存在局限性：模型在运行时占用所有资源，且仅支持顺序执行；此外，如果工作负载无法有效并行化，该方案可能导致效率低下。 
单模型专用设备（SMDD）：SMDD 是另一种常见的模型布局和执行方法（如图 2（b）所示），该方法涉及预先为不同模型分配设备，确保每个模型拥有专用设备以实现并发执行。尽管在 [16] 等研究中采用了此方法，但 SMDD 面临诸多挑战，其资源分配策略复杂且容易导致效率低下。例如，如果模型 1 必须等待模型 2 完成执行，这将导致某些设备出现空闲时间，进而降低资源利用率。 
多模型专用设备（MMDD）：MMDD 整合了上述 MBM 和 SMDD 方法（如图 2（c）所示），该方案将部分模型分配到相同资源，同时为其他模型分配不同资源，从而使某些模型能够并行执行，而分配到相同资源的模型则顺序执行。现有研究（如 [7]）采用了此方案。然而，尽管 MMDD 解决了 MBM 和 SMDD 的部分局限性，但我们发现它缺乏对细粒度调度的考量，而这对于有效管理异构上下文切换至关重要。 



\subsection{DNN 优化方法}
在深度学习领域，已经有许多研究致力于 DNN 的优化。早期的研究主要集中在算法层面，如改进神经网络的结构、优化训练算法等，以提高模型的准确性和泛化能力。近年来，随着硬件性能的提升和 DNN 模型规模的不断扩大，更多的研究开始关注计算效率和资源利用的优化。
一些工作通过优化计算图来提高 DNN 的执行效率。例如，TVM [1] 提出了一种统一的中间表示（IR），能够对不同的深度学习框架和硬件平台进行优化。它通过对计算图进行调度和优化，生成高效的代码。TensorRT [2] 是 NVIDIA 推出的一款深度学习推理优化器，它通过对模型进行层融合、精度校准等操作，减少计算量和内存访问开销，提高推理速度。然而，这些方法主要关注操作符层面的优化，对张量属性的利用不够充分，在处理复杂的长上下文任务时，优化效果有限。
\subsection{张量属性相关研究}
虽然也有一些研究涉及张量的属性，但大多集中在粗粒度的属性，如张量的形状和大小。例如，一些工作通过调整张量的布局来减少内存访问开销 [3]，但这种方法没有考虑到张量的细粒度属性，如规约依赖和广播能力。还有一些研究在特定的操作符或模型中利用张量的某些属性进行优化 [4]，但缺乏系统性和通用性，无法应用于广泛的 DNN 模型。
与现有研究不同，FlashTensor 系统地总结了多种关键的张量属性，并提出了一种通用的方法来识别和利用这些属性进行优化。通过考虑细粒度的张量属性，FlashTensor 能够更深入地分析 DNN 模型的计算特点，从而实现更高效的优化。
\subsection{内核映射相关研究}
在将计算图映射到硬件内核执行方面，也有不少研究。传统的方法通常将计算图中的操作符映射到凸内核中执行 [5]，这种方式在处理简单的计算图时具有一定的有效性，但在面对复杂的计算图结构时，由于其对操作符依赖关系的严格限制，往往无法充分发挥硬件的性能。
一些研究尝试放宽内核的定义，提出了非凸内核的概念 [6]，以提高操作符分配的灵活性。然而，这些研究大多没有结合张量属性进行内核映射，无法根据张量的特点选择最优的内核执行方案。FlashTensor 在非凸内核映射的基础上，充分考虑张量属性，通过综合评估计算强度、内存访问模式和并行性等因素，找到最优的内核映射方案，从而提高计算效率。
