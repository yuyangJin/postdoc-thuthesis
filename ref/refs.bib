@inproceedings{zhang2024h2o,
author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang and Chen, Beidi},
title = {H2O: heavy-hitter oracle for efficient generative inference of large language models},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving longcontent generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H2). Through a comprehensive investigation, we find that (i) the emergence of H2 is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H2O), a KV cache eviction policy that dynamically retains a balance of recent and H 2 tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H2O with 20\% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29\texttimes{}, 29\texttimes{}, and 3\texttimes{} on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9\texttimes{}. The code is available at https://github.com/FMInference/H2O.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1506},
numpages = {50},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@misc{ren2024roco,
      title={On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference}, 
      author={Siyu Ren and Kenny Q. Zhu},
      year={2024},
      eprint={2402.06262},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06262}, 
}

@inproceedings{adnan2024keyformer,
 author = {Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
 pages = {114--127},
 title = {Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference},
 url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf},
 volume = {6},
 publisher = {Machine Learning and Systems},
 address = "Santa Clara, California, USA",
 year = {2024}
}


@misc{li2024snapkv,
      title={SnapKV: LLM Knows What You are Looking for Before Generation}, 
      author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
      year={2024},
      eprint={2404.14469},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14469}, 
}


@misc{dai2024corm,
      title={CORM: Cache Optimization with Recent Message for Large Language Model Inference}, 
      author={Jincheng Dai and Zhuowei Huang and Haiyun Jiang and Chen Chen and Deng Cai and Wei Bi and Shuming Shi},
      year={2024},
      eprint={2404.15949},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.15949}, 
}



@article{xiao2023efficient,

title={Efficient Streaming Language Models with Attention Sinks},

author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},

journal={ICLR},

year={2024}

}

@inproceedings{han2024lm,
    title = "{LM}-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
    author = "Han, Chi  and
      Wang, Qifan  and
      Peng, Hao  and
      Xiong, Wenhan  and
      Chen, Yu  and
      Ji, Heng  and
      Wang, Sinong",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.222/",
    doi = "10.18653/v1/2024.naacl-long.222",
    pages = "3991--4008",
    abstract = "Today`s large language models (LLMs) typically train on short text segments (e.g., {\ensuremath{<}}4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7{\texttimes} decoding speed up and 7.5{\texttimes} memory saving over the original model. Our code will be publicly available upon publication."
}


@inproceedings{dao2022flashattention,
author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
title = {FLASHATTENTION: fast and memory-efficient exact attention with IO-awareness},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware— accounting for reads and writes between levels of GPU memory. We propose FLASHATTENTION, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FLASHATTENTION, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FLASHATTENTION to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FLASHATTENTION trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\texttimes{} speedup on GPT-2 (seq. length 1K), and 2.4\texttimes{} speedup on long-range arena (seq. length 1K-4K). FLASHATTENTION and block-sparse FLASHATTENTION enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1189},
numpages = {16},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@misc{dao2023flashattention,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08691}, 
}

@misc{shah2024flashattention,
      title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision}, 
      author={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
      year={2024},
      eprint={2407.08608},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08608}, 
}


@misc{radford2018gpt,
  title = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year = {2018},
  howpublished = {\url{https://openai.com/blog/language-unsupervised}},
  note = {OpenAI Blog},
}


@article{radford2019gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{brown2020gpt3,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
title = {Language models are few-shot learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}


@misc{achiam2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and others.},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and others},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{dubey2024llama3,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{roziere2023codellama,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.12950}, 
}

@online{MosaicML2023mpt30b,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-30B: Raising the bar
for open-source foundation models},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-30b},
    note      = {Accessed: 2023-06-22},
    organization = {MosaicML},
    urldate   = {2023-06-22}
}

@misc{jiang2023mistralv1,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{peng2023yarn,
      title={YaRN: Efficient Context Window Extension of Large Language Models}, 
      author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
      year={2023},
      eprint={2309.00071},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.00071}, 
}

@inproceedings{dosovitskiy2020vit,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  year         = {2021},
  crossref     = {DBLP:conf/iclr/2021},
  url          = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{liu2022swin,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12009--12019},
  year={2022}
}

@misc{zhao2024opendit,
  author = {Xuanlei Zhao, Zhongkai Zhao, Ziming Liu, Haotian Zhou, Qianli Ma, and Yang You},
  title = {OpenDiT: An Easy, Fast and Memory-Efficient System for DiT Training and Inference},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/NUS-HPC-AI-Lab/OpenDiT}},
}

@misc{zhao2024dsp,
      title={DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers},
      author={Xuanlei Zhao and Shenggan Cheng and Zangwei Zheng and Zheming Yang and Ziming Liu and Yang You},
      year={2024},
      eprint={2403.10266},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{chen2018tvm,
author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
title = {TVM: an automated end-to-end optimizing compiler for deep learning},
year = {2018},
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms - such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
pages = {579–594},
numpages = {16},
location = {Carlsbad, CA, USA},
series = {OSDI'18}
}

@inproceedings{jia2019taso,
author = {Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
title = {TASO: optimizing deep learning computation with automatic generation of graph substitutions},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359630},
doi = {10.1145/3341301.3359630},
abstract = {Existing deep neural network (DNN) frameworks optimize the computation graph of a DNN by applying graph transformations manually designed by human experts. This approach misses possible graph optimizations and is difficult to scale, as new DNN operators are introduced on a regular basis.We propose TASO, the first DNN computation graph optimizer that automatically generates graph substitutions. TASO takes as input a list of operator specifications and generates candidate substitutions using the given operators as basic building blocks. All generated substitutions are formally verified against the operator specifications using an automated theorem prover. To optimize a given DNN computation graph, TASO performs a cost-based backtracking search, applying the substitutions to find an optimized graph, which can be directly used by existing DNN frameworks.Our evaluation on five real-world DNN architectures shows that TASO outperforms existing DNN frameworks by up to 2.8X, while requiring significantly less human effort. For example, TensorFlow currently contains approximately 53,000 lines of manual optimization rules, while the operator specifications needed by TASO are only 1,400 lines of code.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {47–62},
numpages = {16},
keywords = {computation graph substitutions, deep neural network, formal verification, superoptimization},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@inproceedings {wang2021pet,
author = {Haojie Wang and Jidong Zhai and Mingyu Gao and Zixuan Ma and Shizhi Tang and Liyan Zheng and Yuanzhi Li and Kaiyuan Rong and Yuanyong Chen and Zhihao Jia},
title = {{PET}: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections},
booktitle = {15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)},
year = {2021},
isbn = {978-1-939133-22-9},
pages = {37--54},
url = {https://www.usenix.org/conference/osdi21/presentation/wang},
publisher = {USENIX Association},
address = "Virtual",
month = jul
}

@inproceedings{tang2022freetensor,
author = {Tang, Shizhi and Zhai, Jidong and Wang, Haojie and Jiang, Lin and Zheng, Liyan and Yuan, Zhenhao and Zhang, Chen},
title = {FreeTensor: a free-form DSL with holistic optimizations for irregular tensor programs},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523448},
doi = {10.1145/3519939.3523448},
abstract = {Tensor programs are of critical use in many domains. Existing frameworks, such as PyTorch, TensorFlow, and JAX, adopt operator-based programming to ease programming, increase performance, and perform automatic differentiation. However, as the rapid development of tensor programs, operator-based programming shows significant limitations for irregular patterns since a large amount of redundant computation or memory access is introduced. In this work, we propose FreeTensor, a free-form domain specific language which supports redundancy-avoid programming by introducing fine-grained control flow. With optimizations including partial evaluation, dependence-aware transformations, and fine-grained automatic differentiation, FreeTensor is able to generate high performance tensor programs on both CPU and GPU. Experiments show a speedup over existing tensor programming frameworks up to 5.10 \texttimes{} (2.08 \texttimes{} on average) without differentiation, and up to 127.74 \texttimes{} (36.26 \texttimes{} on average) after differentiation, for typical irregular tensor programs.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {872–887},
numpages = {16},
keywords = {tensor computing, optimizing compilers, DSL},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{hu2024korch,
author = {Hu, Muyan and Venkatram, Ashwin and Biswas, Shreyashri and Marimuthu, Balamurugan and Hou, Bohan and Oliaro, Gabriele and Wang, Haojie and Zheng, Liyan and Miao, Xupeng and Zhai, Jidong and Jia, Zhihao},
title = {Optimal Kernel Orchestration for Tensor Programs with Korch},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651383},
doi = {10.1145/3620666.3651383},
abstract = {Kernel orchestration is the task of mapping the computation defined in different operators of a deep neural network (DNN) to the execution of GPU kernels on modern hardware platforms. Prior approaches optimize kernel orchestration by greedily applying operator fusion, which fuses the computation of multiple operators into a single kernel, and miss a variety of optimization opportunities in kernel orchestration.This paper presents Korch, a tensor program optimizer that discovers optimal kernel orchestration strategies for tensor programs. Instead of directly fusing operators, Korch first applies operator fission to decompose tensor operators into a small set of basic tensor algebra primitives. This decomposition enables a diversity of fine-grained, inter-operator optimizations. Next, Korch optimizes kernel orchestration by formalizing it as a constrained optimization problem, leveraging an off-the-shelf binary linear programming solver to discover an optimal orchestration strategy, and generating an executable that can be directly deployed on modern GPU platforms. Evaluation on a variety of DNNs shows that Korch outperforms existing tensor program optimizers by up to 1.7\texttimes{} on V100 GPUs and up to 1.6\texttimes{} on A100 GPUs. Korch is publicly available at https://github.com/humuyan/Korch.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {755–769},
numpages = {15},
keywords = {tensor program, kernel orchestration, machine learning compiler},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}


@inproceedings {zheng2023einnet,
author = {Liyan Zheng and Haojie Wang and Jidong Zhai and Muyan Hu and Zixuan Ma and Tuowei Wang and Shuhong Huang and Xupeng Miao and Shizhi Tang and Kezhao Huang and Zhihao Jia},
title = {{EINNET}: Optimizing Tensor Programs with {Derivation-Based} Transformations},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {739--755},
url = {https://www.usenix.org/conference/osdi23/presentation/zheng},
publisher = {USENIX Association},
month = jul
}

@misc{wu2024mirage,
      title={Mirage: A Multi-Level Superoptimizer for Tensor Programs}, 
      author={Mengdi Wu and Xinhao Cheng and Shengyu Liu and Chunan Shi and Jianan Ji and Kit Ao and Praveen Velliengiri and Xupeng Miao and Oded Padon and Zhihao Jia},
      year={2024},
      eprint={2405.05751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.05751}, 
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 address = "Long Beach, California, USA",
 year = {2017}
}



@techreport{chetlur2014cudnn,
  title={cudnn: Efficient primitives for deep learning},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  institution={NVIDIA},
  year={2014}
}

@inproceedings{tillet2019triton,
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
title = {Triton: an intermediate language and compiler for tiled neural network computations},
year = {2019},
isbn = {9781450367196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315508.3329973},
doi = {10.1145/3315508.3329973},
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {10–19},
numpages = {10},
keywords = {neural networks, compiler, GPU},
location = {Phoenix, AZ, USA},
series = {MAPL 2019}
}

@inproceedings{shao2022metaschedule,
author = {Shao, Junru and Zhou, Xiyou and Feng, Siyuan and Hou, Bohan and Lai, Ruihang and Jin, Hongyi and Lin, Wuwei and Masuda, Masahiro and Yu, Cody Hao and Chen, Tianqi},
title = {Tensor program optimization with probabilistic programs},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Automatic optimization for tensor programs becomes increasingly important as we deploy deep learning in various environments, and efficient optimization relies on a rich search space and effective search. Most existing efforts adopt a search space which lacks the ability to efficiently enable domain experts to grow the search space. This paper introduces MetaSchedule, a domain-specific probabilistic programming language abstraction to construct a rich search space of tensor programs. Our abstraction allows domain experts to analyze the program, and easily propose stochastic choices in a modular way to compose program transformation accordingly. We also build an end-to-end learning-driven framework to find an optimized program for a given search space. Experimental results show that MetaSchedule can cover the search space used in the state-of-the-art tensor program optimization frameworks in a modular way. Additionally, it empowers domain experts to conveniently grow the search space and modularly enhance the system, which brings 48\% speedup on end-to-end deep learning workloads.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2593},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{chen2018autotvm,
author = {Chen, Tianqi and Zheng, Lianmin and Yan, Eddie and Jiang, Ziheng and Moreau, Thierry and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
title = {Learning to optimize tensor programs},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, current systems rely on manually optimized libraries, e.g., cuDNN, that support only a narrow range of server class GPUs. Such reliance limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search using effective model transfer across workloads. Experimental results show that our framework delivers performance that is competitive with state-of-the-art hand-tuned libraries for low-power CPUs, mobile GPUs, and server-class GPUs.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {3393–3404},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}


@inproceedings{zheng2020ansor,
author = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
title = {Ansor: generating high-performance tensor programs for deep learning},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA GPU by up to 3.8\texttimes{}, 2.6\texttimes{}, and 1.7\texttimes{}, respectively.},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {49},
numpages = {17},
series = {OSDI'20}
}


@misc{tensorrt,
  author = {NVIDIA},
  title = {TensorRT},
  note = "\url{https://developer.nvidia.com/tensorrt}",
  year = {2017},
}

@misc{cublas,
  author = {NVIDIA},
  title = {cuBLAS},
  note = "\url{https://developer.nvidia.com/cublas}",
  year = {2016},
}

@misc{onnx,
  author = {ONNX},
  year = {2020},
  note = "\url{https://onnx.ai/}",
}

@article{harris2020numpy,
  author       = {Charles R. Harris and
                  K. Jarrod Millman and
                  St{\'{e}}fan van der Walt and
                  Ralf Gommers and
                  Pauli Virtanen and
                  David Cournapeau and
                  Eric Wieser and
                  Julian Taylor and
                  Sebastian Berg and
                  Nathaniel J. Smith and
                  Robert Kern and
                  Matti Picus and
                  Stephan Hoyer and
                  Marten H. van Kerkwijk and
                  Matthew Brett and
                  Allan Haldane and
                  Jaime Fern{\'{a}}ndez del R{\'{\i}}o and
                  Mark Wiebe and
                  Pearu Peterson and
                  Pierre G{\'{e}}rard{-}Marchant and
                  Kevin Sheppard and
                  Tyler Reddy and
                  Warren Weckesser and
                  Hameer Abbasi and
                  Christoph Gohlke and
                  Travis E. Oliphant},
  title        = {Array Programming with NumPy},
  journal      = {CoRR},
  volume       = {abs/2006.10256},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.10256},
  eprinttype    = {arXiv},
  eprint       = {2006.10256},
  timestamp    = {Sat, 30 Sep 2023 10:08:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-10256.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inbook{pytorch19,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: an imperative style, high-performance deep learning library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
chapter = {721},
pages = {12},
articleno = {721},
numpages = {12}
}

@inproceedings{katharopoulos2020transformers,
author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran\c{c}ois},
title = {Transformers are RNNs: fast autoregressive transformers with linear attention},
year = {2020},
publisher = {JMLR.org},
abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O(N2) to O(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {478},
numpages = {10},
series = {ICML'20}
}

@article{lattner2020mlir,
  author       = {Chris Lattner and
                  Jacques A. Pienaar and
                  Mehdi Amini and
                  Uday Bondhugula and
                  River Riddle and
                  Albert Cohen and
                  Tatiana Shpeisman and
                  Andy Davis and
                  Nicolas Vasilache and
                  Oleksandr Zinenko},
  title        = {{MLIR:} {A} Compiler Infrastructure for the End of Moore's Law},
  journal      = {CoRR},
  volume       = {abs/2002.11054},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.11054},
  eprinttype    = {arXiv},
  eprint       = {2002.11054},
  timestamp    = {Thu, 14 Oct 2021 09:16:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-11054.bib},
  numpages = {21},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ragan2013halide,
author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462176},
doi = {10.1145/2491956.2462176},
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {519–530},
numpages = {12},
keywords = {vectorization, redundant computation, parallelism, optimization, locality, image processing, gpu, domain specific language, compiler, autotuning},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@article{bertsimas1993simulated,
  title={Simulated annealing},
  author={Bertsimas, Dimitris and Tsitsiklis, John},
  journal={Statistical science},
  volume={8},
  number={1},
  pages={10--15},
  year={1993},
  publisher={Institute of Mathematical Statistics}
}

@misc{team2024gemma2,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and others},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}


@article{ye2025flashinfer,
    title = {FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving},
    author = {
      Ye, Zihao and
      Chen, Lequn and
      Lai, Ruihang and
      Lin, Wuwei and
      Zhang, Yineng and
      Wang, Stephanie and
      Chen, Tianqi and
      Kasikci, Baris and
      Grover, Vinod and
      Krishnamurthy, Arvind and
      Ceze, Luis
    },
    journal = {arXiv preprint arXiv:2501.01005},
    year = {2025},
    url = {https://arxiv.org/abs/2501.01005}
}

@inproceedings{MetaFlow,
author = {Jia, Zhihao and Thomas, James and Warzawski, Todd and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
title = {Optimizing DNN Computation with Relaxed Graph Substitutions},
booktitle = {Proceedings of the 2nd Conference on Systems and Machine Learning},
series = {SysML'19},
year = {2019},
}


@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{shamshirband2021health,
  title={A review on deep learning approaches in healthcare systems: Taxonomies, challenges, and open issues},
  author={Shamshirband, Shahab and Fathi, Mahdis and Dehzangi, Abdollah and Chronopoulos, Anthony Theodore and Alinejad-Rokny, Hamid},
  journal={Journal of Biomedical Informatics},
  volume={113},
  pages={103627},
  year={2021},
  publisher={Elsevier}
}

@article{li2024driving,
  title={Improving efficiency of DNN-based relocalization module for autonomous driving with server-side computing},
  author={Li, Dengbo and Zhang, Hanning and Cheng, Jieren and Liu, Bernie},
  journal={Journal of Cloud Computing},
  volume={13},
  number={1},
  pages={25},
  year={2024},
  publisher={Springer}
}


@inproceedings{torchcompile,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and others},
title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640366},
doi = {10.1145/3620665.3640366},
abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {929–947},
numpages = {19},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{niu2021dnnfusion,
author = {Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
title = {DNNFusion: accelerating deep neural networks execution with advanced operator fusion},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454083},
doi = {10.1145/3453483.3454083},
abstract = {Deep Neural Networks (DNNs) have emerged as the core enabler of many major applications on mobile devices. To achieve high accuracy, DNN models have become increasingly deep with hundreds or even thousands of operator layers, leading to high memory and computational requirements for inference. Operator fusion (or kernel/layer fusion) is key optimization in many state-of-the-art DNN execution frameworks, such as TensorFlow, TVM, and MNN, that aim to improve the efficiency of the DNN inference. However, these frameworks usually adopt fusion approaches based on certain patterns that are too restrictive to cover the diversity of operators and layer connections, especially those seen in many extremely deep models. Polyhedral-based loop fusion techniques, on the other hand, work on a low-level view of the computation without operator-level information, and can also miss potential fusion opportunities. To address this challenge, this paper proposes a novel and extensive loop fusion framework called DNNFusion. The basic idea of this work is to work at an operator view of DNNs, but expand fusion opportunities by developing a classification of both individual operators and their combinations. In addition, DNNFusion includes 1) a novel mathematical-property-based graph rewriting framework to reduce evaluation costs and facilitate subsequent operator fusion, 2) an integrated fusion plan generation that leverages the high-level analysis and accurate light-weight profiling, and 3) additional optimizations during fusion code generation. DNNFusion is extensively evaluated on 15 DNN models with varied types of tasks, model sizes, and layer counts. The evaluation results demonstrate that DNNFusion finds up to 8.8 \texttimes{} higher fusion opportunities, outperforms four state-of-the-art DNN execution frameworks with 9.3\texttimes{} speedup. The memory requirement reduction and speedups can enable the execution of many of the target models on mobile devices and even make them part of a real-time application.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {883–898},
numpages = {16},
keywords = {Compiler Optimization, Deep Neural Network, Mobile Devices, Operator Fusion},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{niu2024smartmem,
author = {Niu, Wei and Sanim, Md Musfiqur Rahman and Shu, Zhihao and Guan, Jiexiong and Shen, Xipeng and Yin, Miao and Agrawal, Gagan and Ren, Bin},
title = {SmartMem: Layout Transformation Elimination and Adaptation for Efficient DNN Execution on Mobile},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651384},
doi = {10.1145/3620666.3651384},
abstract = {This work is motivated by recent developments in Deep Neural Networks, particularly the Transformer architectures underlying applications such as ChatGPT, and the need for performing inference on mobile devices. Focusing on emerging transformers (specifically the ones with computationally efficient Swin-like architectures) and large models (e.g., Stable Diffusion and LLMs) based on transformers, we observe that layout transformations between the computational operators cause a significant slowdown in these applications. This paper presents SmartMem, a comprehensive framework for eliminating most layout transformations, with the idea that multiple operators can use the same tensor layout through careful choice of layout and implementation of operations. Our approach is based on classifying the operators into four groups, and considering combinations of producer-consumer edges between the operators. We develop a set of methods for searching such layouts. Another component of our work is developing efficient memory layouts for 2.5 dimensional memory commonly seen in mobile devices. Our experimental results show that SmartMem outperforms 5 state-of-the-art DNN execution frameworks on mobile devices across 18 varied neural networks, including CNNs, Transformers with both local and global attention, as well as LLMs. In particular, compared to DNNFusion, SmartMem achieves an average speedup of 2.8\texttimes{}, and outperforms TVM and MNN with speedups of 6.9\texttimes{} and 7.9\texttimes{}, respectively, on average.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {916–931},
numpages = {16},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}


@inproceedings {zheng2022sparta,
author = {Ningxin Zheng and Bin Lin and Quanlu Zhang and Lingxiao Ma and Yuqing Yang and Fan Yang and Yang Wang and Mao Yang and Lidong Zhou},
title = {{SparTA}: {Deep-Learning} Model Sparsity via {Tensor-with-Sparsity-Attribute}},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {213--232},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-ningxin},
publisher = {USENIX Association},
month = jul
}

@misc{xla,
	author = {TensorFlow XLA},
	year = {2023},
	Note = "\url{https://www.tensorflow.org/xla}",
}

@misc{flexattention,
	author = {Pytorch FlexAttention},
	year = {2024},
	Note = "\url{https://pytorch.org/blog/flexattention/}",
}

@article{williams2009roofline,
  title={Roofline: an insightful visual performance model for multicore architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM New York, NY, USA}
}
@inproceedings{astitch,
author = {Zheng, Zhen and Yang, Xuanda and Zhao, Pengzhan and Long, Guoping and Zhu, Kai and Zhu, Feiwen and Zhao, Wenyi and Liu, Xiaoyong and Yang, Jun and Zhai, Jidong and Song, Shuaiwen Leon and Lin, Wei},
title = {AStitch: Enabling a New Multi-Dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507723},
doi = {10.1145/3503222.3507723},
abstract = {This work reveals that memory-intensive computation is a rising performance-critical factor in recent machine learning models. Due to a unique set of new challenges, existing ML optimizing compilers cannot perform efficient fusion under complex two-level dependencies combined with just-in-time demand. They face the dilemma of either performing costly fusion due to heavy redundant computation, or skipping fusion which results in massive number of kernels. Furthermore, they often suffer from low parallelism due to the lack of support for real-world production workloads with irregular tensor shapes. To address these rising challenges, we propose AStitch, a machine learning optimizing compiler that opens a new multi-dimensional optimization space for memory-intensive ML computations. It systematically abstracts four operator-stitching schemes while considering multi-dimensional optimization objectives, tackles complex computation graph dependencies with novel hierarchical data reuse, and efficiently processes various tensor shapes via adaptive thread mapping. Finally, AStitch provides just-in-time support incorporating our proposed optimizations for both ML training and inference. Although AStitch serves as a stand-alone compiler engine that is portable to any version of TensorFlow, its basic ideas can be generally applied to other ML frameworks and optimization compilers. Experimental results show that AStitch can achieve an average of 1.84x speedup (up to 2.73x) over the state-of-the-art Google's XLA solution across five production workloads. We also deploy AStitch onto a production cluster for ML workloads with thousands of GPUs. The system has been in operation for more than 10 months and saves about 20,000 GPU hours for 70,000 tasks per week.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {359–373},
numpages = {15},
keywords = {Compiler Optimization, Fusion, Machine Learning, Memory-Intensive Computation},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings {shi2023welder,
author = {Yining Shi and Zhi Yang and Jilong Xue and Lingxiao Ma and Yuqing Xia and Ziming Miao and Yuxiao Guo and Fan Yang and Lidong Zhou},
title = {Welder: Scheduling Deep Learning Memory Access via Tile-graph},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {701--718},
url = {https://www.usenix.org/conference/osdi23/presentation/shi},
publisher = {USENIX Association},
month = jul
}


@inproceedings{zhang2024infinitebench,
    title = "$\infty${B}ench: Extending Long Context Evaluation Beyond 100{K} Tokens",
    author = "Zhang, Xinrong  and
      Chen, Yingfa  and
      Hu, Shengding  and
      Xu, Zihang  and
      Chen, Junhao  and
      Hao, Moo  and
      Han, Xu  and
      Thai, Zhen  and
      Wang, Shuo  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.814/",
    doi = "10.18653/v1/2024.acl-long.814",
    pages = "15262--15277",
    abstract = "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose , the first LLM benchmark featuring an average data length surpassing 100K tokens. comprises synthetic and realistic tasks spanning diverse domains in English and Chinese. The tasks in are designed to require an understanding of long dependencies in contexts and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. Based on , we evaluate several state-of-the-art LLMs tailored for processing long contexts. The experimental results indicate that existing long-context LLMs still require significant advancements to process 100K+ contexts effectively. Furthermore, we present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released."
}

@Article{Einstein,
  author =       "Albert Einstein",
  title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
                 [{On} the electrodynamics of moving bodies]",
  journal =      "Annalen der Physik",
  volume =       "322",
  number =       "10",
  pages =        "891--921",
  year =         "1905",
  DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
} 

@online{CUDAStream,
  author       = {NVIDIA},
  title        = {CUDA RUNTIME API},
  year         = {2023},
  url          = {https://docs.nvidia.com/cuda/cuda-runtime-api/stream-sync-behavior.html},
  note         = {10 Dec 2023},
}

@online{TorchPinned,
  author       = {PyTorch},
  title        = {PyTorch Documentation},
  year         = {2023},
  url          = {https://pytorch.org/docs/stable/generated/torch.Tensor.pin_memory.html},
  note         = {10 Dec 2023},
}

@online{TorchCPP,
  author       = {PyTorch},
  title        = {PyTorch Tutorial},
  year         = {2023},
  url          = {https://pytorch.org/tutorials/advanced/cpp_extension.html},
  note         = {10 Dec 2023},
}

@article{TorchFSDP,
  author       = {Yanli Zhao and
                  Andrew Gu and
                  Rohan Varma and
                  Liang Luo and
                  Chien{-}Chin Huang and
                  Min Xu and
                  Less Wright and
                  Hamid Shojanazeri and
                  Myle Ott and
                  Sam Shleifer and
                  Alban Desmaison and
                  Can Balioglu and
                  Bernard Nguyen and
                  Geeta Chauhan and
                  Yuchen Hao and
                  Shen Li},
  title        = {PyTorch {FSDP:} Experiences on Scaling Fully Sharded Data Parallel},
  journal      = {CoRR},
  volume       = {abs/2304.11277},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.11277},
  doi          = {10.48550/arXiv.2304.11277},
  eprinttype    = {arXiv},
  eprint       = {2304.11277},
  timestamp    = {Tue, 02 May 2023 18:58:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-11277.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{jeaugey2017nccl,
  title={Nccl 2.0},
  author={Jeaugey, Sylvain},
  booktitle={GPU Technology Conference (GTC)},
  volume={2},
  year={2017}
}

@inproceedings{he2022fastermoe,
  author    = {Jiaao He and
               Jidong Zhai and
               Tiago Antunes and
               Haojie Wang and
               Fuwen Luo and
               Shangfeng Shi and
               Qin Li},
  title     = {FasterMoE: modeling and optimizing training of large-scale dynamic
               pre-trained models},
  booktitle = {PPoPP '22: 27th {ACM} {SIGPLAN} Symposium on Principles and Practice
               of Parallel Programming, Seoul, Republic of Korea, April 2 - 6, 2022},
  pages     = {120--134},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3503221.3508418},
  doi       = {10.1145/3503221.3508418},
  timestamp = {Wed, 07 Dec 2022 23:13:37 +0100},
  biburl    = {https://dblp.org/rec/conf/ppopp/HeZAWLSL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lepikhin2020gshard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic
               Sharding},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=qrwe7XHTmYb},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LepikhinLXCFHKS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{he2021fastmoe,
  author    = {Jiaao He and
               Jiezhong Qiu and
               Aohan Zeng and
               Zhilin Yang and
               Jidong Zhai and
               Jie Tang},
  title     = {FastMoE: {A} Fast Mixture-of-Expert Training System},
  journal   = {CoRR},
  volume    = {abs/2103.13262},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13262},
  eprinttype = {arXiv},
  eprint    = {2103.13262},
  timestamp = {Thu, 14 Oct 2021 09:16:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13262.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ma2022bagualu,
  author    = {Zixuan Ma and
               Jiaao He and
               Jiezhong Qiu and
               Huanqi Cao and
               Yuanwei Wang and
               Zhenbo Sun and
               Liyan Zheng and
               Haojie Wang and
               Shizhi Tang and
               Tianyu Zheng and
               Junyang Lin and
               Guanyu Feng and
               Zeqiang Huang and
               Jie Gao and
               Aohan Zeng and
               Jianwei Zhang and
               Runxin Zhong and
               Tianhui Shi and
               Sha Liu and
               Weimin Zheng and
               Jie Tang and
               Hongxia Yang and
               Xin Liu and
               Jidong Zhai and
               Wenguang Chen},
  title     = {BaGuaLu: targeting brain scale pretrained models with over 37 million
               cores},
  booktitle = {PPoPP '22: 27th {ACM} {SIGPLAN} Symposium on Principles and Practice
               of Parallel Programming, Seoul, Republic of Korea, April 2 - 6, 2022},
  pages     = {192--204},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3503221.3508417},
  doi       = {10.1145/3503221.3508417},
  timestamp = {Wed, 07 Dec 2022 23:13:36 +0100},
  biburl    = {https://dblp.org/rec/conf/ppopp/MaHQCWSZWTZLFHG22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nie2022hetumoe,
  author    = {Xiaonan Nie and
               Pinxue Zhao and
               Xupeng Miao and
               Tong Zhao and
               Bin Cui},
  title     = {HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed
               Training System},
  journal   = {CoRR},
  volume    = {abs/2203.14685},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2203.14685},
  doi       = {10.48550/arXiv.2203.14685},
  eprinttype = {arXiv},
  eprint    = {2203.14685},
  timestamp = {Mon, 04 Apr 2022 18:01:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2203-14685.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hwang2022tutel,
  author    = {Changho Hwang and
               Wei Cui and
               Yifan Xiong and
               Ziyue Yang and
               Ze Liu and
               Han Hu and
               Zilong Wang and
               Rafael Salas and
               Jithin Jose and
               Prabhat Ram and
               Joe Chau and
               Peng Cheng and
               Fan Yang and
               Mao Yang and
               Yongqiang Xiong},
  title     = {Tutel: Adaptive Mixture-of-Experts at Scale},
  journal   = {CoRR},
  volume    = {abs/2206.03382},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.03382},
  doi       = {10.48550/arXiv.2206.03382},
  eprinttype = {arXiv},
  eprint    = {2206.03382},
  timestamp = {Wed, 05 Oct 2022 11:53:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2206-03382.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rajbhandari2022deepspeed-moe,
  author    = {Samyam Rajbhandari and
               Conglong Li and
               Zhewei Yao and
               Minjia Zhang and
               Reza Yazdani Aminabadi and
               Ammar Ahmad Awan and
               Jeff Rasley and
               Yuxiong He},
  title     = {DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training
               to Power Next-Generation {AI} Scale},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {18332--18346},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/rajbhandari22a.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/RajbhandariLYZA22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li2023lina,
  author       = {Jiamin Li and
                  Yimin Jiang and
                  Yibo Zhu and
                  Cong Wang and
                  Hong Xu},
  editor       = {Julia Lawall and
                  Dan Williams},
  title        = {Accelerating Distributed MoE Training and Inference with Lina},
  booktitle    = {2023 {USENIX} Annual Technical Conference, {USENIX} {ATC} 2023, Boston,
                  MA, USA, July 10-12, 2023},
  pages        = {945--959},
  publisher    = {{USENIX} Association},
  year         = {2023},
  url          = {https://www.usenix.org/conference/atc23/presentation/li-jiamin},
  timestamp    = {Sat, 15 Jul 2023 00:21:53 +0200},
  biburl       = {https://dblp.org/rec/conf/usenix/LiJZ0X23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhai2023smartmoe,
  author       = {Mingshu Zhai and
                  Jiaao He and
                  Zixuan Ma and
                  Zan Zong and
                  Runqing Zhang and
                  Jidong Zhai},
  editor       = {Julia Lawall and
                  Dan Williams},
  title        = {SmartMoE: Efficiently Training Sparsely-Activated Models through Combining
                  Offline and Online Parallelization},
  booktitle    = {2023 {USENIX} Annual Technical Conference, {USENIX} {ATC} 2023, Boston,
                  MA, USA, July 10-12, 2023},
  pages        = {961--975},
  publisher    = {{USENIX} Association},
  year         = {2023},
  url          = {https://www.usenix.org/conference/atc23/presentation/zhai},
  timestamp    = {Sat, 15 Jul 2023 00:21:53 +0200},
  biburl       = {https://dblp.org/rec/conf/usenix/ZhaiHMZZZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{singh2023deepspeed-ted,
  author       = {Siddharth Singh and
                  Olatunji Ruwase and
                  Ammar Ahmad Awan and
                  Samyam Rajbhandari and
                  Yuxiong He and
                  Abhinav Bhatele},
  editor       = {Kyle A. Gallivan and
                  Efstratios Gallopoulos and
                  Dimitrios S. Nikolopoulos and
                  Ram{\'{o}}n Beivide},
  title        = {A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts
                  Training},
  booktitle    = {Proceedings of the 37th International Conference on Supercomputing,
                  {ICS} 2023, Orlando, FL, USA, June 21-23, 2023},
  pages        = {203--214},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3577193.3593704},
  doi          = {10.1145/3577193.3593704},
  timestamp    = {Fri, 07 Jul 2023 23:30:39 +0200},
  biburl       = {https://dblp.org/rec/conf/ics/SinghRARHB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{nie2023flexmoe,
  author       = {Xiaonan Nie and
                  Xupeng Miao and
                  Zilong Wang and
                  Zichao Yang and
                  Jilong Xue and
                  Lingxiao Ma and
                  Gang Cao and
                  Bin Cui},
  title        = {FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via
                  Dynamic Device Placement},
  journal      = {Proc. {ACM} Manag. Data},
  volume       = {1},
  number       = {1},
  pages        = {110:1--110:19},
  year         = {2023},
  url          = {https://doi.org/10.1145/3588964},
  doi          = {10.1145/3588964},
  timestamp    = {Thu, 15 Jun 2023 21:57:48 +0200},
  biburl       = {https://dblp.org/rec/journals/pacmmod/NieMWYXMC023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2023janus,
  author       = {Juncai Liu and
                  Jessie Hui Wang and
                  Yimin Jiang},
  editor       = {Henning Schulzrinne and
                  Vishal Misra and
                  Eddie Kohler and
                  David A. Maltz},
  title        = {Janus: {A} Unified Distributed Training Framework for Sparse Mixture-of-Experts
                  Models},
  booktitle    = {Proceedings of the {ACM} {SIGCOMM} 2023 Conference, {ACM} {SIGCOMM}
                  2023, New York, NY, USA, 10-14 September 2023},
  pages        = {486--498},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3603269.3604869},
  doi          = {10.1145/3603269.3604869},
  timestamp    = {Sun, 24 Sep 2023 15:46:15 +0200},
  biburl       = {https://dblp.org/rec/conf/sigcomm/LiuWJ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gale2022megablocks,
  author       = {Trevor Gale and
                  Deepak Narayanan and
                  Cliff Young and
                  Matei Zaharia},
  title        = {MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},
  journal      = {CoRR},
  volume       = {abs/2211.15841},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.15841},
  doi          = {10.48550/arXiv.2211.15841},
  eprinttype    = {arXiv},
  eprint       = {2211.15841},
  timestamp    = {Fri, 02 Dec 2022 15:46:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-15841.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{xu2021gspmd,
  author    = {Yuanzhong Xu and
               HyoukJoong Lee and
               Dehao Chen and
               Blake A. Hechtman and
               Yanping Huang and
               Rahul Joshi and
               Maxim Krikun and
               Dmitry Lepikhin and
               Andy Ly and
               Marcello Maggioni and
               Ruoming Pang and
               Noam Shazeer and
               Shibo Wang and
               Tao Wang and
               Yonghui Wu and
               Zhifeng Chen},
  title     = {{GSPMD:} General and Scalable Parallelization for {ML} Computation
               Graphs},
  journal   = {CoRR},
  volume    = {abs/2105.04663},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.04663},
  eprinttype = {arXiv},
  eprint    = {2105.04663},
  timestamp = {Fri, 14 May 2021 12:13:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-04663.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lewis2021baselayers,
  author    = {Mike Lewis and
               Shruti Bhosale and
               Tim Dettmers and
               Naman Goyal and
               Luke Zettlemoyer},
  title     = {{BASE} Layers: Simplifying Training of Large, Sparse Models},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {6265--6274},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/lewis21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/LewisBDGZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jangda2022coconet,
  author       = {Abhinav Jangda and
                  Jun Huang and
                  Guodong Liu and
                  Amir Hossein Nodehi Sabet and
                  Saeed Maleki and
                  Youshan Miao and
                  Madanlal Musuvathi and
                  Todd Mytkowicz and
                  Olli Saarikivi},
  editor       = {Babak Falsafi and
                  Michael Ferdman and
                  Shan Lu and
                  Thomas F. Wenisch},
  title        = {Breaking the computation and communication abstraction barrier in
                  distributed machine learning workloads},
  booktitle    = {{ASPLOS} '22: 27th {ACM} International Conference on Architectural
                  Support for Programming Languages and Operating Systems, Lausanne,
                  Switzerland, 28 February 2022 - 4 March 2022},
  pages        = {402--416},
  publisher    = {{ACM}},
  year         = {2022},
  url          = {https://doi.org/10.1145/3503222.3507778},
  doi          = {10.1145/3503222.3507778},
  timestamp    = {Mon, 05 Dec 2022 13:35:43 +0100},
  biburl       = {https://dblp.org/rec/conf/asplos/JangdaHLSMMMMS22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zhuang2022alpacomm,
  author       = {Yonghao Zhuang and
                  Hexu Zhao and
                  Lianmin Zheng and
                  Zhuohan Li and
                  Eric P. Xing and
                  Qirong Ho and
                  Joseph E. Gonzalez and
                  Ion Stoica and
                  Hao Zhang},
  title        = {On Optimizing the Communication of Model Parallelism},
  journal      = {CoRR},
  volume       = {abs/2211.05322},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05322},
  doi          = {10.48550/arXiv.2211.05322},
  eprinttype    = {arXiv},
  eprint       = {2211.05322},
  timestamp    = {Tue, 15 Nov 2022 15:45:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05322.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2023googlecomm,
  author       = {Shibo Wang and
                  Jinliang Wei and
                  Amit Sabne and
                  Andy Davis and
                  Berkin Ilbeyi and
                  Blake Hechtman and
                  Dehao Chen and
                  Karthik Srinivasa Murthy and
                  Marcello Maggioni and
                  Qiao Zhang and
                  Sameer Kumar and
                  Tongfei Guo and
                  Yuanzhong Xu and
                  Zongwei Zhou},
  editor       = {Tor M. Aamodt and
                  Natalie D. Enright Jerger and
                  Michael M. Swift},
  title        = {Overlap Communication with Dependent Computation via Decomposition
                  in Large Deep Learning Models},
  booktitle    = {Proceedings of the 28th {ACM} International Conference on Architectural
                  Support for Programming Languages and Operating Systems, Volume 1,
                  {ASPLOS} 2023, Vancouver, BC, Canada, March 25-29, 2023},
  pages        = {93--106},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3567955.3567959},
  doi          = {10.1145/3567955.3567959},
  timestamp    = {Thu, 22 Dec 2022 11:53:15 +0100},
  biburl       = {https://dblp.org/rec/conf/asplos/WangWSDIHCMMZKG23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cowan2023mscclang,
  author       = {Meghan Cowan and
                  Saeed Maleki and
                  Madanlal Musuvathi and
                  Olli Saarikivi and
                  Yifan Xiong},
  editor       = {Tor M. Aamodt and
                  Natalie D. Enright Jerger and
                  Michael M. Swift},
  title        = {MSCCLang: Microsoft Collective Communication Language},
  booktitle    = {Proceedings of the 28th {ACM} International Conference on Architectural
                  Support for Programming Languages and Operating Systems, Volume 2,
                  {ASPLOS} 2023, Vancouver, BC, Canada, March 25-29, 2023},
  pages        = {502--514},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3575693.3575724},
  doi          = {10.1145/3575693.3575724},
  timestamp    = {Mon, 20 Mar 2023 11:43:57 +0100},
  biburl       = {https://dblp.org/rec/conf/asplos/CowanMMSX23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cai2021sccl,
  author       = {Zixian Cai and
                  Zhengyang Liu and
                  Saeed Maleki and
                  Madanlal Musuvathi and
                  Todd Mytkowicz and
                  Jacob Nelson and
                  Olli Saarikivi},
  editor       = {Jaejin Lee and
                  Erez Petrank},
  title        = {Synthesizing optimal collective algorithms},
  booktitle    = {PPoPP '21: 26th {ACM} {SIGPLAN} Symposium on Principles and Practice
                  of Parallel Programming, Virtual Event, Republic of Korea, February
                  27- March 3, 2021},
  pages        = {62--75},
  publisher    = {{ACM}},
  year         = {2021},
  url          = {https://doi.org/10.1145/3437801.3441620},
  doi          = {10.1145/3437801.3441620},
  timestamp    = {Fri, 11 Aug 2023 15:15:41 +0200},
  biburl       = {https://dblp.org/rec/conf/ppopp/CaiLMMMNS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2016recomp,
  author       = {Tianqi Chen and
                  Bing Xu and
                  Chiyuan Zhang and
                  Carlos Guestrin},
  title        = {Training Deep Nets with Sublinear Memory Cost},
  journal      = {CoRR},
  volume       = {abs/1604.06174},
  year         = {2016},
  url          = {http://arxiv.org/abs/1604.06174},
  eprinttype    = {arXiv},
  eprint       = {1604.06174},
  timestamp    = {Sat, 17 Dec 2022 01:15:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/ChenXZG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zz2022ipdps,
  author       = {Lijie Wen and
                  Zan Zong and
                  Li Lin and
                  Leilei Lin},
  title        = {A Swap Dominated Tensor Re-Generation Strategy for Training Deep Learning
                  Models},
  booktitle    = {2022 {IEEE} International Parallel and Distributed Processing Symposium,
                  {IPDPS} 2022, Lyon, France, May 30 - June 3, 2022},
  pages        = {996--1006},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/IPDPS53621.2022.00101},
  doi          = {10.1109/IPDPS53621.2022.00101},
  timestamp    = {Fri, 22 Jul 2022 11:43:23 +0200},
  biburl       = {https://dblp.org/rec/conf/ipps/WenZ0L22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zz2023tpds,
  author       = {Zan Zong and
                  Li Lin and
                  Leilei Lin and
                  Lijie Wen and
                  Yu Sun},
  title        = {{STR:} Hybrid Tensor Re-Generation to Break Memory Wall for {DNN}
                  Training},
  journal      = {{IEEE} Trans. Parallel Distributed Syst.},
  volume       = {34},
  number       = {8},
  pages        = {2403--2418},
  year         = {2023},
  url          = {https://doi.org/10.1109/TPDS.2023.3266110},
  doi          = {10.1109/TPDS.2023.3266110},
  timestamp    = {Fri, 21 Jul 2023 22:26:10 +0200},
  biburl       = {https://dblp.org/rec/journals/tpds/ZongLLWS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jain2020checkmate,
  author       = {Paras Jain and
                  Ajay Jain and
                  Aniruddha Nrusimha and
                  Amir Gholami and
                  Pieter Abbeel and
                  Kurt Keutzer and
                  Ion Stoica and
                  Joseph Gonzalez},
  editor       = {Inderjit S. Dhillon and
                  Dimitris S. Papailiopoulos and
                  Vivienne Sze},
  title        = {Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization},
  booktitle    = {Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin,
                  TX, USA, March 2-4, 2020},
  publisher    = {mlsys.org},
  year         = {2020},
  url          = {https://proceedings.mlsys.org/book/320.pdf},
  timestamp    = {Thu, 18 Jun 2020 15:48:04 +0200},
  biburl       = {https://dblp.org/rec/conf/mlsys/0001JNGAKS020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Beaumont2021nips,
  author       = {Olivier Beaumont and
                  Lionel Eyraud{-}Dubois and
                  Alena Shilova},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Efficient Combination of Rematerialization and Offloading for Training
                  DNNs},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {23844--23857},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/c8461bf13fca8a2b9912ab2eb1668e4b-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:49 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BeaumontES21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhou2023mpress,
  author       = {Quan Zhou and
                  Haiquan Wang and
                  Xiaoyan Yu and
                  Cheng Li and
                  Youhui Bai and
                  Feng Yan and
                  Yinlong Xu},
  title        = {MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers
                  via Memory-Saving Inter-Operator Parallelism},
  booktitle    = {{IEEE} International Symposium on High-Performance Computer Architecture,
                  {HPCA} 2023, Montreal, QC, Canada, February 25 - March 1, 2023},
  pages        = {556--569},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/HPCA56546.2023.10071077},
  doi          = {10.1109/HPCA56546.2023.10071077},
  timestamp    = {Wed, 29 Mar 2023 11:07:46 +0200},
  biburl       = {https://dblp.org/rec/conf/hpca/ZhouWYLBYX23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{krothikanti2022megatronv3,
  author       = {Vijay Korthikanti and
                  Jared Casper and
                  Sangkug Lym and
                  Lawrence McAfee and
                  Michael Andersch and
                  Mohammad Shoeybi and
                  Bryan Catanzaro},
  title        = {Reducing Activation Recomputation in Large Transformer Models},
  journal      = {CoRR},
  volume       = {abs/2205.05198},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2205.05198},
  doi          = {10.48550/arXiv.2205.05198},
  eprinttype    = {arXiv},
  eprint       = {2205.05198},
  timestamp    = {Tue, 17 May 2022 17:31:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2205-05198.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rhu2016vdnn,
  author       = {Minsoo Rhu and
                  Natalia Gimelshein and
                  Jason Clemons and
                  Arslan Zulfiqar and
                  Stephen W. Keckler},
  title        = {vDNN: Virtualized deep neural networks for scalable, memory-efficient
                  neural network design},
  booktitle    = {49th Annual {IEEE/ACM} International Symposium on Microarchitecture,
                  {MICRO} 2016, Taipei, Taiwan, October 15-19, 2016},
  pages        = {18:1--18:13},
  publisher    = {{IEEE} Computer Society},
  year         = {2016},
  url          = {https://doi.org/10.1109/MICRO.2016.7783721},
  doi          = {10.1109/MICRO.2016.7783721},
  timestamp    = {Sun, 02 Oct 2022 16:12:01 +0200},
  biburl       = {https://dblp.org/rec/conf/micro/RhuGCZK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2019swap,
  author       = {Junzhe Zhang and
                  Sai{-}Ho Yeung and
                  Yao Shu and
                  Bingsheng He and
                  Wei Wang},
  title        = {Efficient Memory Management for GPU-based Deep Learning Systems},
  journal      = {CoRR},
  volume       = {abs/1903.06631},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.06631},
  eprinttype    = {arXiv},
  eprint       = {1903.06631},
  timestamp    = {Mon, 01 Apr 2019 14:07:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-06631.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2020swapadvisor,
  author       = {Chien{-}Chin Huang and
                  Gu Jin and
                  Jinyang Li},
  editor       = {James R. Larus and
                  Luis Ceze and
                  Karin Strauss},
  title        = {SwapAdvisor: Pushing Deep Learning Beyond the {GPU} Memory Limit via
                  Smart Swapping},
  booktitle    = {{ASPLOS} '20: Architectural Support for Programming Languages and
                  Operating Systems, Lausanne, Switzerland, March 16-20, 2020},
  pages        = {1341--1355},
  publisher    = {{ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1145/3373376.3378530},
  doi          = {10.1145/3373376.3378530},
  timestamp    = {Tue, 21 Jul 2020 12:07:35 +0200},
  biburl       = {https://dblp.org/rec/conf/asplos/HuangJ020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ren2021zerooffload,
  author       = {Jie Ren and
                  Samyam Rajbhandari and
                  Reza Yazdani Aminabadi and
                  Olatunji Ruwase and
                  Shuangyan Yang and
                  Minjia Zhang and
                  Dong Li and
                  Yuxiong He},
  editor       = {Irina Calciu and
                  Geoff Kuenning},
  title        = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  booktitle    = {2021 {USENIX} Annual Technical Conference, {USENIX} {ATC} 2021, July
                  14-16, 2021},
  pages        = {551--564},
  publisher    = {{USENIX} Association},
  year         = {2021},
  url          = {https://www.usenix.org/conference/atc21/presentation/ren-jie},
  timestamp    = {Thu, 12 Aug 2021 18:08:26 +0200},
  biburl       = {https://dblp.org/rec/conf/usenix/0015RARYZ0H21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{raj2021zeroinfinity,
  author       = {Samyam Rajbhandari and
                  Olatunji Ruwase and
                  Jeff Rasley and
                  Shaden Smith and
                  Yuxiong He},
  editor       = {Bronis R. de Supinski and
                  Mary W. Hall and
                  Todd Gamblin},
  title        = {ZeRO-infinity: breaking the {GPU} memory wall for extreme scale deep
                  learning},
  booktitle    = {International Conference for High Performance Computing, Networking,
                  Storage and Analysis, {SC} 2021, St. Louis, Missouri, USA, November
                  14-19, 2021},
  pages        = {59},
  publisher    = {{ACM}},
  year         = {2021},
  url          = {https://doi.org/10.1145/3458817.3476205},
  doi          = {10.1145/3458817.3476205},
  timestamp    = {Tue, 08 Nov 2022 16:03:02 +0100},
  biburl       = {https://dblp.org/rec/conf/sc/RajbhandariRRSH21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{fang2023patrick,
  author       = {Jiarui Fang and
                  Zilin Zhu and
                  Shenggui Li and
                  Hui Su and
                  Yang Yu and
                  Jie Zhou and
                  Yang You},
  title        = {Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory
                  Management},
  journal      = {{IEEE} Trans. Parallel Distributed Syst.},
  volume       = {34},
  number       = {1},
  pages        = {304--315},
  year         = {2023},
  url          = {https://doi.org/10.1109/TPDS.2022.3219819},
  doi          = {10.1109/TPDS.2022.3219819},
  timestamp    = {Tue, 11 Apr 2023 19:05:10 +0200},
  biburl       = {https://dblp.org/rec/journals/tpds/FangZLSYZY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2022stronghold,
  author       = {Xiaoyang Sun and
                  Wei Wang and
                  Shenghao Qiu and
                  Renyu Yang and
                  Songfang Huang and
                  Jie Xu and
                  Zheng Wang},
  editor       = {Felix Wolf and
                  Sameer Shende and
                  Candace Culhane and
                  Sadaf R. Alam and
                  Heike Jagode},
  title        = {{STRONGHOLD:} Fast and Affordable Billion-Scale Deep Learning Model
                  Training},
  booktitle    = {{SC22:} International Conference for High Performance Computing, Networking,
                  Storage and Analysis, Dallas, TX, USA, November 13-18, 2022},
  pages        = {71:1--71:17},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/SC41404.2022.00076},
  doi          = {10.1109/SC41404.2022.00076},
  timestamp    = {Fri, 14 Jul 2023 09:32:34 +0200},
  biburl       = {https://dblp.org/rec/conf/sc/SunWQYHXW22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sheng2023flexgen,
  author       = {Ying Sheng and
                  Lianmin Zheng and
                  Binhang Yuan and
                  Zhuohan Li and
                  Max Ryabinin and
                  Beidi Chen and
                  Percy Liang and
                  Christopher R{\'{e}} and
                  Ion Stoica and
                  Ce Zhang},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {FlexGen: High-Throughput Generative Inference of Large Language Models
                  with a Single {GPU}},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31094--31116},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/sheng23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0007ZYLRCLRSZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {jiang2020byteps,
author = {Yimin Jiang and Yibo Zhu and Chang Lan and Bairen Yi and Yong Cui and Chuanxiong Guo},
title = {A Unified Architecture for Accelerating Distributed {DNN} Training in Heterogeneous {GPU/CPU} Clusters},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {463--479},
url = {https://www.usenix.org/conference/osdi20/presentation/jiang},
publisher = {USENIX Association},
month = nov
}

@article{xu2020googlezero,
  author       = {Yuanzhong Xu and
                  HyoukJoong Lee and
                  Dehao Chen and
                  Hongjun Choi and
                  Blake A. Hechtman and
                  Shibo Wang},
  title        = {Automatic Cross-Replica Sharding of Weight Update in Data-Parallel
                  Training},
  journal      = {CoRR},
  volume       = {abs/2004.13336},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.13336},
  eprinttype    = {arXiv},
  eprint       = {2004.13336},
  timestamp    = {Sat, 02 May 2020 19:17:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-13336.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{raj2020zero,
  author       = {Samyam Rajbhandari and
                  Jeff Rasley and
                  Olatunji Ruwase and
                  Yuxiong He},
  editor       = {Christine Cuicchi and
                  Irene Qualters and
                  William T. Kramer},
  title        = {ZeRO: memory optimizations toward training trillion parameter models},
  booktitle    = {Proceedings of the International Conference for High Performance Computing,
                  Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta,
                  Georgia, USA, November 9-19, 2020},
  pages        = {20},
  publisher    = {{IEEE/ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1109/SC41405.2020.00024},
  doi          = {10.1109/SC41405.2020.00024},
  timestamp    = {Wed, 04 May 2022 13:02:27 +0200},
  biburl       = {https://dblp.org/rec/conf/sc/RajbhandariRRH20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{jung2023deepum,
  author       = {Jaehoon Jung and
                  Jinpyo Kim and
                  Jaejin Lee},
  editor       = {Tor M. Aamodt and
                  Natalie D. Enright Jerger and
                  Michael M. Swift},
  title        = {DeepUM: Tensor Migration and Prefetching in Unified Memory},
  booktitle    = {Proceedings of the 28th {ACM} International Conference on Architectural
                  Support for Programming Languages and Operating Systems, Volume 2,
                  {ASPLOS} 2023, Vancouver, BC, Canada, March 25-29, 2023},
  pages        = {207--221},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3575693.3575736},
  doi          = {10.1145/3575693.3575736},
  timestamp    = {Thu, 02 Feb 2023 08:48:06 +0100},
  biburl       = {https://dblp.org/rec/conf/asplos/JungKL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mark2020autotm,
  author       = {Mark Hildebrand and
                  Jawad Khan and
                  Sanjeev Trika and
                  Jason Lowe{-}Power and
                  Venkatesh Akella},
  editor       = {James R. Larus and
                  Luis Ceze and
                  Karin Strauss},
  title        = {AutoTM: Automatic Tensor Movement in Heterogeneous Memory Systems using Integer Linear Programming},
  booktitle    = {{ASPLOS} '20: Architectural Support for Programming Languages and Operating Systems, Lausanne, Switzerland, March 16-20, 2020},
  pages        = {875--890},
  publisher    = {{ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1145/3373376.3378465},
  doi          = {10.1145/3373376.3378465},
  timestamp    = {Sun, 25 Jul 2021 11:54:03 +0200},
  biburl       = {https://dblp.org/rec/conf/asplos/HildebrandKTLA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bae2021flashneuron,
  author       = {Jonghyun Bae and
                  Jongsung Lee and
                  Yunho Jin and
                  Sam Son and
                  Shine Kim and
                  Hakbeom Jang and
                  Tae Jun Ham and
                  Jae W. Lee},
  editor       = {Marcos K. Aguilera and
                  Gala Yadgar},
  title        = {FlashNeuron: SSD-Enabled Large-Batch Training of Very Deep Neural
                  Networks},
  booktitle    = {19th {USENIX} Conference on File and Storage Technologies, {FAST}
                  2021, February 23-25, 2021},
  pages        = {387--401},
  publisher    = {{USENIX} Association},
  year         = {2021},
  url          = {https://www.usenix.org/conference/fast21/presentation/bae},
  timestamp    = {Mon, 07 Nov 2022 13:50:16 +0100},
  biburl       = {https://dblp.org/rec/conf/fast/BaeLJSKJHL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2018superneurons,
  author       = {Linnan Wang and
                  Jinmian Ye and
                  Yiyang Zhao and
                  Wei Wu and
                  Ang Li and
                  Shuaiwen Leon Song and
                  Zenglin Xu and
                  Tim Kraska},
  editor       = {Andreas Krall and
                  Thomas R. Gross},
  title        = {Superneurons: dynamic {GPU} memory management for training deep neural
                  networks},
  booktitle    = {Proceedings of the 23rd {ACM} {SIGPLAN} Symposium on Principles and
                  Practice of Parallel Programming, PPoPP 2018, Vienna, Austria, February
                  24-28, 2018},
  pages        = {41--53},
  publisher    = {{ACM}},
  year         = {2018},
  url          = {https://doi.org/10.1145/3178487.3178491},
  doi          = {10.1145/3178487.3178491},
  timestamp    = {Tue, 29 Nov 2022 22:22:37 +0100},
  biburl       = {https://dblp.org/rec/conf/ppopp/WangYZWLSXK18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peng2020capuchin,
  author       = {Xuan Peng and
                  Xuanhua Shi and
                  Hulin Dai and
                  Hai Jin and
                  Weiliang Ma and
                  Qian Xiong and
                  Fan Yang and
                  Xuehai Qian},
  editor       = {James R. Larus and
                  Luis Ceze and
                  Karin Strauss},
  title        = {Capuchin: Tensor-based {GPU} Memory Management for Deep Learning},
  booktitle    = {{ASPLOS} '20: Architectural Support for Programming Languages and
                  Operating Systems, Lausanne, Switzerland, March 16-20, 2020},
  pages        = {891--905},
  publisher    = {{ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1145/3373376.3378505},
  doi          = {10.1145/3373376.3378505},
  timestamp    = {Tue, 31 Aug 2021 20:51:08 +0200},
  biburl       = {https://dblp.org/rec/conf/asplos/PengSD0MXYQ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {choi2022uvm,
author = {Sangjin Choi and Taeksoo Kim and Jinwoo Jeong and Rachata Ausavarungnirun and Myeongjae Jeon and Youngjin Kwon and Jeongseob Ahn},
title = {Memory Harvesting in {Multi-GPU} Systems with Hierarchical Unified Virtual Memory},
booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
year = {2022},
isbn = {978-1-939133-29-66},
address = {Carlsbad, CA},
pages = {625--638},
url = {https://www.usenix.org/conference/atc22/presentation/choi-sangjin},
publisher = {USENIX Association},
month = jul
}

@inproceedings{huang2021gnn,
  author       = {Kezhao Huang and
                  Jidong Zhai and
                  Zhen Zheng and
                  Youngmin Yi and
                  Xipeng Shen},
  editor       = {Jaejin Lee and
                  Erez Petrank},
  title        = {Understanding and bridging the gaps in current {GNN} performance optimizations},
  booktitle    = {PPoPP '21: 26th {ACM} {SIGPLAN} Symposium on Principles and Practice
                  of Parallel Programming, Virtual Event, Republic of Korea, February
                  27- March 3, 2021},
  pages        = {119--132},
  publisher    = {{ACM}},
  year         = {2021},
  url          = {https://doi.org/10.1145/3437801.3441585},
  doi          = {10.1145/3437801.3441585},
  timestamp    = {Sun, 12 Jun 2022 19:46:08 +0200},
  biburl       = {https://dblp.org/rec/conf/ppopp/HuangZZYS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{huang2023gnn,
  author       = {Kezhao Huang and
                  Haitian Jiang and
                  Minjie Wang and
                  Guangxuan Xiao and
                  David Wipf and
                  Xiang Song and
                  Quan Gan and
                  Zengfeng Huang and
                  Jidong Zhai and
                  Zheng Zhang},
  title        = {ReFresh: Reducing Memory Access from Exploiting Stable Historical
                  Embeddings for Graph Neural Network Training},
  journal      = {CoRR},
  volume       = {abs/2301.07482},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2301.07482},
  doi          = {10.48550/arXiv.2301.07482},
  eprinttype    = {arXiv},
  eprint       = {2301.07482},
  timestamp    = {Tue, 21 Mar 2023 21:05:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2301-07482.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kurniawan2023EVStore,
author = {Kurniawan, Daniar H. and Wang, Ruipu and Zulkifli, Kahfi S. and Wiranata, Fandi A. and Bent, John and Vigfusson, Ymir and Gunawi, Haryadi S.},
title = {EVStore: Storage and Caching Capabilities for Scaling Embedding Tables in Deep Recommendation Systems},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575718},
doi = {10.1145/3575693.3575718},
abstract = {Modern recommendation systems, primarily driven by deep-learning models, depend on fast model inferences to be useful. To tackle the sparsity in the input space, particularly for categorical variables, such inferences are made by storing increasingly large embedding vector (EV) tables in memory. A core challenge is that the inference operation has an all-or-nothing property: each inference requires multiple EV table lookups, but if any memory access is slow, the whole inference request is slow. In our paper, we design, implement and evaluate EVStore, a 3-layer EV table lookup system that harnesses both structural regularity in inference operations and domain-specific approximations to provide optimized caching, yielding up to 23\% and 27\% reduction on the average and p90 latency while quadrupling throughput at 0.2\% loss in accuracy. Finally, we show that at a minor cost of accuracy, EVStore can reduce the Deep Recommendation System (DRS) memory usage by up to 94\%, yielding potentially enormous savings for these costly, pervasive systems.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {281–294},
numpages = {14},
keywords = {Inference systems, Recommendation Systems, Deep learning, Caching systems, Performance},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings {Lai2023AdaEmbed,
author = {Fan Lai and Wei Zhang and Rui Liu and William Tsai and Xiaohan Wei and Yuxi Hu and Sabin Devkota and Jianyu Huang and Jongsoo Park and Xing Liu and Zeliang Chen and Ellie Wen and Paul Rivera and Jie You and Chun-cheng Jason Chen and Mosharaf Chowdhury},
title = {{AdaEmbed}: Adaptive Embedding for {Large-Scale} Recommendation Models},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {817--831},
url = {https://www.usenix.org/conference/osdi23/presentation/lai},
publisher = {USENIX Association},
month = jul
}

@article{Sethi2023FlexShard,
  author       = {Geet Sethi and
                  Pallab Bhattacharya and
                  Dhruv Choudhary and
                  Carole{-}Jean Wu and
                  Christos Kozyrakis},
  title        = {FlexShard: Flexible Sharding for Industry-Scale Sequence Recommendation
                  Models},
  journal      = {CoRR},
  volume       = {abs/2301.02959},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2301.02959},
  doi          = {10.48550/arXiv.2301.02959},
  eprinttype    = {arXiv},
  eprint       = {2301.02959},
  timestamp    = {Tue, 10 Jan 2023 15:10:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2301-02959.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zhao2022recd,
  author       = {Mark Zhao and
                  Dhruv Choudhary and
                  Devashish Tyagi and
                  Ajay Somani and
                  Max Kaplan and
                  Sung{-}Han Lin and
                  Sarunya Pumma and
                  Jongsoo Park and
                  Aarti Basant and
                  Niket Agarwal and
                  Carole{-}Jean Wu and
                  Christos Kozyrakis},
  title        = {RecD: Deduplication for End-to-End Deep Learning Recommendation Model
                  Training Infrastructure},
  journal      = {CoRR},
  volume       = {abs/2211.05239},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05239},
  doi          = {10.48550/arXiv.2211.05239},
  eprinttype    = {arXiv},
  eprint       = {2211.05239},
  timestamp    = {Wed, 23 Nov 2022 14:32:39 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05239.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {Sima2022Ekko,
author = {Chijun Sima and Yao Fu and Man-Kit Sit and Liyi Guo and Xuri Gong and Feng Lin and Junyu Wu and Yongsheng Li and Haidong Rong and Pierre-Louis Aublin and Luo Mai},
title = {Ekko: A {Large-Scale} Deep Learning Recommender System with {Low-Latency} Model Update},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {821--839},
url = {https://www.usenix.org/conference/osdi22/presentation/sima},
publisher = {USENIX Association},
month = jul
}

@inproceedings{zhang2022picasso,
  author       = {Yuanxing Zhang and
                  Langshi Chen and
                  Siran Yang and
                  Man Yuan and
                  Huimin Yi and
                  Jie Zhang and
                  Jiamang Wang and
                  Jianbo Dong and
                  Yunlong Xu and
                  Yue Song and
                  Yong Li and
                  Di Zhang and
                  Wei Lin and
                  Lin Qu and
                  Bo Zheng},
  title        = {{PICASSO:} Unleashing the Potential of GPU-centric Training for Wide-and-deep
                  Recommender Systems},
  booktitle    = {38th {IEEE} International Conference on Data Engineering, {ICDE} 2022,
                  Kuala Lumpur, Malaysia, May 9-12, 2022},
  pages        = {3453--3466},
  publisher    = {{IEEE}},
  year         = {2022},
  url          = {https://doi.org/10.1109/ICDE53745.2022.00324},
  doi          = {10.1109/ICDE53745.2022.00324},
  timestamp    = {Sat, 26 Aug 2023 17:33:04 +0200},
  biburl       = {https://dblp.org/rec/conf/icde/ZhangCYYYZWDXSL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{miao2021het,
  author       = {Xupeng Miao and
                  Hailin Zhang and
                  Yining Shi and
                  Xiaonan Nie and
                  Zhi Yang and
                  Yangyu Tao and
                  Bin Cui},
  title        = {{HET:} Scaling out Huge Embedding Model Training via Cache-enabled
                  Distributed Framework},
  journal      = {Proc. {VLDB} Endow.},
  volume       = {15},
  number       = {2},
  pages        = {312--320},
  year         = {2021},
  url          = {http://www.vldb.org/pvldb/vol15/p312-miao.pdf},
  doi          = {10.14778/3489496.3489511},
  timestamp    = {Wed, 10 May 2023 16:06:52 +0200},
  biburl       = {https://dblp.org/rec/journals/pvldb/MiaoZSNYTC21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{miao2022hetgmp,
  author       = {Xupeng Miao and
                  Yining Shi and
                  Hailin Zhang and
                  Xin Zhang and
                  Xiaonan Nie and
                  Zhi Yang and
                  Bin Cui},
  editor       = {Zachary G. Ives and
                  Angela Bonifati and
                  Amr El Abbadi},
  title        = {{HET-GMP:} {A} Graph-based System Approach to Scaling Large Embedding
                  Model Training},
  booktitle    = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
                  PA, USA, June 12 - 17, 2022},
  pages        = {470--480},
  publisher    = {{ACM}},
  year         = {2022},
  url          = {https://doi.org/10.1145/3514221.3517902},
  doi          = {10.1145/3514221.3517902},
  timestamp    = {Wed, 10 May 2023 16:06:48 +0200},
  biburl       = {https://dblp.org/rec/conf/sigmod/MiaoSZZNY022.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xie2022fleche,
author = {Xie, Minhui and Lu, Youyou and Lin, Jiazhen and Wang, Qing and Gao, Jian and Ren, Kai and Shu, Jiwu},
title = {Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3519554},
doi = {10.1145/3492321.3519554},
abstract = {Deep learning based models have dominated current production recommendation systems. However, the gap between CPU-side DRAM data accessing and GPU processing still impedes their inference performance. GPU-resident cache can bridge this gap, but we find that existing systems leave the benefits to cache the embedding table, a huge sparse structure, on GPU unexploited. In this paper, we present Fleche, a holistic cache scheme with detailed designs for efficient GPU-resident embedding caching. Fleche (1) uses one cache backend for all embedding tables to improve the total cache utilization, and (2) merges small kernel calls into one unitary call to reduce the overhead of kernel maintenance (e.g., kernel launching and synchronizing). Furthermore, we carefully design the cache query workflow for finer-grain parallelism. Evaluations with real-world datasets show that compared with the prior art, Fleche significantly improves the throughput of embedding layer by 2.0 -- 5.4\texttimes{}, and gets up to 2.4\texttimes{} speedup of end-to-end inference throughput.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {402–416},
numpages = {15},
keywords = {deep learning recommendation models, memory management, embedding lookup, GPU cache},
location = {Rennes, France},
series = {EuroSys '22}
}

@inproceedings{feng2023mobius,
  author       = {Yangyang Feng and
                  Minhui Xie and
                  Zijie Tian and
                  Shuo Wang and
                  Youyou Lu and
                  Jiwu Shu},
  editor       = {Tor M. Aamodt and
                  Natalie D. Enright Jerger and
                  Michael M. Swift},
  title        = {Mobius: Fine Tuning Large-Scale Models on Commodity {GPU} Servers},
  booktitle    = {Proceedings of the 28th {ACM} International Conference on Architectural
                  Support for Programming Languages and Operating Systems, Volume 2,
                  {ASPLOS} 2023, Vancouver, BC, Canada, March 25-29, 2023},
  pages        = {489--501},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3575693.3575703},
  doi          = {10.1145/3575693.3575703},
  timestamp    = {Thu, 02 Feb 2023 08:48:06 +0100},
  biburl       = {https://dblp.org/rec/conf/asplos/FengXTWLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xie2020kraken,
  author       = {Minhui Xie and
                  Kai Ren and
                  Youyou Lu and
                  Guangxu Yang and
                  Qingxing Xu and
                  Bihai Wu and
                  Jiazhen Lin and
                  Hongbo Ao and
                  Wanhong Xu and
                  Jiwu Shu},
  editor       = {Christine Cuicchi and
                  Irene Qualters and
                  William T. Kramer},
  title        = {Kraken: memory-efficient continual learning for large-scale real-time
                  recommendations},
  booktitle    = {Proceedings of the International Conference for High Performance Computing,
                  Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta,
                  Georgia, USA, November 9-19, 2020},
  pages        = {21},
  publisher    = {{IEEE/ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1109/SC41405.2020.00025},
  doi          = {10.1109/SC41405.2020.00025},
  timestamp    = {Wed, 04 May 2022 13:02:27 +0200},
  biburl       = {https://dblp.org/rec/conf/sc/XieRLYXWLAXS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{waleffe2023MariusGNN,
  author       = {Roger Waleffe and
                  Jason Mohoney and
                  Theodoros Rekatsinas and
                  Shivaram Venkataraman},
  editor       = {Giuseppe Antonio Di Luna and
                  Leonardo Querzoni and
                  Alexandra Fedorova and
                  Dushyanth Narayanan},
  title        = {MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural
                  Networks},
  booktitle    = {Proceedings of the Eighteenth European Conference on Computer Systems,
                  EuroSys 2023, Rome, Italy, May 8-12, 2023},
  pages        = {144--161},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3552326.3567501},
  doi          = {10.1145/3552326.3567501},
  timestamp    = {Wed, 17 May 2023 21:55:32 +0200},
  biburl       = {https://dblp.org/rec/conf/eurosys/WaleffeMRV23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2023tcgnn,
  author       = {Yuke Wang and
                  Boyuan Feng and
                  Zheng Wang and
                  Guyue Huang and
                  Yufei Ding},
  editor       = {Julia Lawall and
                  Dan Williams},
  title        = {{TC-GNN:} Bridging Sparse {GNN} Computation and Dense Tensor Cores
                  on GPUs},
  booktitle    = {2023 {USENIX} Annual Technical Conference, {USENIX} {ATC} 2023, Boston,
                  MA, USA, July 10-12, 2023},
  pages        = {149--164},
  publisher    = {{USENIX} Association},
  year         = {2023},
  url          = {https://www.usenix.org/conference/atc23/presentation/wang-yuke},
  timestamp    = {Sat, 15 Jul 2023 00:21:53 +0200},
  biburl       = {https://dblp.org/rec/conf/usenix/WangFWHD23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2021gnnadvisor,
  author       = {Yuke Wang and
                  Boyuan Feng and
                  Gushu Li and
                  Shuangchen Li and
                  Lei Deng and
                  Yuan Xie and
                  Yufei Ding},
  editor       = {Angela Demke Brown and
                  Jay R. Lorch},
  title        = {GNNAdvisor: An Adaptive and Efficient Runtime System for {GNN} Acceleration
                  on GPUs},
  booktitle    = {15th {USENIX} Symposium on Operating Systems Design and Implementation,
                  {OSDI} 2021, July 14-16, 2021},
  pages        = {515--531},
  publisher    = {{USENIX} Association},
  year         = {2021},
  url          = {https://www.usenix.org/conference/osdi21/presentation/wang-yuke},
  timestamp    = {Thu, 12 Aug 2021 18:19:16 +0200},
  biburl       = {https://dblp.org/rec/conf/osdi/WangFLL00D21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2023mgg,
  author       = {Yuke Wang and
                  Boyuan Feng and
                  Zheng Wang and
                  Tong Geng and
                  Kevin J. Barker and
                  Ang Li and
                  Yufei Ding},
  editor       = {Roxana Geambasu and
                  Ed Nightingale},
  title        = {{MGG:} Accelerating Graph Neural Networks with Fine-Grained Intra-Kernel
                  Communication-Computation Pipelining on Multi-GPU Platforms},
  booktitle    = {17th {USENIX} Symposium on Operating Systems Design and Implementation,
                  {OSDI} 2023, Boston, MA, USA, July 10-12, 2023},
  pages        = {779--795},
  publisher    = {{USENIX} Association},
  year         = {2023},
  url          = {https://www.usenix.org/conference/osdi23/presentation/wang-yuke},
  timestamp    = {Sat, 15 Jul 2023 00:21:53 +0200},
  biburl       = {https://dblp.org/rec/conf/osdi/WangFWGB0D23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hwang2023pregatedmoe,
  author       = {Ranggi Hwang and
                  Jianyu Wei and
                  Shijie Cao and
                  Changho Hwang and
                  Xiaohu Tang and
                  Ting Cao and
                  Mao Yang and
                  Minsoo Rhu},
  title        = {Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable
                  Mixture-of-Expert Inference},
  journal      = {CoRR},
  volume       = {abs/2308.12066},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.12066},
  doi          = {10.48550/arXiv.2308.12066},
  eprinttype    = {arXiv},
  eprint       = {2308.12066},
  timestamp    = {Wed, 30 Aug 2023 17:27:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-12066.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{kong2023edgemoe,
  author       = {Rui Kong and
                  Yuanchun Li and
                  Qingtian Feng and
                  Weijun Wang and
                  Linghe Kong and
                  Yunxin Liu},
  title        = {Serving MoE Models on Resource-constrained Edge Devices via Dynamic
                  Expert Swapping},
  journal      = {CoRR},
  volume       = {abs/2308.15030},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.15030},
  doi          = {10.48550/arXiv.2308.15030},
  eprinttype    = {arXiv},
  eprint       = {2308.15030},
  timestamp    = {Mon, 04 Sep 2023 15:29:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-15030.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2023pipemoe,
  author       = {Xin Chen and
                  Hengheng Zhang and
                  Xiaotao Gu and
                  Kaifeng Bi and
                  Lingxi Xie and
                  Qi Tian},
  title        = {Pipeline MoE: {A} Flexible MoE Implementation with Pipeline Parallelism},
  journal      = {CoRR},
  volume       = {abs/2304.11414},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.11414},
  doi          = {10.48550/arXiv.2304.11414},
  eprinttype    = {arXiv},
  eprint       = {2304.11414},
  timestamp    = {Tue, 02 May 2023 18:58:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-11414.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{huang2023metamoe,
  author       = {Haiyang Huang and
                  Newsha Ardalani and
                  Anna Sun and
                  Liu Ke and
                  Hsien{-}Hsin S. Lee and
                  Anjali Sridhar and
                  Shruti Bhosale and
                  Carole{-}Jean Wu and
                  Benjamin Lee},
  title        = {Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert
                  (MoE) Inference},
  journal      = {CoRR},
  volume       = {abs/2303.06182},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.06182},
  doi          = {10.48550/arXiv.2303.06182},
  eprinttype    = {arXiv},
  eprint       = {2303.06182},
  timestamp    = {Thu, 16 Mar 2023 16:04:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-06182.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{zhang2023mpipemoe,

  author={Zhang, Zheng and Yang, Donglin and Xia, Yaqi and Ding, Liang and Tao, Dacheng and Zhou, Xiaobo and Cheng, Dazhao},

  booktitle={2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 

  title={MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism}, 

  year={2023},

  volume={},

  number={},

  pages={167-177},

  doi={10.1109/IPDPS54959.2023.00026}}


@inproceedings{roc,
  author    = {Zhihao Jia and
               Sina Lin and
               Mingyu Gao and
               Matei Zaharia and
               Alex Aiken},
  editor    = {Inderjit S. Dhillon and
               Dimitris S. Papailiopoulos and
               Vivienne Sze},
  title     = {Improving the Accuracy, Scalability, and Performance of Graph Neural
               Networks with Roc},
  booktitle = {Proceedings of Machine Learning and Systems 2020, MLSys 2020, March 2-4, 2020},
  publisher = {mlsys.org},
  year      = {2020},
  url       = {https://proceedings.mlsys.org/book/300.pdf},
  timestamp = {Thu, 20 Aug 2020 17:55:21 +0200},
  biburl    = {https://dblp.org/rec/conf/mlsys/JiaLGZA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pages = {187--198},
  address = {Austin, TX, USA}
}


@inproceedings{neugraph,
author = {Lingxiao Ma and Zhi Yang and Youshan Miao and Jilong Xue and Ming Wu and Lidong Zhou and Yafei Dai},
title = {NeuGraph: Parallel Deep Neural Network Computation on Large Graphs},
booktitle = {2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {443--458},
url = {https://www.usenix.org/conference/atc19/presentation/ma},
publisher = {{USENIX} Association},
month = jul,
}

@inproceedings{pcgcn,
  title={PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network},
  author={Tian, Chao and Ma, Lingxiao and Yang, Zhi and Dai, Yafei},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={936--945},
  year={2020},
  organization={IEEE}
}

@inproceedings{rasley2020DeepSpeed,
  author    = {Jeff Rasley and
               Samyam Rajbhandari and
               Olatunji Ruwase and
               Yuxiong He},
  title     = {DeepSpeed: System Optimizations Enable Training Deep Learning Models
               with Over 100 Billion Parameters},
  booktitle = {{KDD} '20: The 26th {ACM} {SIGKDD} Conference on Knowledge Discovery
               and Data Mining, Virtual Event, CA, USA, August 23-27, 2020},
  pages     = {3505--3506},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3394486.3406703},
  doi       = {10.1145/3394486.3406703},
  timestamp = {Tue, 09 Mar 2021 09:46:47 +0100},
  biburl    = {https://dblp.org/rec/conf/kdd/RasleyRRH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shoeybi2019megatron,
  author    = {Mohammad Shoeybi and
               Mostofa Patwary and
               Raul Puri and
               Patrick LeGresley and
               Jared Casper and
               Bryan Catanzaro},
  title     = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
               Model Parallelism},
  journal   = {CoRR},
  volume    = {abs/1909.08053},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.08053},
  eprinttype = {arXiv},
  eprint    = {1909.08053},
  timestamp = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2019gpipe,
  author    = {Yanping Huang and
               Youlong Cheng and
               Ankur Bapna and
               Orhan Firat and
               Dehao Chen and
               Mia Xu Chen and
               HyoukJoong Lee and
               Jiquan Ngiam and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen},
  title     = {GPipe: Efficient Training of Giant Neural Networks using Pipeline
               Parallelism},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {103--112},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/HuangCBFCCLNLWC19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fan2021dapple,
  author    = {Shiqing Fan and
               Yi Rong and
               Chen Meng and
               Zongyan Cao and
               Siyu Wang and
               Zhen Zheng and
               Chuan Wu and
               Guoping Long and
               Jun Yang and
               Lixue Xia and
               Lansong Diao and
               Xiaoyong Liu and
               Wei Lin},
  title     = {{DAPPLE:} a pipelined data parallel approach for training large models},
  booktitle = {PPoPP '21: 26th {ACM} {SIGPLAN} Symposium on Principles and Practice
               of Parallel Programming, Virtual Event, Republic of Korea, February
               27- March 3, 2021},
  pages     = {431--445},
  publisher = {{ACM}},
  year      = {2021},
  url       = {https://doi.org/10.1145/3437801.3441593},
  doi       = {10.1145/3437801.3441593},
  timestamp = {Sun, 12 Jun 2022 19:46:08 +0200},
  biburl    = {https://dblp.org/rec/conf/ppopp/FanRMCWZWLYXDLL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{narayanan2019pipedream,
  author    = {Deepak Narayanan and
               Aaron Harlap and
               Amar Phanishayee and
               Vivek Seshadri and
               Nikhil R. Devanur and
               Gregory R. Ganger and
               Phillip B. Gibbons and
               Matei Zaharia},
  title     = {PipeDream: generalized pipeline parallelism for {DNN} training},
  booktitle = {Proceedings of the 27th {ACM} Symposium on Operating Systems Principles,
               {SOSP} 2019, Huntsville, ON, Canada, October 27-30, 2019},
  pages     = {1--15},
  publisher = {{ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1145/3341301.3359646},
  doi       = {10.1145/3341301.3359646},
  timestamp = {Tue, 19 Nov 2019 12:45:13 +0100},
  biburl    = {https://dblp.org/rec/conf/sosp/NarayananHPSDGG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{narayanan2021memory,
  author    = {Deepak Narayanan and
               Amar Phanishayee and
               Kaiyu Shi and
               Xie Chen and
               Matei Zaharia},
  title     = {Memory-Efficient Pipeline-Parallel {DNN} Training},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {7937--7947},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/narayanan21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/NarayananPSCZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{bloom176b,
  author       = {Teven Le Scao and
                  Angela Fan and
                  Christopher Akiki and
                  Ellie Pavlick and
                  Suzana Ilic and
                  Daniel Hesslow and
                  Roman Castagn{\'{e}} and
                  Alexandra Sasha Luccioni and
                  Fran{\c{c}}ois Yvon and
                  Matthias Gall{\'{e}} and
                  Jonathan Tow and
                  Alexander M. Rush and
                  Stella Biderman and
                  Albert Webson and
                  Pawan Sasanka Ammanamanchi and
                  Thomas Wang and
                  Beno{\^{\i}}t Sagot and
                  Niklas Muennighoff and
                  Albert Villanova del Moral and
                  Olatunji Ruwase and
                  Rachel Bawden and
                  Stas Bekman and
                  Angelina McMillan{-}Major and
                  Iz Beltagy and
                  Huu Nguyen and
                  Lucile Saulnier and
                  Samson Tan and
                  Pedro Ortiz Suarez and
                  Victor Sanh and
                  Hugo Lauren{\c{c}}on and
                  Yacine Jernite and
                  Julien Launay and
                  Margaret Mitchell and
                  Colin Raffel and
                  Aaron Gokaslan and
                  Adi Simhi and
                  Aitor Soroa and
                  Alham Fikri Aji and
                  Amit Alfassy and
                  Anna Rogers and
                  Ariel Kreisberg Nitzav and
                  Canwen Xu and
                  Chenghao Mou and
                  Chris Emezue and
                  Christopher Klamm and
                  Colin Leong and
                  Daniel van Strien and
                  David Ifeoluwa Adelani and
                  et al.},
  title        = {{BLOOM:} {A} 176B-Parameter Open-Access Multilingual Language Model},
  journal      = {CoRR},
  volume       = {abs/2211.05100},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05100},
  doi          = {10.48550/arXiv.2211.05100},
  eprinttype    = {arXiv},
  eprint       = {2211.05100},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05100.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{GPT-3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:56:50 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{T5,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {J. Mach. Learn. Res.},
  volume    = {21},
  pages     = {140:1--140:67},
  year      = {2020},
  url       = {http://jmlr.org/papers/v21/20-074.html},
  timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ScalingLaws,
  author    = {Jared Kaplan and
               Sam McCandlish and
               Tom Henighan and
               Tom B. Brown and
               Benjamin Chess and
               Rewon Child and
               Scott Gray and
               Alec Radford and
               Jeffrey Wu and
               Dario Amodei},
  title     = {Scaling Laws for Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2001.08361},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.08361},
  eprinttype = {arXiv},
  eprint    = {2001.08361},
  timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{GLaM,
  author    = {Nan Du and
               Yanping Huang and
               Andrew M. Dai and
               Simon Tong and
               Dmitry Lepikhin and
               Yuanzhong Xu and
               Maxim Krikun and
               Yanqi Zhou and
               Adams Wei Yu and
               Orhan Firat and
               Barret Zoph and
               Liam Fedus and
               Maarten P. Bosma and
               Zongwei Zhou and
               Tao Wang and
               Yu Emma Wang and
               Kellie Webster and
               Marie Pellat and
               Kevin Robinson and
               Kathleen S. Meier{-}Hellstern and
               Toju Duke and
               Lucas Dixon and
               Kun Zhang and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen and
               Claire Cui},
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {5547--5569},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/du22c.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/DuHDTLXKZYFZFBZ22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Switch,
  author    = {William Fedus and
               Barret Zoph and
               Noam Shazeer},
  title     = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
               and Efficient Sparsity},
  journal   = {CoRR},
  volume    = {abs/2101.03961},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.03961},
  eprinttype = {arXiv},
  eprint    = {2101.03961},
  timestamp = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-03961.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zheng2022alpa,
  author       = {Lianmin Zheng and
                  Zhuohan Li and
                  Hao Zhang and
                  Yonghao Zhuang and
                  Zhifeng Chen and
                  Yanping Huang and
                  Yida Wang and
                  Yuanzhong Xu and
                  Danyang Zhuo and
                  Eric P. Xing and
                  Joseph E. Gonzalez and
                  Ion Stoica},
  title        = {Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed
                  Deep Learning},
  booktitle    = {16th {USENIX} Symposium on Operating Systems Design and Implementation,
                  {OSDI} 2022, Carlsbad, CA, USA, July 11-13, 2022},
  pages        = {559--578},
  publisher    = {{USENIX} Association},
  year         = {2022},
  url          = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
  timestamp    = {Tue, 11 Oct 2022 16:51:12 +0200},
  biburl       = {https://dblp.org/rec/conf/osdi/ZhengLZZCHWXZXG22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{peft-adapter-lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/ARXIV.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peft-firstpaper,
  author       = {Neil Houlsby and
                  Andrei Giurgiu and
                  Stanislaw Jastrzebski and
                  Bruna Morrone and
                  Quentin de Laroussilhe and
                  Andrea Gesmundo and
                  Mona Attariyan and
                  Sylvain Gelly},
  editor       = {Kamalika Chaudhuri and
                  Ruslan Salakhutdinov},
  title        = {Parameter-Efficient Transfer Learning for {NLP}},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {2790--2799},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/houlsby19a.html},
  timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/HoulsbyGJMLGAG19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{peft-overview,
  author       = {Vladislav Lialin and
                  Vijeta Deshpande and
                  Anna Rumshisky},
  title        = {Scaling Down to Scale Up: {A} Guide to Parameter-Efficient Fine-Tuning},
  journal      = {CoRR},
  volume       = {abs/2303.15647},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.15647},
  doi          = {10.48550/ARXIV.2303.15647},
  eprinttype    = {arXiv},
  eprint       = {2303.15647},
  timestamp    = {Fri, 14 Apr 2023 17:38:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-15647.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peft-prompt-tuning,
  author       = {Brian Lester and
                  Rami Al{-}Rfou and
                  Noah Constant},
  editor       = {Marie{-}Francine Moens and
                  Xuanjing Huang and
                  Lucia Specia and
                  Scott Wen{-}tau Yih},
  title        = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican
                  Republic, 7-11 November, 2021},
  pages        = {3045--3059},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.emnlp-main.243},
  doi          = {10.18653/V1/2021.EMNLP-MAIN.243},
  timestamp    = {Wed, 16 Mar 2022 23:55:07 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LesterAC21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peft-bitfit,
  author       = {Elad Ben Zaken and
                  Yoav Goldberg and
                  Shauli Ravfogel},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based
                  Masked Language-models},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 2: Short Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {1--9},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-short.1},
  doi          = {10.18653/V1/2022.ACL-SHORT.1},
  timestamp    = {Mon, 01 Aug 2022 16:27:50 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZakenGR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{peft-adapter-IA3,
  author       = {Haokun Liu and
                  Derek Tam and
                  Mohammed Muqeeth and
                  Jay Mohta and
                  Tenghao Huang and
                  Mohit Bansal and
                  Colin Raffel},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than
                  In-Context Learning},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/LiuTMMHBR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{delta-tuning-overview,
  author       = {Ning Ding and
                  Yujia Qin and
                  Guang Yang and
                  Fuchao Wei and
                  Zonghan Yang and
                  Yusheng Su and
                  Shengding Hu and
                  Yulin Chen and
                  Chi{-}Min Chan and
                  Weize Chen and
                  Jing Yi and
                  Weilin Zhao and
                  Xiaozhi Wang and
                  Zhiyuan Liu and
                  Hai{-}Tao Zheng and
                  Jianfei Chen and
                  Yang Liu and
                  Jie Tang and
                  Juanzi Li and
                  Maosong Sun},
  title        = {Delta Tuning: {A} Comprehensive Study of Parameter Efficient Methods
                  for Pre-trained Language Models},
  journal      = {CoRR},
  volume       = {abs/2203.06904},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2203.06904},
  doi          = {10.48550/ARXIV.2203.06904},
  eprinttype    = {arXiv},
  eprint       = {2203.06904},
  timestamp    = {Sat, 30 Sep 2023 10:08:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-06904.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{math-datatset,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/HendrycksBKABTS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhou2022pets,
  title={$\{$PetS$\}$: A Unified Framework for $\{$Parameter-Efficient$\}$ Transformers Serving},
  author={Zhou, Zhe and Wei, Xuechao and Zhang, Jiejing and Sun, Guangyu},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={489--504},
  year={2022}
}

@inproceedings{peft-diff-pruning,
  author       = {Demi Guo and
                  Alexander M. Rush and
                  Yoon Kim},
  editor       = {Chengqing Zong and
                  Fei Xia and
                  Wenjie Li and
                  Roberto Navigli},
  title        = {Parameter-Efficient Transfer Learning with Diff Pruning},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational
                  Linguistics and the 11th International Joint Conference on Natural
                  Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
                  Event, August 1-6, 2021},
  pages        = {4884--4896},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.acl-long.378},
  doi          = {10.18653/V1/2021.ACL-LONG.378},
  timestamp    = {Sat, 09 Apr 2022 12:33:45 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/GuoRK20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{peft-Computation-and-Parameter-Sharing,
  title = 	 {Learn-to-Share: A Hardware-friendly Transfer Learning Framework Exploiting Computation and Parameter Sharing},
  author =       {Fu, Cheng and Huang, Hanxian and Chen, Xinyun and Tian, Yuandong and Zhao, Jishen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3469--3479},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/fu21a/fu21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/fu21a.html},
  abstract = 	 {Task-specific fine-tuning on pre-trained transformers has achieved performance breakthroughs in multiple NLP tasks. Yet, as both computation and parameter size grows linearly with the number of sub-tasks, it is increasingly difficult to adopt such methods to the real world due to unrealistic memory and computation overhead on computing devices. Previous works on fine-tuning focus on reducing the growing parameter size to save storage cost by parameter sharing. However, compared to storage, the constraint of computation is a more critical issue with the fine-tuning models in modern computing environments. In this work, we propose LeTS, a framework that leverages both computation and parameter sharing across multiple tasks. Compared to traditional fine-tuning, LeTS proposes a novel neural architecture that contains a fixed pre-trained transformer model, plus learnable additive components for sub-tasks. The learnable components reuse the intermediate activations in the fixed pre-trained model, decoupling computation dependency. Differentiable neural architecture search is used to determine a task-specific computation sharing scheme, and a novel early stage pruning is applied to additive components for sparsity to achieve parameter sharing. Extensive experiments show that with 1.4% of extra parameters per task, LeTS reduces the computation by 49.5% on GLUE benchmarks with only 0.2% accuracy loss compared to full fine-tuning.}
}

@inproceedings{peft-adapterfusion,
  author       = {Jonas Pfeiffer and
                  Aishwarya Kamath and
                  Andreas R{\"{u}}ckl{\'{e}} and
                  Kyunghyun Cho and
                  Iryna Gurevych},
  editor       = {Paola Merlo and
                  J{\"{o}}rg Tiedemann and
                  Reut Tsarfaty},
  title        = {AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  booktitle    = {Proceedings of the 16th Conference of the European Chapter of the
                  Association for Computational Linguistics: Main Volume, {EACL} 2021,
                  Online, April 19 - 23, 2021},
  pages        = {487--503},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.eacl-main.39},
  doi          = {10.18653/V1/2021.EACL-MAIN.39},
  timestamp    = {Thu, 20 Jan 2022 10:02:52 +0100},
  biburl       = {https://dblp.org/rec/conf/eacl/PfeifferKRCG21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{peft-P-tuning,
  author       = {Xiao Liu and
                  Kaixuan Ji and
                  Yicheng Fu and
                  Zhengxiao Du and
                  Zhilin Yang and
                  Jie Tang},
  title        = {P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally
                  Across Scales and Tasks},
  journal      = {CoRR},
  volume       = {abs/2110.07602},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.07602},
  eprinttype    = {arXiv},
  eprint       = {2110.07602},
  timestamp    = {Mon, 04 Sep 2023 20:40:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-07602.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{slora,
      title={S-LoRA: Serving Thousands of Concurrent LoRA Adapters}, 
      author={Ying Sheng and Shiyi Cao and Dacheng Li and Coleman Hooper and Nicholas Lee and Shuo Yang and Christopher Chou and Banghua Zhu and Lianmin Zheng and Kurt Keutzer and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2311.03285},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@Misc{huggingface-peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@article{qlora,
  author       = {Tim Dettmers and
                  Artidoro Pagnoni and
                  Ari Holtzman and
                  Luke Zettlemoyer},
  title        = {QLoRA: Efficient Finetuning of Quantized LLMs},
  journal      = {CoRR},
  volume       = {abs/2305.14314},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.14314},
  doi          = {10.48550/ARXIV.2305.14314},
  eprinttype    = {arXiv},
  eprint       = {2305.14314},
  timestamp    = {Mon, 05 Jun 2023 15:42:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-14314.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{prefix-tuning,
  author       = {Xiang Lisa Li and
                  Percy Liang},
  title        = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  journal      = {CoRR},
  volume       = {abs/2101.00190},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.00190},
  eprinttype    = {arXiv},
  eprint       = {2101.00190},
  timestamp    = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-00190.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wortsman2022robust,
  title={Robust fine-tuning of zero-shot models},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7959--7971},
  year={2022}
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18--20, 2019, Proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer}
}

@article{peft-math-llama,
  author       = {Tiedong Liu and
                  Bryan Kian Hsiang Low},
  title        = {Goat: Fine-tuned LLaMA Outperforms {GPT-4} on Arithmetic Tasks},
  journal      = {CoRR},
  volume       = {abs/2305.14201},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.14201},
  doi          = {10.48550/ARXIV.2305.14201},
  eprinttype    = {arXiv},
  eprint       = {2305.14201},
  timestamp    = {Mon, 05 Jun 2023 15:42:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-14201.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{peft-github-llama,
  author       = {Carlos E. Jimenez and
                  John Yang and
                  Alexander Wettig and
                  Shunyu Yao and
                  Kexin Pei and
                  Ofir Press and
                  Karthik Narasimhan},
  title        = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  journal      = {CoRR},
  volume       = {abs/2310.06770},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.06770},
  doi          = {10.48550/ARXIV.2310.06770},
  eprinttype    = {arXiv},
  eprint       = {2310.06770},
  timestamp    = {Tue, 24 Oct 2023 14:46:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-06770.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{huatuo-llama,
  author       = {Haochun Wang and
                  Chi Liu and
                  Nuwa Xi and
                  Zewen Qiang and
                  Sendong Zhao and
                  Bing Qin and
                  Ting Liu},
  title        = {HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge},
  journal      = {CoRR},
  volume       = {abs/2304.06975},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.06975},
  doi          = {10.48550/ARXIV.2304.06975},
  eprinttype    = {arXiv},
  eprint       = {2304.06975},
  timestamp    = {Wed, 19 Apr 2023 12:42:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-06975.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{finance-gpt,
  author       = {Hongyang Yang and
                  Xiao{-}Yang Liu and
                  Christina Dan Wang},
  title        = {FinGPT: Open-Source Financial Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2306.06031},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.06031},
  doi          = {10.48550/ARXIV.2306.06031},
  eprinttype    = {arXiv},
  eprint       = {2306.06031},
  timestamp    = {Wed, 14 Jun 2023 13:17:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-06031.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{radiology-llama2,
  author       = {Zhengliang Liu and
                  Yiwei Li and
                  Peng Shu and
                  Aoxiao Zhong and
                  Longtao Yang and
                  Chao Ju and
                  Zihao Wu and
                  Chong Ma and
                  Jie Luo and
                  Cheng Chen and
                  Sekeun Kim and
                  Jiang Hu and
                  Haixing Dai and
                  Lin Zhao and
                  Dajiang Zhu and
                  Jun Liu and
                  Wei Liu and
                  Dinggang Shen and
                  Tianming Liu and
                  Quanzheng Li and
                  Xiang Li},
  title        = {Radiology-Llama2: Best-in-Class Large Language Model for Radiology},
  journal      = {CoRR},
  volume       = {abs/2309.06419},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.06419},
  doi          = {10.48550/ARXIV.2309.06419},
  eprinttype    = {arXiv},
  eprint       = {2309.06419},
  timestamp    = {Tue, 21 Nov 2023 16:23:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-06419.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{dodge2020fine,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}

@inproceedings {atc-finetune,
author = {Saar Eliad and Ido Hakimi and Alon De Jagger and Mark Silberstein and Assaf Schuster},
title = {Fine-tuning giant neural networks on commodity hardware with automatic pipeline model parallelism},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {381--396},
url = {https://www.usenix.org/conference/atc21/presentation/eliad},
publisher = {USENIX Association},
month = jul
}

@article{ftsurvey-1,
  title={A survey of self-supervised and few-shot object detection},
  author={Huang, Gabriel and Laradji, Issam and Vazquez, David and Lacoste-Julien, Simon and Rodriguez, Pau},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={4},
  pages={4071--4089},
  year={2022},
  publisher={IEEE}
}
@article{ftsurvey-2,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}

@article{ftsurvey-3,
  title={A survey on recent approaches for natural language processing in low-resource scenarios},
  author={Hedderich, Michael A and Lange, Lukas and Adel, Heike and Str{\"o}tgen, Jannik and Klakow, Dietrich},
  journal={arXiv preprint arXiv:2010.12309},
  year={2020}
}

@article{opendelta,
  title={OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models},
  author={Hu, Shengding and Ding, Ning and Zhao, Weilin and Lv, Xingtai and Zhang, Zhen and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.03084},
  year={2023}
}

@article{gpt4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/ARXIV.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 28 Aug 2023 21:26:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{flux,
      title={FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion}, 
      author={Li-Wen Chang and Wenlei Bao and Qi Hou and Chengquan Jiang and Ningxin Zheng and Yinmin Zhong and Xuanrun Zhang and Zuquan Song and Chengji Yao and Ziheng Jiang and Haibin Lin and Xin Jin and Xin Liu},
      year={2024},
      eprint={2406.06858},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06858}, 
}

@misc{transformer_engine,
  author = {NVIDIA},
  title = {Transformer Engine},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/NVIDIA/TransformerEngine}},
}


@misc{cybertronai_gradient_checkpointing,
  author = {Tim Salimans and Yaroslav Bulatov},
  title = {Gradient Checkpointing},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/cybertronai/gradient-checkpointing}},
}

@inproceedings {better-together,
author = {Kshiteej Mahajan and Ching-Hsiang Chu and Srinivas Sridharan and Aditya Akella},
title = {Better Together: Jointly Optimizing {ML} Collective Scheduling and Execution Planning using {SYNDICATE}},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {809--824},
url = {https://www.usenix.org/conference/nsdi23/presentation/mahajan},
publisher = {USENIX Association},
month = apr
}

@inproceedings{centauri,
author = {Chen, Chang and Li, Xiuhong and Zhu, Qianchao and Duan, Jiangfei and Sun, Peng and Zhang, Xingcheng and Yang, Chao},
title = {Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651379},
doi = {10.1145/3620666.3651379},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {178–191},
numpages = {14},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{zero-bubble,
  title={Zero Bubble (Almost) Pipeline Parallelism},
  author={Qi, Penghui and Wan, Xinyi and Huang, Guangxing and Lin, Min},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{hotswitch,
  title={Enabling Parallelism Hot Switching for Efficient Training of Large Language Models},
  author={Ge, Hao and Fu, Fangcheng and Li, Haoyang and Wang, Xuanyu and Lin, Sheng and Wang, Yujie and Nie, Xiaonan and Zhang, Hailin and Miao, Xupeng and Cui, Bin},
  booktitle={Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
  pages={178--194},
  year={2024}
}

@inproceedings{unity,
  title={Unity: Accelerating $\{$DNN$\}$ training through joint optimization of algebraic transformations and parallelization},
  author={Unger, Colin and Jia, Zhihao and Wu, Wei and Lin, Sina and Baines, Mandeep and Narvaez, Carlos Efrain Quintero and Ramakrishnaiah, Vinay and Prajapati, Nirmal and McCormick, Pat and Mohd-Yusof, Jamaludin and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={267--284},
  year={2022}
}

@inproceedings{adapipe,
author = {Sun, Zhenbo and Cao, Huanqi and Wang, Yuanwei and Feng, Guanyu and Chen, Shengqi and Wang, Haojie and Chen, Wenguang},
title = {AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651359},
doi = {10.1145/3620666.3651359},
abstract = {Large language models (LLMs) have demonstrated powerful capabilities, requiring huge memory with their increasing sizes and sequence lengths, thus demanding larger parallel systems. The broadly adopted pipeline parallelism introduces even heavier and unbalanced memory consumption. Recomputation is a widely employed technique to mitigate the problem but introduces extra computation overhead.This paper proposes AdaPipe, which aims to find the optimized recomputation and pipeline stage partitioning strategy. AdaPipe employs adaptive recomputation to maximize memory utilization and reduce the computation cost of each pipeline stage. A flexible stage partitioning algorithm is also adopted to balance the computation between different stages. We evaluate AdaPipe by training two representative models, GPT-3 (175B) and Llama 2 (70B), achieving up to 1.32\texttimes{} and 1.22\texttimes{} speedup on clusters with NVIDIA GPUs and Ascend NPUs respectively.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {86–100},
numpages = {15},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}



@Book{arpachiDusseau18:osbook,
  author =       {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau Andrea C.},
  title =        {Operating Systems: Three Easy Pieces},
  publisher =    {Arpaci-Dusseau Books, LLC},
  year =         2015,
  edition =      {1.00},
  note =         {\url{http://pages.cs.wisc.edu/~remzi/OSTEP/}}
}
@InProceedings{waldspurger02,
  author =       {Waldspurger, Carl A.},
  title =        {Memory resource management in {VMware ESX} server},
  booktitle =    {USENIX Symposium on Operating System Design and
                  Implementation (OSDI)},
  year =         2002,
  pages =        {181--194},
  note =         {\url{https://www.usenix.org/legacy/event/osdi02/tech/waldspurger/waldspurger.pdf}}}

@inproceedings {liaman2022alpa,
    author      = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
    title       = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
    booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
    year        = {2022},
    isbn        = {978-1-939133-28-1},
    pages       = {559--578},
    url         = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
}

@inproceedings {zhuohan2023alpaserve,
    author      = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
    title       = {{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
    booktitle   = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
    year        = {2023},
    isbn        = {978-1-939133-34-2},
    pages       = {663--679},
    url         = {https://www.usenix.org/conference/osdi23/presentation/li-zhouhan},
}

@article{chatgpt,
    author      = {OpenAI},
    title       = {Chatgpt},
    note        = {\url{https://chat.openai.com/chat}},
}

@article{bard,
    author      = {Google},
    title       = {Bard},
    note        = {\url{https://bard.google.com/}},
}

@inproceedings {arpan2022clockwork,
    author      = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
    title       = {Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up},
    booktitle   = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
    year        = {2020},
    pages       = {443--462},
}

@article{yao2023deepspeedchat,
    author      = {Yao, Zhewei and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Rajbhandari, Samyam and Wu, Xiaoxia and Awan, Ammar Ahmad and Rasley, Jeff and Zhang, Minjia and Li, Conglong and Holmes, Connor and others},
    title       = {DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales},
    journal     = {arXiv preprint arXiv:2308.01320},
    year        = {2023}
}

@article{openai2023gpt4,
      author={OpenAI},
      title={{GPT-4} Technical Report}, 
      journal     = {arXiv preprint arXiv:2308.01320},
      year={2023},
}

@inproceedings{rajbhandari2020zero,
  author        = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  title         = {Zero: Memory optimizations toward training trillion parameter models},
  booktitle     = {SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  year          = {2020},
  pages         = {1--16},
  organization  = {IEEE}
}

@inproceedings{narayanan2021megatronv2,
    author      = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
    title       = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
    year        = {2021},
    isbn        = {9781450384421},
    publisher   = {Association for Computing Machinery},
    booktitle   = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno   = {58},
    numpages    = {15},
}

@inproceedings{wang2019tofu,
  title={Supporting very large models using automatic dataflow graph partitioning},
  author={Wang, Minjie and Huang, Chien-chin and Li, Jinyang},
  booktitle={Proceedings of the Fourteenth EuroSys Conference 2019},
  pages={1--17},
  year={2019}
}

@inproceedings{kwon2023vllm,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{aminabadi2022deepspeedinfer,
  title={DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022},
  organization={IEEE}
}

@article{wu2023visualchatgpt,
  title         ={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author        = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal       = {arXiv preprint arXiv:2303.04671},
  year          = {2023}
}

@inproceedings{havrilla2023trlx,
  title={trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback},
  author={Havrilla, Alexander and Zhuravinskyi, Maksym and Phung, Duy and Tiwari, Aman and Tow, Jonathan and Biderman, Stella and Anthony, Quentin and Castricato, Louis},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={8578--8595},
  year={2023}
}

@inproceedings{ma2022bagualu,
  title={BaGuaLu: targeting brain scale pretrained models with over 37 million cores},
  author={Ma, Zixuan and He, Jiaao and Qiu, Jiezhong and Cao, Huanqi and Wang, Yuanwei and Sun, Zhenbo and Zheng, Liyan and Wang, Haojie and Tang, Shizhi and Zheng, Tianyu and others},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={192--204},
  year={2022}
}

@inproceedings{li2023colossal,
  title={Colossal-ai: A unified deep learning system for large-scale parallel training},
  author={Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
  booktitle={Proceedings of the 52nd International Conference on Parallel Processing},
  pages={766--775},
  year={2023}
}

@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th USENIX symposium on operating systems design and implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{xiao2020antman,
  title={$\{$AntMan$\}$: Dynamic scaling on $\{$GPU$\}$ clusters for deep learning},
  author={Xiao, Wencong and Ren, Shiru and Li, Yong and Zhang, Yang and Hou, Pengyang and Li, Zhi and Feng, Yihui and Lin, Wei and Jia, Yangqing},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages={533--548},
  year={2020}
}

@inproceedings{xiao2018gandiva,
  title={Gandiva: Introspective cluster scheduling for deep learning},
  author={Xiao, Wencong and Bhardwaj, Romil and Ramjee, Ramachandran and Sivathanu, Muthian and Kwatra, Nipun and Han, Zhenhua and Patel, Pratyush and Peng, Xuan and Zhao, Hanyu and Zhang, Quanlu and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={595--610},
  year={2018}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@article{safe-rlhf,
  title={Safe RLHF: Safe Reinforcement Learning from Human Feedback},
  author={Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
  journal={arXiv preprint arXiv:2310.12773},
  year={2023}
}

@inproceedings{flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single GPU},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}


@inproceedings{lu2017flexflow,
  title={Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks},
  author={Lu, Wenyan and Yan, Guihai and Li, Jiajun and Gong, Shijun and Han, Yinhe and Li, Xiaowei},
  booktitle={2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={553--564},
  year={2017},
  organization={IEEE}
}

@inproceedings{feng2024accelerating,
  title={Accelerating Communication in Deep Learning Recommendation Model Training with Dual-Level Adaptive Lossy Compression},
  author={Feng, Hao and Zhang, Boyuan and Ye, Fanjiang and Si, Min and Chu, Ching-Hsiang and Tian, Jiannan and Yin, Chunxing and Deng, Summer and Hao, Yuchen and Balaji, Pavan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
  pages={1--16},
  year={2024}
}

@article{wang2024hiding,
  title={Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution},
  author={Wang, Haiquan and Ruan, Chaoyi and He, Jia and Ruan, Jiaqi and Tang, Chengjie and Ma, Xiaosong and Li, Cheng},
  journal={arXiv preprint arXiv:2411.15871},
  year={2024}
}

@inproceedings{jiasdp4bit,
  title={SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training},
  author={Jia, Jinda and Xie, Cong and Lu, Hanlin and Wang, Daoce and Feng, Hao and Zhang, Chengming and Sun, Baixi and Lin, Haibin and Zhang, Zhi and Liu, Xin and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{li2014communication,
  title={Communication efficient distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Smola, Alexander and Yu, Kai},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{ouyang2022instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2023dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{zhang2023hir,
  title={The Wisdom of Hindsight Makes Language Models Better Instruction Followers},
  author={Zhang, Tianjun and Liu, Fangchen and Wong, Justin and Abbeel, Pieter and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2302.05206},
  year={2023}
}

@article{ramamurthy2022reinforcement,
  title={Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.01241},
  year={2022}
}

@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

@article{liu2023languages,
  title={Languages are rewards: Hindsight finetuning using human feedback},
  author={Liu, Hao and Sferrazza, Carmelo and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2302.02676},
  year={2023}
}

@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2305.14387},
  year={2023}
}

@article{kreutzer2018reliability,
  title={Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning},
  author={Kreutzer, Julia and Uyheng, Joshua and Riezler, Stefan},
  journal={arXiv preprint arXiv:1805.10627},
  year={2018}
}

@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@article{lee2023aligning,
  title={Aligning text-to-image models using human feedback},
  author={Lee, Kimin and Liu, Hao and Ryu, Moonkyung and Watkins, Olivia and Du, Yuqing and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2302.12192},
  year={2023}
}