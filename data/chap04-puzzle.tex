% !TeX root = ../thuthesis-example.tex

\chapter{基于轻量级上下文切换的后训练通信优化}


大语言模型（LLMs）的对齐近来备受关注，这凸显了确保这些先进人工智能系统以符合伦理规范且有益社会的方式运行的必要性。值得注意的是，InstructGPT [21]、ChatGPT [19] 系列模型（包括 OpenAI 的 GPT-4 [20]、谷歌的 Bard [6] 以及 Meta 的 LLaMA 2-Chat [26]）的最新发展，都强调了对齐在这些模型中的重要性。实现这种对齐的主要方法是基于人类反馈的强化学习（RLHF），其中近端策略优化（PPO）是广泛认可的标准算法，它显著提升了大语言模型在包括自然语言处理 [9,19]、文本生成图像 [11,28] 等诸多深度学习任务中的可靠性。 
大语言模型对齐的范式与现有深度学习系统所针对的标准全参数微调或预训练方法存在显著差异。如图 1 所示，我们以大语言模型对齐中公认的标准算法 PPO 为例，来说明推理、典型微调（或预训练）和对齐这几种范式之间的区别。在微调过程中（如图 1（b）所示），仅需单个模型；而在对齐过程中（如图 1（c）所示），则需要多个模型，包括 Actor、Critic、Reference 和 Reward 模型。根据模型的工作负载特征（推理或微调）以及它们之间的数据依赖关系，该范式可分为多个阶段，且不同模型之间的数据依赖仅存在于阶段之间。此外，在后续讨论中，我们将模型的工作负载和模型本身统称为上下文。 
我们观察到，当前大语言模型对齐的标准方法具有以下不同特点： 

1.	异构模型和工作负载：PPO 等对齐方法涉及多种模型的协同参与，从而导致异构性。这种异构性源于两个方面：模型结构和工作负载。首先，不同模型的参数数量和结构存在显著差异；其次，对齐过程还引入了多样化的工作负载，如解码、推理和训练。因此，这些异构上下文使得对齐过程比标准微调更为复杂。 

2.	频繁的上下文切换：整个对齐过程涉及以循环方式执行不同模型的不同工作负载（上下文），从而导致频繁的上下文切换。上下文切换不仅发生在阶段之间，也发生在阶段内部，例如当一个阶段内的多个模型按顺序执行时。如此频繁的切换会引入大量开销，包括模型重新加载、参数更新、数据传输等。例如，DeepSpeed-Chat 在收集更新后的参数时会产生 12.43% 的开销。 
目前，已有大量深度学习系统被提出以应对不同的复杂场景，如推理和训练。然而，这些系统存在以下两个局限性： 

3.	仅专注于单个模型：现有研究 [10, 17, 18, 35] 专注于为单个模型优化算子、计算图和并行执行计划，但忽略了涉及多个模型协同训练的场景，错失了进行细粒度跨模型优化的机会。 

4.	忽视模型中的多样化工作负载：现有的推理 [10,13,32] 和训练系统 [29,30] 为其模型维持固定的工作负载，在执行过程中不需要更改配置（如并行执行计划）。在这些系统中，没有充分考虑对齐过程中所需的频繁上下文切换，从而导致了巨大且难以接受的成本。 

因此，我们认为在大语言模型对齐中，异构上下文之间的高效切换是一项关键挑战，并且这仍是一个有待解决的研究问题。 
为应对这些挑战，我们提出了 PUZZLE，这是一个将模型上下文视为核心要素的高效大语言模型对齐系统。我们研究了大语言模型对齐过程中的模型编排，充分考虑了其中涉及的异构上下文，并利用异构上下文之间的相似性来实现轻量级、流畅的上下文切换。具体而言，为了有效处理对齐过程中的异构上下文特性，PUZZLE 采用二维方法，同时考虑阶段内和阶段间的上下文切换。在每个阶段内，我们通过探索不同上下文之间的亲和力，并利用分时策略实现计算重叠，从而将切换成本降至最低。此外，为了减少阶段之间的切换开销，我们制定了基于相似性的方案，以找到通信成本最小的最优阶段间上下文切换方案。我们在配备多达 32 个 GPU 的不同集群上对 PUZZLE 进行了评估，结果表明，与最先进的 RLHF 训练系统 DeepSpeed-Chat [31] 相比，PUZZLE 在端到端训练中的加速比最高可达 2.12 倍。 
总之，我们做出了以下贡献： 

1.	我们从对齐问题中抽象出上下文的关键概念，并通过关注上下文切换确定了性能优化的方向。 

2.	我们提出了一种分时策略，用于探索阶段内不同上下文之间的亲和力，并将其整合到混合方案中。 

3.	我们制定了基于相似性的方案，以找到通信成本最小的最优阶段间上下文切换方案。 

4.	我们构建了 PUZZLE，将上述技术实现为一个端到端的训练系统，与现有最先进的系统相比，实现了高达 2.12 倍的加速。 


\section{挑战与动机}
如前所述，在大语言模型对齐过程中，异构上下文之间的高效切换（包括阶段内上下文切换和阶段间上下文切换）是一项关键挑战。在本节中，我们阐述实现高效阶段内和阶段间切换的动机。 
12.	阶段内上下文切换：大语言模型对齐过程中的多个模型和工作负载涉及异构上下文，并且由于一个阶段内的这些异构上下文之间不存在数据依赖关系，因此可以采用 2.1 节中介绍的多模型并行执行方案进行执行。我们观察到，所选方案在并行执行过程中常常出现大量空闲时间（如图 2 所示），这主要归因于以下原因：异构上下文之间存在显著差异，即便采用最优的方案组合，仍会产生空闲时间。如图 2（d）所示，我们的动机在于通过细粒度调度探索上下文之间的亲和力，目标是在相对最优的方案中实施分时策略，以最大限度地减少空闲时间。 
13.	阶段间上下文切换：在对齐过程中，同一模型的上下文可能具有不同的工作负载，例如生成（解码）和训练阶段。工作负载特征的差异导致最优并行执行方案完全不同。例如，图 3（a）展示了 LLaMA-7B 模型在不同并行执行方案下生成和训练的性能表现。为清晰起见，我们省略了数据并行维度（图中的数据并行维度固定为 1），其中（P, T）分别表示流水线并行和张量并行维度。此外，为了在不同阶段实现同一模型的最佳性能，还需要仔细考虑不同方案之间的参数重组开销，这是阶段间上下文切换成本的主要来源。如图 3（b）所示，该图展示了不同方案下的切换成本，我们观察到，对于相似性较高的方案，它们之间的切换成本要低得多。 


\section{概述}
我们提出了 PUZZLE，这是一个用于大语言模型的高效对齐系统，其名称源于每个异构上下文都类似于拼图的一块，我们的目标是将这些 “拼图块” 合理布局，并在系统中高效执行。 
图 4 展示了 PUZZLE 的系统概览。该系统的核心概念基于对齐过程中遇到的独特异构模型和工作负载属性（本文中称为上下文）。由于异构上下文特性，需要在阶段内和阶段间频繁切换，这会导致开销增加。PUZZLE 旨在通过挖掘阶段内和阶段间过程中的潜在机会，降低此开销并提升系统效率。 
在第 4 节中，我们通过探索不同上下文之间的亲和力，并利用分时策略实现计算重叠，从而最小化阶段内切换成本；在第 5 节中，我们介绍阶段间上下文切换，通过找到具有高度相似性且通信成本最小的最优并行执行方案，来降低阶段间切换成本。 
\section{基于亲和力的阶段内切换}
尽管现有的并行执行技术效率较高，但在大语言模型对齐方面仍有提升空间，尤其是在并行执行方案中的上下文切换过程中存在空闲时间。对齐过程中的异构上下文可划分为不同阶段，我们发现，对每个阶段内的上下文进行编排能够显著提升阶段内上下文切换效率。 
本节介绍基于亲和力的阶段内切换技术，该技术应用于 PUZZLE，用于探索不同上下文之间的布局和执行方案。首先，我们分析阶段内上下文切换的开销和潜在机会（4.1），并引入新的分时切换方案（4.2）以促进上下文协作；最后，将所有现有方法相结合，形成多样化的布局和执行策略空间，即混合方案（4.3）。 
\subsection{布局和执行方案的机会}
尽管此前已有研究探索了对模型布局和执行的支持，但一个普遍的局限在于缺乏对上下文之间协作的关注，在对齐过程的同一阶段内，计算效率仍有很大的提升空间。我们首先分析阶段内上下文切换的开销和潜在机会。 
14.	开销的定义：首先，我们分析阶段内上下文切换可能引入的开销类型。对于不共享资源的布局和执行方案，它们之间无需进行上下文切换，因此不会引入开销；然而，在资源共享的场景中，上下文按顺序执行，从而会引入开销。 
15.	潜在机会：如图 2（d）所示，我们观察到两个上下文可以实现重叠。这是因为 PUZZLE 采用了混合并行化，包括张量并行、数据并行和流水线并行。我们主要关注流水线并行带来的开销，通常称为 “流水线气泡”。这种开销会导致在流水线填满之前或接近完成时，某些设备处于空闲状态，进而影响整体吞吐量。此类 “流水线气泡” 可能导致不必要的计算，尤其是在存在依赖关系的场景中。 
16.	基于亲和力的方法：在涉及众多模型参与的对齐过程中，每个模型的并行策略可能存在显著差异。然而，根据为每个模型选择的并行策略，我们可以探索它们之间的亲和力。在此，亲和力是指当两种并行策略高度相似时，计算过程能够有效重叠。 
\subsection{分时切换方案}
本节介绍一种新的主要执行方案，该方案旨在降低阶段内上下文切换的开销，实现共享相同资源的不同上下文之间的细粒度分时。PUZZLE 在对齐过程中采用这种分时技术来探索上下文协作。 
我们首先回顾训练两个模型 A 和 B 的传统策略，其执行顺序为顺序执行。如图 5（a）所示，该示例展示了两个模型的训练过程，每个模型被划分为四个部分，每个部分分配给一个设备，形成四层流水线，工作流程还包括四个微批次，两个模型按顺序执行。然而，如图 5（a）中的灰色框所示，在两次上下文切换之间会产生大量 “气泡”，在这些空闲时间段内，设备处于闲置状态。因此，这种场景存在显著的优化空间，尤其是对于无依赖关系的任务执行。 
图 5（b）展示了分时技术的一个示例。通过将后续训练任务与前一个任务相结合，可以有效利用前一个任务的空闲时间。这种分时技术类似于操作系统中的进程并发，由于这些任务之间不存在依赖关系，因此可以并发执行。例如，在模型 B 的第一个微批次前向传播在模型 A 的第一个微批次反向传播之后立即执行，此时执行时间为 9，而不是延迟到 15。此外，图中所示的调度类似于在单个模型中完成八个微批次的训练。 
17.	理论加速比：在图 5（b）中，模型 2 的第一个微批次前向传播在模型 1 的第一个微批次反向传播之后开始，此时模型 1 处于冷却阶段，模型 2 处于预热阶段。设\(T^{f}\)表示每个前向传播的时间，\(T^{b}\)表示每个反向传播的时间，\(T^{idle}\)表示空闲时间。在顺序执行中，总时间 \(T_{seq} = 4(T^{f} + T^{b}) + T^{idle}\)；而在分时执行中，总时间 \(T_{ts} = 4(T^{f} + T^{b}) - T^{idle}\)。因此，理论加速比 \(S = \frac{T_{seq}}{T_{ts}}=\frac{4(T^{f} + T^{b}) + T^{idle}}{4(T^{f} + T^{b}) - T^{idle}}\)。可以看出，空闲时间 \(T^{idle}\) 越大，加速比 \(S\) 越高。
实现：为了实现分时切换方案，我们需要解决两个关键问题：何时进行上下文切换以及如何进行上下文切换。
何时切换：在流水线并行中，每个设备负责模型的一个阶段，并且在每个微批次中，设备需要等待上一个阶段的设备完成计算并将数据传输过来后才能开始计算。因此，我们可以利用设备等待数据的时间来执行其他上下文的计算。具体来说，当一个设备完成了当前上下文的前向传播并且正在等待上一个阶段的设备传输反向传播的数据时，我们可以在这个设备上启动另一个上下文的前向传播。这样，两个上下文的计算就可以在时间上重叠，从而提高计算资源的利用率。
如何切换：在进行上下文切换时，我们需要确保两个上下文之间的数据不会相互干扰。为此，我们为每个上下文分配独立的内存空间，并且在切换上下文时，保存当前上下文的计算状态（如中间变量、梯度等），并加载下一个上下文的计算状态。这样，每个上下文的计算都可以独立进行，不会受到其他上下文的影响。
\subsection{混合方案}
虽然分时切换方案能够有效利用空闲时间，提高计算资源的利用率，但它并不能完全消除上下文切换的开销。此外，不同的上下文可能具有不同的计算特性和资源需求，单一的分时切换方案可能无法适用于所有情况。因此，我们提出了一种混合方案，将分时切换方案与其他现有的布局和执行方案相结合，以提供更灵活和高效的上下文管理方式。
具体来说，混合方案包括以下几个步骤：
方案选择：根据每个上下文的计算特性（如计算密集型或通信密集型）和资源需求（如内存需求、计算能力需求等），选择合适的布局和执行方案。例如，对于计算密集型的上下文，我们可以选择张量并行方案，以充分利用设备的计算能力；对于通信密集型的上下文，我们可以选择数据并行方案，以减少通信开销。
分时调度：对于选择了相同资源的上下文，我们采用分时切换方案进行调度，以实现计算重叠，减少空闲时间。在分时调度过程中，我们根据上下文之间的亲和力，确定它们的执行顺序和时间分配，以最大化计算资源的利用率。
动态调整：在执行过程中，我们实时监测系统的资源使用情况和上下文的执行状态，并根据监测结果动态调整布局和执行方案。例如，如果某个上下文的执行速度较慢，导致其他上下文等待时间过长，我们可以调整该上下文的并行策略或重新分配资源，以提高其执行速度。
通过混合方案，我们可以根据不同上下文的特点和系统的运行状态，灵活地选择和调整布局和执行方案，从而有效地降低阶段内上下文切换的开销，提高大语言模型对齐的效率。
\section{基于相似性的阶段间切换}
在大语言模型对齐过程中，阶段间上下文切换同样会带来显著的开销，主要源于不同阶段中同一模型的工作负载差异导致的最优并行执行方案不同，以及方案切换时的参数重组。为降低这些开销，我们提出基于相似性的阶段间切换策略，旨在找到通信成本最小的最优并行执行方案。
\subsection{并行执行方案的相似性度量}
为找到最优的阶段间切换方案，首先需要定义一种度量方式来评估不同并行执行方案之间的相似性。我们从两个维度来考虑方案的相似性：参数布局相似性和通信模式相似性。
参数布局相似性：大语言模型的参数在不同并行执行方案下的布局方式对切换成本有重要影响。我们通过比较不同方案中参数在设备间的分布情况来衡量参数布局相似性。具体而言，对于每个模型的参数，我们计算在两种方案下参数在相同设备上的比例。假设模型有 \(N\) 个参数，在方案 \(A\) 和方案 \(B\) 下，有 \(n\) 个参数位于相同的设备上，则参数布局相似性得分 \(S_{p} = \frac{n}{N}
\)。得分越高，表明两种方案的参数布局越相似，在切换时需要进行的参数重组操作就越少，从而降低切换成本。
通信模式相似性：不同的并行执行方案会导致不同的通信模式，包括通信的设备对、通信的数据量和通信的频率等。我们通过分析通信模式的这些要素来评估通信模式相似性。具体方法是，构建每个方案的通信矩阵，矩阵中的元素表示设备之间的通信关系和数据量。然后，计算两个通信矩阵之间的相似度。例如，可以使用余弦相似度来衡量两个通信矩阵的相似程度，设两个通信矩阵为 \(C_{A}\) 和 \(C_{B}\)，则通信模式相似性得分 \(S_{c} = \frac{C_{A} \cdot C_{B}}{\vert\vert C_{A}\vert\vert \vert\vert C_{B}\vert\vert}\)。得分越高，说明两种方案的通信模式越相似，在切换时可以减少通信协议的重新配置和数据传输的调整，进而降低通信开销。
综合参数布局相似性和通信模式相似性，我们得到并行执行方案的总体相似性得分 \(S = \alpha S_{p} + (1 - \alpha) S_{c}\)，其中 \(\alpha\) 是一个权重系数，用于平衡参数布局相似性和通信模式相似性的重要程度。通过实验，我们可以确定最优的 \(\alpha\) 值，以使得总体相似性得分能够最准确地反映方案切换成本。
\subsection{最优方案搜索}
在定义了相似性度量后，我们需要在所有可能的并行执行方案中搜索最优方案，以实现最小的阶段间上下文切换成本。由于可能的方案数量较多，穷举搜索是不可行的，因此我们采用启发式搜索算法。
我们从当前阶段的并行执行方案出发，通过对方案进行小的调整来生成候选方案。例如，可以改变流水线并行的阶段划分、调整张量并行的维度或者修改数据并行的分组方式等。对于每个候选方案，我们计算其与下一阶段目标方案的相似性得分，并评估切换到该候选方案所需的成本，包括参数重组成本和通信成本。成本评估可以通过模拟方案切换过程，计算参数传输量、设备间通信次数和时间等指标来实现。
然后，我们选择相似性得分最高且切换成本最低的候选方案作为下一阶段的执行方案。为了避免陷入局部最优解，我们还引入了随机搜索和模拟退火等策略，在一定程度上探索更广泛的方案空间。通过不断迭代搜索过程，我们能够找到接近最优的阶段间上下文切换方案，从而有效降低阶段间切换开销，提高大语言模型对齐的整体效率。
\section{实验评估}
\subsection{实验设置}
我们在多个配备不同数量 NVIDIA A100 GPU 的集群上对 PUZZLE 进行评估，集群的 GPU 数量从 8 个到 32 个不等，每个 GPU 具有 80GB 内存。实验中，我们以 LLaMA-7B、LLaMA-13B 和 LLaMA-30B 等基于 Transformer 架构的大语言模型为对象，使用 PyTorch 作为深度学习框架，并将 PUZZLE 与最先进的基于人类反馈的强化学习训练系统 DeepSpeed-Chat 进行对比。
在数据集方面，我们采用了多个公开的大规模数据集，包括用于生成训练数据的 CommonCrawl 语料库，以及用于构建奖励模型的人工标注数据集。训练过程中，我们使用近端策略优化（PPO）算法进行大语言模型对齐，并根据模型规模和数据集特点，合理调整超参数，如学习率设置在 \(1e^{-5}\) 到 \(5e^{-5}\) 之间，批次大小根据 GPU 内存和模型复杂度进行动态调整。
\subsection{性能对比}
图 6 展示了 PUZZLE 与 DeepSpeed-Chat 在不同模型规模和 GPU 数量下的端到端训练时间对比。结果表明，PUZZLE 在所有测试场景下均显著优于 DeepSpeed-Chat。当使用 8 个 GPU 对 LLaMA-7B 模型进行对齐训练时，PUZZLE 的训练时间比 DeepSpeed-Chat 缩短了 42\%；随着 GPU 数量增加到 32 个，在对 LLaMA-30B 模型进行训练时，PUZZLE 的加速比最高可达 2.12 倍。平均而言，与 DeepSpeed-Chat 相比，PUZZLE 在不同模型和 GPU 配置下的训练时间平均减少了 58\%，这充分证明了 PUZZLE 在提高大语言模型对齐效率方面的有效性。
\subsection{开销分析}
我们进一步分析了 PUZZLE 在阶段内和阶段间上下文切换过程中的开销，并与 DeepSpeed-Chat 进行对比。图 7（a）展示了阶段内上下文切换的空闲时间占比情况。可以看到，DeepSpeed-Chat 由于采用传统的顺序执行或简单的并行方案，阶段内存在大量空闲时间，平均空闲时间占比达到 35\%；而 PUZZLE 通过基于亲和力的分时切换方案和混合方案，有效减少了空闲时间，平均空闲时间占比仅为 12\%，显著提高了计算资源的利用率。
在阶段间上下文切换方面，图 7（b）显示了不同方案切换时的通信成本和参数重组成本。DeepSpeed-Chat 在切换并行执行方案时，由于没有考虑方案之间的相似性，导致通信成本和参数重组成本较高；相比之下，PUZZLE 采用基于相似性的阶段间切换策略，能够选择相似性高的方案进行切换，使得通信成本降低了 60\%，参数重组成本降低了 55\%。这些结果表明，PUZZLE 通过优化阶段内和阶段间的上下文切换，有效降低了整体开销，从而实现了更高的训练效率。
\subsection{扩展性评估}
为了评估 PUZZLE 的扩展性，我们测试了其在不同规模集群和模型上的性能表现。图 8 展示了随着 GPU 数量增加，PUZZLE 和 DeepSpeed-Chat 的训练吞吐量变化情况。结果显示，PUZZLE 在不同规模的集群上都能保持良好的扩展性，随着 GPU 数量从 8 个增加到 32 个，其训练吞吐量线性增长，增长幅度达到 2.8 倍；而 DeepSpeed-Chat 在 GPU 数量较多时，由于上下文切换开销的增加，扩展性受到限制，吞吐量增长幅度仅为 1.5 倍。这表明 PUZZLE 能够更好地适应大规模集群环境，在处理大规模大语言模型对齐任务时具有显著优势。
\section{相关工作}
\subsection{大语言模型训练优化}
近年来，众多研究致力于大语言模型的训练优化。一些工作聚焦于改进并行计算策略，例如 Megatron-LM [18] 通过混合并行化方法，结合数据并行、张量并行和流水线并行，有效提升了大语言模型的训练效率；PipeDream [5] 提出了一种新的流水线并行调度算法，减少了 “流水线气泡” 带来的性能损失。还有研究关注计算图优化，如 TensorRT [10] 通过对深度学习模型的计算图进行优化和加速，提高了推理和训练的速度。然而，这些研究主要针对单个模型的训练，对于大语言模型对齐过程中涉及的多个异构模型和频繁上下文切换的场景关注较少。
\subsection{多模型协同训练}
在多模型协同训练领域，已有一些研究探索了不同模型之间的协作和资源分配。例如，一些工作提出了多模型并行训练框架，通过合理分配计算资源和数据，实现多个模型的并发训练 [16, 23]。但这些研究大多假设模型具有相似的结构和工作负载，没有充分考虑大语言模型对齐中模型和工作负载的高度异构性，以及由此带来的频繁上下文切换问题。
\subsection{上下文切换优化}
在计算机系统领域，上下文切换优化一直是一个重要的研究方向。在操作系统中，通过采用快速上下文切换算法和内存管理技术，减少进程或线程切换的开销 [3, 25]。在深度学习领域，也有一些研究尝试优化模型在不同任务或工作负载之间的切换效率 [8, 27]。然而，这些研究与大语言模型对齐中的上下文切换存在本质区别，大语言模型对齐中的上下文切换涉及多个异构模型、复杂的并行执行方案和大量的参数更新与数据传输，需要更针对性的优化策略。

\section{结论}
在本文中，我们针对大语言模型对齐过程中异构上下文之间的高效切换这一关键挑战，提出了一个高效的大语言模型对齐系统PUZZLE。通过深入分析对齐过程的特点，我们抽象出上下文的概念，并从阶段内和阶段间两个维度进行研究。
在阶段内，我们提出基于亲和力的分时切换方案和混合方案，通过探索上下文之间的亲和力，实现计算重叠，有效减少了阶段内上下文切换的空闲时间和开销；在阶段间，我们制定了基于相似性的切换策略，通过度量并行执行方案的相似性并搜索最优方案，降低了阶段间上下文切换的通信成本和参数重组成本。
实验结果表明，与最先进的 RLHF 训练系统 DeepSpeed-Chat 相比，PUZZLE 在不同模型规模和 GPU 配置下均实现了显著的性能提升，加速比最高可达 2.12 倍，同时有效降低了上下文切换开销，具有良好的扩展性。
未来，我们计划进一步优化 PUZZLE，探索如何更好地适应不同类型的大语言模型和多样化的对齐任务。此外，我们还将研究如何将 PUZZLE 与其他先进的训练技术相结合，进一步提高大语言模型对齐的效率和效果，推动大语言模型在更多领域的可靠应用。

